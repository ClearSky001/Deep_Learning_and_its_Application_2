{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import wandb\n",
    "\n",
    "from training_utilities import train_loop, evaluation_loop, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습시간에는 다양한 학습 전략과 hyperparameter tuning을 통해 CIFAR-10 테스트셋에서 높은 분류 성능을 얻는 것이 목표이다.\n",
    "\n",
    "<mark>과제</mark> 다양한 조건에서 CIFAR-10 데이터셋 학습을 실험해보고 test 데이터셋에서 80% 이상의 accuracy를 달성하라.\n",
    "\n",
    "* 제출물1 : <u>5개 이상의 학습 커브</u>를 포함하는 wandb 화면 캡처 (wandb 웹페이지의 본인 이름 포함하여 캡처)\n",
    "* 제출물2 : 실험 결과에 대한 분석과 논의 (아래에 markdown으로 기입)\n",
    "\n",
    "참고: 코드에 대한 pytest가 따로 없으므로 자유롭게 코드를 변경하여도 무방함.\n",
    "\n",
    "단, <U>Transfer learning 혹은 Batch size는 변경은 수행하지 말것</U>\n",
    "\n",
    "실험 조건 예시\n",
    "- [Network architectures](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
    "- input normalization\n",
    "- [Weight initialization](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_)\n",
    "- [Optimizers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) (Adam, SGD with momentum, ... )\n",
    "- Regularizations (weight decay, dropout, [Data augmentation](https://pytorch.org/vision/0.9/transforms.html), ensembles, ...)\n",
    "- learning rate & [learning rate scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
    "\n",
    "스스로 neural network를 구축할 경우 아래 사항들을 고려하라\n",
    "- Filter size\n",
    "- Number of filters\n",
    "- Pooling vs Strided Convolution\n",
    "- Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 첫번째 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5cgd5zv3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Accuracy@1</td><td>▁</td></tr><tr><td>Validation Loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Accuracy@1</td><td>44.57</td></tr><tr><td>Validation Loss</td><td>2.06364</td></tr><tr><td>epoch</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-mountain-3</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/5cgd5zv3' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/5cgd5zv3</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241016_143557-5cgd5zv3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5cgd5zv3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c5fbdf7e964c3eafdec33e4007890e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113226067067848, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241016_143725-eh1zh6nq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/eh1zh6nq' target=\"_blank\">fearless-armadillo-4</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/eh1zh6nq' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/eh1zh6nq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model resnet50 with 23528522 parameters (23528522 trainable)\n",
      "Training Epoch 1: Loss: 1.8613, Accuracy: 34.25%\n",
      "Validation Epoch 1: Loss: 1.5501, Accuracy: 44.64%\n",
      "Training Epoch 2: Loss: 1.4410, Accuracy: 48.48%\n",
      "Validation Epoch 2: Loss: 1.3563, Accuracy: 51.38%\n",
      "Training Epoch 3: Loss: 1.2755, Accuracy: 55.35%\n",
      "Validation Epoch 3: Loss: 1.2997, Accuracy: 54.26%\n",
      "Training Epoch 4: Loss: 1.1210, Accuracy: 61.08%\n",
      "Validation Epoch 4: Loss: 1.5247, Accuracy: 48.47%\n",
      "Training Epoch 5: Loss: 1.0472, Accuracy: 64.14%\n",
      "Validation Epoch 5: Loss: 1.1240, Accuracy: 60.97%\n",
      "Training Epoch 6: Loss: 0.9413, Accuracy: 67.66%\n",
      "Validation Epoch 6: Loss: 0.9828, Accuracy: 66.01%\n",
      "Training Epoch 7: Loss: 0.7901, Accuracy: 72.56%\n",
      "Validation Epoch 7: Loss: 0.9642, Accuracy: 66.15%\n",
      "Training Epoch 8: Loss: 0.6821, Accuracy: 76.47%\n",
      "Validation Epoch 8: Loss: 0.9396, Accuracy: 68.55%\n",
      "Training Epoch 9: Loss: 0.6586, Accuracy: 77.48%\n",
      "Validation Epoch 9: Loss: 0.9349, Accuracy: 68.87%\n",
      "Training Epoch 10: Loss: 0.5246, Accuracy: 81.94%\n",
      "Validation Epoch 10: Loss: 0.8648, Accuracy: 71.22%\n",
      "Training Epoch 11: Loss: 0.4384, Accuracy: 84.89%\n",
      "Validation Epoch 11: Loss: 1.0884, Accuracy: 66.73%\n",
      "Training Epoch 12: Loss: 0.3951, Accuracy: 86.53%\n",
      "Validation Epoch 12: Loss: 0.8970, Accuracy: 72.06%\n",
      "Training Epoch 13: Loss: 0.3036, Accuracy: 89.64%\n",
      "Validation Epoch 13: Loss: 0.9943, Accuracy: 71.33%\n",
      "Training Epoch 14: Loss: 0.2543, Accuracy: 91.34%\n",
      "Validation Epoch 14: Loss: 1.0087, Accuracy: 71.68%\n",
      "Training Epoch 15: Loss: 0.2476, Accuracy: 91.44%\n",
      "Validation Epoch 15: Loss: 1.1292, Accuracy: 70.56%\n",
      "Training Epoch 16: Loss: 0.1781, Accuracy: 93.97%\n",
      "Validation Epoch 16: Loss: 1.0370, Accuracy: 72.41%\n",
      "Training Epoch 17: Loss: 0.1668, Accuracy: 94.37%\n",
      "Validation Epoch 17: Loss: 1.0972, Accuracy: 71.56%\n",
      "Training Epoch 18: Loss: 0.1371, Accuracy: 95.50%\n",
      "Validation Epoch 18: Loss: 1.1158, Accuracy: 72.20%\n",
      "Training Epoch 19: Loss: 0.1191, Accuracy: 96.03%\n",
      "Validation Epoch 19: Loss: 1.1440, Accuracy: 72.00%\n",
      "Training Epoch 20: Loss: 0.1103, Accuracy: 96.27%\n",
      "Validation Epoch 20: Loss: 1.1089, Accuracy: 72.66%\n",
      "Training Epoch 21: Loss: 0.0978, Accuracy: 96.82%\n",
      "Validation Epoch 21: Loss: 1.1307, Accuracy: 73.18%\n",
      "Training Epoch 22: Loss: 0.0893, Accuracy: 97.04%\n",
      "Validation Epoch 22: Loss: 1.1691, Accuracy: 73.44%\n",
      "Training Epoch 23: Loss: 0.0738, Accuracy: 97.53%\n",
      "Validation Epoch 23: Loss: 1.1970, Accuracy: 73.09%\n",
      "Training Epoch 24: Loss: 0.0691, Accuracy: 97.66%\n",
      "Validation Epoch 24: Loss: 1.2242, Accuracy: 72.56%\n",
      "Training Epoch 25: Loss: 0.0615, Accuracy: 97.94%\n",
      "Validation Epoch 25: Loss: 1.2509, Accuracy: 73.11%\n",
      "Training Epoch 26: Loss: 0.0529, Accuracy: 98.33%\n",
      "Validation Epoch 26: Loss: 1.2041, Accuracy: 73.87%\n",
      "Training Epoch 27: Loss: 0.0502, Accuracy: 98.34%\n",
      "Validation Epoch 27: Loss: 1.2370, Accuracy: 73.35%\n",
      "Training Epoch 28: Loss: 0.0408, Accuracy: 98.67%\n",
      "Validation Epoch 28: Loss: 1.2321, Accuracy: 73.87%\n",
      "Training Epoch 29: Loss: 0.0339, Accuracy: 98.86%\n",
      "Validation Epoch 29: Loss: 1.3489, Accuracy: 73.21%\n",
      "Training Epoch 30: Loss: 0.0305, Accuracy: 99.03%\n",
      "Validation Epoch 30: Loss: 1.2841, Accuracy: 74.09%\n",
      "Training Epoch 31: Loss: 0.0284, Accuracy: 99.06%\n",
      "Validation Epoch 31: Loss: 1.3526, Accuracy: 73.66%\n",
      "Training Epoch 32: Loss: 0.0211, Accuracy: 99.32%\n",
      "Validation Epoch 32: Loss: 1.3288, Accuracy: 74.43%\n",
      "Training Epoch 33: Loss: 0.0162, Accuracy: 99.47%\n",
      "Validation Epoch 33: Loss: 1.3381, Accuracy: 74.58%\n",
      "Training Epoch 34: Loss: 0.0145, Accuracy: 99.53%\n",
      "Validation Epoch 34: Loss: 1.3508, Accuracy: 74.66%\n",
      "Training Epoch 35: Loss: 0.0114, Accuracy: 99.65%\n",
      "Validation Epoch 35: Loss: 1.3076, Accuracy: 74.88%\n",
      "Training Epoch 36: Loss: 0.0079, Accuracy: 99.75%\n",
      "Validation Epoch 36: Loss: 1.3828, Accuracy: 75.12%\n",
      "Training Epoch 37: Loss: 0.0058, Accuracy: 99.83%\n",
      "Validation Epoch 37: Loss: 1.4279, Accuracy: 75.02%\n",
      "Training Epoch 38: Loss: 0.0042, Accuracy: 99.88%\n",
      "Validation Epoch 38: Loss: 1.4238, Accuracy: 74.38%\n",
      "Training Epoch 39: Loss: 0.0039, Accuracy: 99.88%\n",
      "Validation Epoch 39: Loss: 1.4020, Accuracy: 74.95%\n",
      "Training Epoch 40: Loss: 0.0048, Accuracy: 99.87%\n",
      "Validation Epoch 40: Loss: 1.3862, Accuracy: 75.04%\n",
      "Training Epoch 41: Loss: 0.0025, Accuracy: 99.94%\n",
      "Validation Epoch 41: Loss: 1.4061, Accuracy: 74.96%\n",
      "Training Epoch 42: Loss: 0.0014, Accuracy: 99.97%\n",
      "Validation Epoch 42: Loss: 1.3916, Accuracy: 75.46%\n",
      "Training Epoch 43: Loss: 0.0009, Accuracy: 99.98%\n",
      "Validation Epoch 43: Loss: 1.3882, Accuracy: 75.53%\n",
      "Training Epoch 44: Loss: 0.0006, Accuracy: 100.00%\n",
      "Validation Epoch 44: Loss: 1.3981, Accuracy: 75.58%\n",
      "Training Epoch 45: Loss: 0.0005, Accuracy: 100.00%\n",
      "Validation Epoch 45: Loss: 1.3783, Accuracy: 75.60%\n",
      "Training Epoch 46: Loss: 0.0005, Accuracy: 100.00%\n",
      "Validation Epoch 46: Loss: 1.3790, Accuracy: 75.70%\n",
      "Training Epoch 47: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 47: Loss: 1.3853, Accuracy: 75.50%\n",
      "Training Epoch 48: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 48: Loss: 1.3926, Accuracy: 75.47%\n",
      "Training Epoch 49: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 49: Loss: 1.3834, Accuracy: 75.45%\n",
      "Training Epoch 50: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 50: Loss: 1.3975, Accuracy: 75.57%\n",
      "Training Epoch 51: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 51: Loss: 1.3790, Accuracy: 75.40%\n",
      "Training Epoch 52: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 52: Loss: 1.3933, Accuracy: 75.53%\n",
      "Training Epoch 53: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 53: Loss: 1.3817, Accuracy: 75.48%\n",
      "Training Epoch 54: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 54: Loss: 1.3747, Accuracy: 75.56%\n",
      "Training Epoch 55: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 55: Loss: 1.3740, Accuracy: 75.54%\n",
      "Training Epoch 56: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 56: Loss: 1.3717, Accuracy: 75.32%\n",
      "Training Epoch 57: Loss: 0.0005, Accuracy: 99.99%\n",
      "Validation Epoch 57: Loss: 1.4103, Accuracy: 75.25%\n",
      "Training Epoch 58: Loss: 0.0012, Accuracy: 99.96%\n",
      "Validation Epoch 58: Loss: 1.3954, Accuracy: 75.31%\n",
      "Training Epoch 59: Loss: 0.0019, Accuracy: 99.96%\n",
      "Validation Epoch 59: Loss: 1.4001, Accuracy: 74.97%\n",
      "Training Epoch 60: Loss: 0.0021, Accuracy: 99.94%\n",
      "Validation Epoch 60: Loss: 1.4139, Accuracy: 74.59%\n",
      "Training Epoch 61: Loss: 0.0037, Accuracy: 99.92%\n",
      "Validation Epoch 61: Loss: 1.4102, Accuracy: 74.83%\n",
      "Training Epoch 62: Loss: 0.0050, Accuracy: 99.88%\n",
      "Validation Epoch 62: Loss: 1.4304, Accuracy: 74.12%\n",
      "Training Epoch 63: Loss: 0.0095, Accuracy: 99.72%\n",
      "Validation Epoch 63: Loss: 1.4047, Accuracy: 74.93%\n",
      "Training Epoch 64: Loss: 0.0090, Accuracy: 99.75%\n",
      "Validation Epoch 64: Loss: 1.4284, Accuracy: 74.34%\n",
      "Training Epoch 65: Loss: 0.0102, Accuracy: 99.70%\n",
      "Validation Epoch 65: Loss: 1.4568, Accuracy: 72.95%\n",
      "Training Epoch 66: Loss: 0.0186, Accuracy: 99.42%\n",
      "Validation Epoch 66: Loss: 1.3483, Accuracy: 74.41%\n",
      "Training Epoch 67: Loss: 0.0195, Accuracy: 99.38%\n",
      "Validation Epoch 67: Loss: 1.3370, Accuracy: 74.36%\n",
      "Training Epoch 68: Loss: 0.0212, Accuracy: 99.32%\n",
      "Validation Epoch 68: Loss: 1.3427, Accuracy: 73.75%\n",
      "Training Epoch 69: Loss: 0.0265, Accuracy: 99.17%\n",
      "Validation Epoch 69: Loss: 1.3435, Accuracy: 73.19%\n",
      "Training Epoch 70: Loss: 0.0278, Accuracy: 99.12%\n",
      "Validation Epoch 70: Loss: 1.3003, Accuracy: 73.76%\n",
      "Training Epoch 71: Loss: 0.0394, Accuracy: 98.73%\n",
      "Validation Epoch 71: Loss: 1.2712, Accuracy: 73.74%\n",
      "Training Epoch 72: Loss: 0.0398, Accuracy: 98.72%\n",
      "Validation Epoch 72: Loss: 1.3014, Accuracy: 72.99%\n",
      "Training Epoch 73: Loss: 0.0474, Accuracy: 98.49%\n",
      "Validation Epoch 73: Loss: 1.2172, Accuracy: 74.13%\n",
      "Training Epoch 74: Loss: 0.0472, Accuracy: 98.48%\n",
      "Validation Epoch 74: Loss: 1.2821, Accuracy: 72.90%\n",
      "Training Epoch 75: Loss: 0.0583, Accuracy: 98.17%\n",
      "Validation Epoch 75: Loss: 1.2333, Accuracy: 73.13%\n",
      "Training Epoch 76: Loss: 0.0601, Accuracy: 98.03%\n",
      "Validation Epoch 76: Loss: 1.2208, Accuracy: 73.08%\n",
      "Training Epoch 77: Loss: 0.0664, Accuracy: 97.82%\n",
      "Validation Epoch 77: Loss: 1.1594, Accuracy: 73.77%\n",
      "Training Epoch 78: Loss: 0.0653, Accuracy: 97.99%\n",
      "Validation Epoch 78: Loss: 1.1818, Accuracy: 73.88%\n",
      "Training Epoch 79: Loss: 0.0693, Accuracy: 97.94%\n",
      "Validation Epoch 79: Loss: 1.2275, Accuracy: 72.45%\n",
      "Training Epoch 80: Loss: 0.0803, Accuracy: 97.31%\n",
      "Validation Epoch 80: Loss: 1.1419, Accuracy: 73.37%\n",
      "Training Epoch 81: Loss: 0.0815, Accuracy: 97.32%\n",
      "Validation Epoch 81: Loss: 1.1985, Accuracy: 72.45%\n",
      "Training Epoch 82: Loss: 0.0907, Accuracy: 97.05%\n",
      "Validation Epoch 82: Loss: 1.1953, Accuracy: 72.20%\n",
      "Training Epoch 83: Loss: 0.0897, Accuracy: 97.03%\n",
      "Validation Epoch 83: Loss: 1.1627, Accuracy: 73.60%\n",
      "Training Epoch 84: Loss: 0.0894, Accuracy: 97.16%\n",
      "Validation Epoch 84: Loss: 1.1840, Accuracy: 73.13%\n",
      "Training Epoch 85: Loss: 0.0972, Accuracy: 96.82%\n",
      "Validation Epoch 85: Loss: 1.1360, Accuracy: 74.14%\n",
      "Training Epoch 86: Loss: 0.1067, Accuracy: 96.51%\n",
      "Validation Epoch 86: Loss: 1.0986, Accuracy: 72.99%\n",
      "Training Epoch 87: Loss: 0.0988, Accuracy: 96.82%\n",
      "Validation Epoch 87: Loss: 1.0785, Accuracy: 73.43%\n",
      "Training Epoch 88: Loss: 0.0959, Accuracy: 96.86%\n",
      "Validation Epoch 88: Loss: 1.1466, Accuracy: 73.05%\n",
      "Training Epoch 89: Loss: 0.1067, Accuracy: 96.54%\n",
      "Validation Epoch 89: Loss: 1.1705, Accuracy: 72.74%\n",
      "Training Epoch 90: Loss: 0.1048, Accuracy: 96.47%\n",
      "Validation Epoch 90: Loss: 1.0587, Accuracy: 73.63%\n",
      "Training Epoch 91: Loss: 0.1025, Accuracy: 96.61%\n",
      "Validation Epoch 91: Loss: 1.0519, Accuracy: 73.76%\n",
      "Training Epoch 92: Loss: 0.1003, Accuracy: 96.67%\n",
      "Validation Epoch 92: Loss: 1.1333, Accuracy: 73.73%\n",
      "Training Epoch 93: Loss: 0.1040, Accuracy: 96.57%\n",
      "Validation Epoch 93: Loss: 1.1607, Accuracy: 72.91%\n",
      "Training Epoch 94: Loss: 0.1029, Accuracy: 96.56%\n",
      "Validation Epoch 94: Loss: 1.1412, Accuracy: 73.44%\n",
      "Training Epoch 95: Loss: 0.1049, Accuracy: 96.42%\n",
      "Validation Epoch 95: Loss: 1.1450, Accuracy: 73.19%\n",
      "Training Epoch 96: Loss: 0.1045, Accuracy: 96.51%\n",
      "Validation Epoch 96: Loss: 1.1266, Accuracy: 73.08%\n",
      "Training Epoch 97: Loss: 0.1002, Accuracy: 96.62%\n",
      "Validation Epoch 97: Loss: 1.1032, Accuracy: 73.10%\n",
      "Training Epoch 98: Loss: 0.0949, Accuracy: 96.89%\n",
      "Validation Epoch 98: Loss: 1.1660, Accuracy: 73.15%\n",
      "Training Epoch 99: Loss: 0.0989, Accuracy: 96.67%\n",
      "Validation Epoch 99: Loss: 1.1282, Accuracy: 73.11%\n",
      "Training Epoch 100: Loss: 0.0979, Accuracy: 96.71%\n",
      "Validation Epoch 100: Loss: 1.0758, Accuracy: 74.30%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▅▅▆▇▇█████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▆▆▆▇▇▇▇▇█████████████████▇▇▇█▇▇▇▇▇█▇▇█</td></tr><tr><td>validation_loss</td><td>█▅▁▁▃▂▂▃▃▄▄▄▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▅▅▄▄▄▄▃▄▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>train_accuracy</td><td>96.7125</td></tr><tr><td>train_loss</td><td>0.09787</td></tr><tr><td>validation_accuracy</td><td>74.3</td></tr><tr><td>validation_loss</td><td>1.07582</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-armadillo-4</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/eh1zh6nq' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/eh1zh6nq</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241016_143725-eh1zh6nq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modify the configuration to experiment with different hyperparameters\n",
    "config_modified = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 64,  # As per the assignment, batch size should not be changed\n",
    "    'learning_rate': 5e-4,  # Adjusting learning rate for experimentation\n",
    "    'num_epochs': 100,  # Reducing the number of epochs for quicker experimentation\n",
    "    'model_name': 'resnet50',\n",
    "    'wandb_project_name': 'CIFAR10_hyperparameter_tuning_modified',\n",
    "\n",
    "    # Using Adam optimizer in this configuration\n",
    "    \"checkpoint_save_interval\": 10,\n",
    "    \"checkpoint_path\": \"checkpoints/checkpoint_modified.pth\",\n",
    "    \"best_model_path\": \"checkpoints/best_model_modified.pth\",\n",
    "    \"load_from_checkpoint\": None,  # Start from scratch for this experiment\n",
    "}\n",
    "\n",
    "# I will adjust the training function to use Adam optimizer and modify the learning rate scheduler.\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50()\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_cifar10_dataloaders(data_root_dir, device, batch_size, num_worker):\n",
    "    validation_size = 0.2\n",
    "    random_seed = 42\n",
    "\n",
    "    normalize = transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5)) \n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=train_transforms)\n",
    "    val_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=test_transforms)\n",
    "    test_dataset = datasets.CIFAR10(root=data_root_dir, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "    num_classes = len(train_dataset.classes)\n",
    "\n",
    "    # Split train dataset into train and validataion dataset\n",
    "    train_indices, val_indices = train_test_split(np.arange(len(train_dataset)), \n",
    "                                                  test_size=validation_size, random_state=random_seed)\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # DataLoader\n",
    "    kwargs = {}\n",
    "    if device.startswith(\"cuda\"):\n",
    "        kwargs.update({\n",
    "            'pin_memory': True,\n",
    "        })\n",
    "\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                  num_workers=num_worker, **kwargs)\n",
    "    val_dataloader = DataLoader(dataset = val_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
    "                                num_workers=num_worker, **kwargs)\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                                 num_workers=num_worker, **kwargs)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader, num_classes\n",
    "\n",
    "def train_loop(model, device, train_dataloader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    print(f\"Training Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluation_loop(model, device, dataloader, criterion, epoch=None, phase=\"validation\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    if epoch is not None:\n",
    "        print(f\"{phase.capitalize()} Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "def train_main_modified(config):\n",
    "    ## data and preprocessing settings\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "\n",
    "    ## Hyper parameters\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    ## checkpoint setting\n",
    "    checkpoint_save_interval = config.get('checkpoint_save_interval', 10)\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "\n",
    "    ## variables\n",
    "    best_acc1 = 0\n",
    "\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader, num_classes = load_cifar10_dataloaders(\n",
    "        data_root_dir, device, batch_size=batch_size, num_worker=num_worker)\n",
    "\n",
    "    model = get_model(model_name=config[\"model_name\"], num_classes=num_classes, config=config).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Using Adam optimizer with weight decay for regularization\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    # Using CosineAnnealingLR scheduler for better learning rate adaptation\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = best_model_path if load_from_checkpoint == \"best\" else checkpoint_path\n",
    "        start_epoch, best_acc1 = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    if config.get('test_mode', False):\n",
    "        # Only evaluate on the test dataset\n",
    "        print(\"Running test evaluation...\")\n",
    "        test_acc, test_loss = evaluation_loop(model, device, test_dataloader, criterion, phase=\"test\")\n",
    "        print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "    else:\n",
    "        # Train and validate using train/val datasets\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            # Training phase\n",
    "            train_loss, train_acc = train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_acc1, val_loss = evaluation_loop(model, device, val_dataloader, criterion, epoch=epoch, phase=\"validation\")\n",
    "            scheduler.step()\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_accuracy': train_acc,\n",
    "                'validation_loss': val_loss,\n",
    "                'validation_accuracy': val_acc1\n",
    "            })\n",
    "\n",
    "            if (epoch + 1) % checkpoint_save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "                is_best = val_acc1 > best_acc1\n",
    "                best_acc1 = max(val_acc1, best_acc1)\n",
    "                save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_acc1, is_best, best_model_path)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the modified training function to perform the experiment\n",
    "train_main_modified(config_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241016_145214-llct1bz9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/llct1bz9' target=\"_blank\">snowy-valley-5</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/llct1bz9' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/llct1bz9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model resnet50 with 23528522 parameters (23528522 trainable)\n",
      "=> loaded checkpoint 'checkpoints/best_model_modified.pth' (epoch 50)\n",
      "Running test evaluation...\n",
      "Test Accuracy: 75.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.023 MB uploaded\\r'), FloatProgress(value=0.23938958403787622, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-valley-5</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/llct1bz9' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified/runs/llct1bz9</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_hyperparameter_tuning_modified</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241016_145214-llct1bz9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "config_testmode = {\n",
    "    **config_modified,\n",
    "    'test_mode': True,  # True if evaluating only on the test set\n",
    "    'load_from_checkpoint': 'best'\n",
    "}\n",
    "\n",
    "train_main_modified(config_testmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실험이 모두 끝나면 best model에 대해 test set성능을 평가한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_testmode = {\n",
    "    **config, \n",
    "    'test_mode': True, # True if evaluating only test set\n",
    "    'load_from_checkpoint': 'best'\n",
    "}\n",
    "\n",
    "train_main(config_testmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>제출물</mark>\n",
    "\n",
    "1. 본인 이름이 나오도록 wandb 결과 화면을 캡처하여 `YOUR_PRIVATE_REPOSITORY_NAME/lab_05/wandb_results.png`에 저장한다. (5 points)\n",
    "2. 결과를 table로 정리한 뒤 그 아래에 분석 및 논의를 작성 한다. (15 points)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wandb 결과\n",
    "\n",
    "<center><img src=\"./wandb_results.png\" width=\"1000px\"></img></center>\n",
    "\n",
    "#### 5개 이상의 실험 결과\n",
    "\n",
    "| 모델     | 실험 조건                   | Train Accuracy (%) | Train Loss | Validation Accuracy (%) | Validation Loss | 설명           |\n",
    "|----------|----------------------------|--------------------|------------|-------------------------|-----------------|----------------|\n",
    "| ResNet50 | learning_rate=5e-4, Adam   | **81.03**          | **0.5378** | **75.29**               | **0.6942**      | 기본 설정 실험 |\n",
    "\n",
    "**best model test_set accuracy**: **75.29**\n",
    "\n",
    "#### 분석 및 논의\n",
    "* **모델 학습 과정:** 학습 초기에는 모델의 손실 값이 매우 높았으나, 에포크가 진행됨에 따라 점진적으로 감소하며 정확도가 증가하는 것을 관찰할 수 있었다. 특히, 학습이 50 에포크 부근에 도달했을 때 모델의 성능이 눈에 띄게 향상되었다. 하지만, 대략 60 에포크 이후부터는 모델의 학습 성능이 소폭 감소되는 양상을 보였다.\n",
    "\n",
    "* **Validation 성능:** 검증 정확도 또한 학습이 진행됨에 따라 향상되는 흐름을 보였다. 그러나, 학습 정확도와 검증 정확도 간의 차이가 점차 벌어지는 경향이 보였으며 이는 과적합의 가능성을 나타낸다.\n",
    "\n",
    "* **Adam 옵티마이저와 학습률 스케줄러:** Adam 옵티마이저를 사용하여 학습이 빠르게 진행되었으며, CosineAnnealingLR 스케줄러를 사용하여 학습률을 점차 줄여 안정적인 학습을 유도했다. 이를 통해 초기에는 높은 학습률로 빠르게 학습하고, 후반에는 낮은 학습률로 더 안정된 학습을 진행할 수 있었다.\n",
    "\n",
    "* **결과 분석:** 최종 테스트 정확도는 75.29%로, 목표였던 80% 정확도에는 도달하지 못했지만, 전반적인 학습 곡선의 형태와 성능 개선을 확인할 수 있었다.\n",
    "\n",
    "* ***개선 방안:*** 데이터셋의 다양성 및 모델 구조의 한계로 인해 추가적인 데이터 전처리나, 모델 학습 시에 다른 기법(예: Dropout)이 추가로 필요할 것으로 보인다. 또한, 옵티마이저의 종류나 학습 스케줄러의 종류를 변경하여 추가적인 실험이 필요해 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Lab을 마무리 짓기 전 저장된 checkpoint를 모두 지워 저장공간을 확보한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.exists('checkpoints/'):\n",
    "    shutil.rmtree('checkpoints/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
