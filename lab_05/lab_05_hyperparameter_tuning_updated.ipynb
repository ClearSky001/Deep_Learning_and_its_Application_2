{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2aa2f4a-d69f-443c-ae3d-99091b7ed4dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import wandb\n",
    "\n",
    "from training_utilities import train_loop, evaluation_loop, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0250789f-c01b-4120-a68c-2c8c2c1b8bb8",
   "metadata": {},
   "source": [
    "이번 실습시간에는 다양한 학습 전략과 hyperparameter tuning을 통해 CIFAR-10 테스트셋에서 높은 분류 성능을 얻는 것이 목표이다.\n",
    "\n",
    "<mark>과제</mark> 다양한 조건에서 CIFAR-10 데이터셋 학습을 실험해보고 test 데이터셋에서 80% 이상의 accuracy를 달성하라.\n",
    "\n",
    "* 제출물1 : <u>5개 이상의 학습 커브</u>를 포함하는 wandb 화면 캡처 (wandb 웹페이지의 본인 이름 포함하여 캡처)\n",
    "* 제출물2 : 실험 결과에 대한 분석과 논의 (아래에 markdown으로 기입)\n",
    "\n",
    "참고: 코드에 대한 pytest가 따로 없으므로 자유롭게 코드를 변경하여도 무방함.\n",
    "\n",
    "단, <U>Transfer learning 혹은 Batch size는 변경은 수행하지 말것</U>\n",
    "\n",
    "실험 조건 예시\n",
    "- [Network architectures](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
    "- input normalization\n",
    "- [Weight initialization](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_)\n",
    "- [Optimizers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) (Adam, SGD with momentum, ... )\n",
    "- Regularizations (weight decay, dropout, [Data augmentation](https://pytorch.org/vision/0.9/transforms.html), ensembles, ...)\n",
    "- learning rate & [learning rate scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
    "\n",
    "스스로 neural network를 구축할 경우 아래 사항들을 고려하라\n",
    "- Filter size\n",
    "- Number of filters\n",
    "- Pooling vs Strided Convolution\n",
    "- Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce373e-fa0f-4d3f-a5bc-fe32b2956de7",
   "metadata": {},
   "source": [
    "# 첫번째 모델(ResNet50_first_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60121f69-5782-434f-a382-4d8821f0a504",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33munknownlimitless0301\u001b[0m (\u001b[33munknownlimitless0301-university-of-suwon6591\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241018_230241-lw7bzoas</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/lw7bzoas' target=\"_blank\">ResNet50_first_train</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/lw7bzoas' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/lw7bzoas</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model resnet50 with 23528522 parameters (23528522 trainable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: Loss: 1.8563, Accuracy: 35.45%\n",
      "Validation Epoch 1: Loss: 1.5008, Accuracy: 46.40%\n",
      "Training Epoch 2: Loss: 1.4420, Accuracy: 49.31%\n",
      "Validation Epoch 2: Loss: 1.4008, Accuracy: 50.24%\n",
      "Training Epoch 3: Loss: 1.3728, Accuracy: 52.01%\n",
      "Validation Epoch 3: Loss: 1.3183, Accuracy: 54.47%\n",
      "Training Epoch 4: Loss: 1.2554, Accuracy: 56.02%\n",
      "Validation Epoch 4: Loss: 1.3444, Accuracy: 53.03%\n",
      "Training Epoch 5: Loss: 1.1155, Accuracy: 61.15%\n",
      "Validation Epoch 5: Loss: 1.0975, Accuracy: 61.48%\n",
      "Training Epoch 6: Loss: 1.0150, Accuracy: 64.80%\n",
      "Validation Epoch 6: Loss: 1.0542, Accuracy: 63.60%\n",
      "Training Epoch 7: Loss: 0.8854, Accuracy: 69.33%\n",
      "Validation Epoch 7: Loss: 1.0268, Accuracy: 63.80%\n",
      "Training Epoch 8: Loss: 0.7886, Accuracy: 72.48%\n",
      "Validation Epoch 8: Loss: 0.9087, Accuracy: 68.36%\n",
      "Training Epoch 9: Loss: 0.6714, Accuracy: 76.80%\n",
      "Validation Epoch 9: Loss: 0.9742, Accuracy: 66.79%\n",
      "Training Epoch 10: Loss: 0.7482, Accuracy: 74.58%\n",
      "Validation Epoch 10: Loss: 0.8816, Accuracy: 70.50%\n",
      "Training Epoch 11: Loss: 0.5425, Accuracy: 81.36%\n",
      "Validation Epoch 11: Loss: 0.9021, Accuracy: 71.25%\n",
      "Training Epoch 12: Loss: 0.4523, Accuracy: 84.82%\n",
      "Validation Epoch 12: Loss: 0.8913, Accuracy: 71.91%\n",
      "Training Epoch 13: Loss: 0.3394, Accuracy: 88.42%\n",
      "Validation Epoch 13: Loss: 0.9190, Accuracy: 71.77%\n",
      "Training Epoch 14: Loss: 0.2980, Accuracy: 89.80%\n",
      "Validation Epoch 14: Loss: 0.9461, Accuracy: 72.01%\n",
      "Training Epoch 15: Loss: 0.2684, Accuracy: 90.84%\n",
      "Validation Epoch 15: Loss: 1.0310, Accuracy: 71.06%\n",
      "Training Epoch 16: Loss: 0.2224, Accuracy: 92.52%\n",
      "Validation Epoch 16: Loss: 1.0182, Accuracy: 72.61%\n",
      "Training Epoch 17: Loss: 0.1634, Accuracy: 94.40%\n",
      "Validation Epoch 17: Loss: 1.1205, Accuracy: 71.35%\n",
      "Training Epoch 18: Loss: 0.1475, Accuracy: 94.93%\n",
      "Validation Epoch 18: Loss: 1.1320, Accuracy: 72.01%\n",
      "Training Epoch 19: Loss: 0.1272, Accuracy: 95.67%\n",
      "Validation Epoch 19: Loss: 1.1418, Accuracy: 71.61%\n",
      "Training Epoch 20: Loss: 0.1329, Accuracy: 95.69%\n",
      "Validation Epoch 20: Loss: 1.1489, Accuracy: 72.16%\n",
      "Training Epoch 21: Loss: 0.1122, Accuracy: 96.21%\n",
      "Validation Epoch 21: Loss: 1.3514, Accuracy: 68.82%\n",
      "Training Epoch 22: Loss: 0.1003, Accuracy: 96.67%\n",
      "Validation Epoch 22: Loss: 1.1990, Accuracy: 72.94%\n",
      "Training Epoch 23: Loss: 0.0737, Accuracy: 97.51%\n",
      "Validation Epoch 23: Loss: 1.2405, Accuracy: 72.76%\n",
      "Training Epoch 24: Loss: 0.0774, Accuracy: 97.40%\n",
      "Validation Epoch 24: Loss: 1.1803, Accuracy: 73.34%\n",
      "Training Epoch 25: Loss: 0.0580, Accuracy: 98.05%\n",
      "Validation Epoch 25: Loss: 1.2347, Accuracy: 73.39%\n",
      "Training Epoch 26: Loss: 0.0578, Accuracy: 98.05%\n",
      "Validation Epoch 26: Loss: 1.2391, Accuracy: 73.19%\n",
      "Training Epoch 27: Loss: 0.0461, Accuracy: 98.51%\n",
      "Validation Epoch 27: Loss: 1.2632, Accuracy: 73.58%\n",
      "Training Epoch 28: Loss: 0.0449, Accuracy: 98.58%\n",
      "Validation Epoch 28: Loss: 1.2923, Accuracy: 72.96%\n",
      "Training Epoch 29: Loss: 0.0377, Accuracy: 98.77%\n",
      "Validation Epoch 29: Loss: 1.2583, Accuracy: 73.58%\n",
      "Training Epoch 30: Loss: 0.0281, Accuracy: 99.10%\n",
      "Validation Epoch 30: Loss: 1.3302, Accuracy: 73.58%\n",
      "Training Epoch 31: Loss: 0.0247, Accuracy: 99.23%\n",
      "Validation Epoch 31: Loss: 1.3120, Accuracy: 74.07%\n",
      "Training Epoch 32: Loss: 0.0258, Accuracy: 99.14%\n",
      "Validation Epoch 32: Loss: 1.3378, Accuracy: 73.68%\n",
      "Training Epoch 33: Loss: 0.0169, Accuracy: 99.45%\n",
      "Validation Epoch 33: Loss: 1.3681, Accuracy: 73.89%\n",
      "Training Epoch 34: Loss: 0.0124, Accuracy: 99.61%\n",
      "Validation Epoch 34: Loss: 1.4022, Accuracy: 74.36%\n",
      "Training Epoch 35: Loss: 0.0115, Accuracy: 99.68%\n",
      "Validation Epoch 35: Loss: 1.3682, Accuracy: 74.64%\n",
      "Training Epoch 36: Loss: 0.0091, Accuracy: 99.75%\n",
      "Validation Epoch 36: Loss: 1.4086, Accuracy: 74.51%\n",
      "Training Epoch 37: Loss: 0.0076, Accuracy: 99.77%\n",
      "Validation Epoch 37: Loss: 1.3944, Accuracy: 75.00%\n",
      "Training Epoch 38: Loss: 0.0073, Accuracy: 99.78%\n",
      "Validation Epoch 38: Loss: 1.3704, Accuracy: 74.90%\n",
      "Training Epoch 39: Loss: 0.0039, Accuracy: 99.89%\n",
      "Validation Epoch 39: Loss: 1.3758, Accuracy: 75.21%\n",
      "Training Epoch 40: Loss: 0.0021, Accuracy: 99.94%\n",
      "Validation Epoch 40: Loss: 1.3851, Accuracy: 75.08%\n",
      "Training Epoch 41: Loss: 0.0018, Accuracy: 99.96%\n",
      "Validation Epoch 41: Loss: 1.4044, Accuracy: 75.27%\n",
      "Training Epoch 42: Loss: 0.0014, Accuracy: 99.97%\n",
      "Validation Epoch 42: Loss: 1.4216, Accuracy: 74.89%\n",
      "Training Epoch 43: Loss: 0.0010, Accuracy: 99.98%\n",
      "Validation Epoch 43: Loss: 1.4203, Accuracy: 75.49%\n",
      "Training Epoch 44: Loss: 0.0008, Accuracy: 99.99%\n",
      "Validation Epoch 44: Loss: 1.4056, Accuracy: 75.23%\n",
      "Training Epoch 45: Loss: 0.0005, Accuracy: 100.00%\n",
      "Validation Epoch 45: Loss: 1.3921, Accuracy: 75.32%\n",
      "Training Epoch 46: Loss: 0.0005, Accuracy: 100.00%\n",
      "Validation Epoch 46: Loss: 1.3883, Accuracy: 75.34%\n",
      "Training Epoch 47: Loss: 0.0005, Accuracy: 100.00%\n",
      "Validation Epoch 47: Loss: 1.3834, Accuracy: 75.41%\n",
      "Training Epoch 48: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 48: Loss: 1.3988, Accuracy: 75.54%\n",
      "Training Epoch 49: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 49: Loss: 1.3723, Accuracy: 75.38%\n",
      "Training Epoch 50: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 50: Loss: 1.3941, Accuracy: 75.37%\n",
      "Training Epoch 51: Loss: 0.0003, Accuracy: 100.00%\n",
      "Validation Epoch 51: Loss: 1.3765, Accuracy: 75.48%\n",
      "Training Epoch 52: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 52: Loss: 1.3834, Accuracy: 75.40%\n",
      "Training Epoch 53: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 53: Loss: 1.3780, Accuracy: 75.35%\n",
      "Training Epoch 54: Loss: 0.0003, Accuracy: 100.00%\n",
      "Validation Epoch 54: Loss: 1.3836, Accuracy: 75.34%\n",
      "Training Epoch 55: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 55: Loss: 1.3829, Accuracy: 75.60%\n",
      "Training Epoch 56: Loss: 0.0005, Accuracy: 100.00%\n",
      "Validation Epoch 56: Loss: 1.4035, Accuracy: 75.56%\n",
      "Training Epoch 57: Loss: 0.0004, Accuracy: 100.00%\n",
      "Validation Epoch 57: Loss: 1.3905, Accuracy: 75.53%\n",
      "Training Epoch 58: Loss: 0.0008, Accuracy: 99.99%\n",
      "Validation Epoch 58: Loss: 1.4180, Accuracy: 75.00%\n",
      "Training Epoch 59: Loss: 0.0024, Accuracy: 99.94%\n",
      "Validation Epoch 59: Loss: 1.4035, Accuracy: 75.06%\n",
      "Training Epoch 60: Loss: 0.0028, Accuracy: 99.93%\n",
      "Validation Epoch 60: Loss: 1.4110, Accuracy: 75.03%\n",
      "Training Epoch 61: Loss: 0.0034, Accuracy: 99.90%\n",
      "Validation Epoch 61: Loss: 1.4692, Accuracy: 74.51%\n",
      "Training Epoch 62: Loss: 0.0075, Accuracy: 99.78%\n",
      "Validation Epoch 62: Loss: 1.4153, Accuracy: 74.29%\n",
      "Training Epoch 63: Loss: 0.0072, Accuracy: 99.77%\n",
      "Validation Epoch 63: Loss: 1.4044, Accuracy: 74.87%\n",
      "Training Epoch 64: Loss: 0.0101, Accuracy: 99.70%\n",
      "Validation Epoch 64: Loss: 1.3906, Accuracy: 74.02%\n",
      "Training Epoch 65: Loss: 0.0120, Accuracy: 99.64%\n",
      "Validation Epoch 65: Loss: 1.3945, Accuracy: 74.30%\n",
      "Training Epoch 66: Loss: 0.0154, Accuracy: 99.52%\n",
      "Validation Epoch 66: Loss: 1.3054, Accuracy: 74.74%\n",
      "Training Epoch 67: Loss: 0.0179, Accuracy: 99.47%\n",
      "Validation Epoch 67: Loss: 1.3160, Accuracy: 74.90%\n",
      "Training Epoch 68: Loss: 0.0254, Accuracy: 99.19%\n",
      "Validation Epoch 68: Loss: 1.3071, Accuracy: 73.97%\n",
      "Training Epoch 69: Loss: 0.0252, Accuracy: 99.24%\n",
      "Validation Epoch 69: Loss: 1.3337, Accuracy: 74.04%\n",
      "Training Epoch 70: Loss: 0.0322, Accuracy: 98.95%\n",
      "Validation Epoch 70: Loss: 1.3727, Accuracy: 72.80%\n",
      "Training Epoch 71: Loss: 0.0419, Accuracy: 98.61%\n",
      "Validation Epoch 71: Loss: 1.2772, Accuracy: 73.28%\n",
      "Training Epoch 72: Loss: 0.0412, Accuracy: 98.72%\n",
      "Validation Epoch 72: Loss: 1.2816, Accuracy: 73.88%\n",
      "Training Epoch 73: Loss: 0.0404, Accuracy: 98.69%\n",
      "Validation Epoch 73: Loss: 1.2729, Accuracy: 73.59%\n",
      "Training Epoch 74: Loss: 0.0547, Accuracy: 98.25%\n",
      "Validation Epoch 74: Loss: 1.2916, Accuracy: 72.63%\n",
      "Training Epoch 75: Loss: 0.0596, Accuracy: 98.07%\n",
      "Validation Epoch 75: Loss: 1.2597, Accuracy: 72.65%\n",
      "Training Epoch 76: Loss: 0.0580, Accuracy: 98.08%\n",
      "Validation Epoch 76: Loss: 1.2566, Accuracy: 72.15%\n",
      "Training Epoch 77: Loss: 0.0639, Accuracy: 97.96%\n",
      "Validation Epoch 77: Loss: 1.2108, Accuracy: 73.25%\n",
      "Training Epoch 78: Loss: 0.0698, Accuracy: 97.72%\n",
      "Validation Epoch 78: Loss: 1.2036, Accuracy: 73.37%\n",
      "Training Epoch 79: Loss: 0.0798, Accuracy: 97.39%\n",
      "Validation Epoch 79: Loss: 1.1824, Accuracy: 72.67%\n",
      "Training Epoch 80: Loss: 0.0779, Accuracy: 97.40%\n",
      "Validation Epoch 80: Loss: 1.1706, Accuracy: 73.19%\n",
      "Training Epoch 81: Loss: 0.0861, Accuracy: 97.14%\n",
      "Validation Epoch 81: Loss: 1.1762, Accuracy: 72.96%\n",
      "Training Epoch 82: Loss: 0.0819, Accuracy: 97.32%\n",
      "Validation Epoch 82: Loss: 1.2191, Accuracy: 71.96%\n",
      "Training Epoch 83: Loss: 0.0934, Accuracy: 96.89%\n",
      "Validation Epoch 83: Loss: 1.1678, Accuracy: 72.56%\n",
      "Training Epoch 84: Loss: 0.0913, Accuracy: 97.07%\n",
      "Validation Epoch 84: Loss: 1.1859, Accuracy: 72.64%\n",
      "Training Epoch 85: Loss: 0.0962, Accuracy: 96.78%\n",
      "Validation Epoch 85: Loss: 1.1666, Accuracy: 72.86%\n",
      "Training Epoch 86: Loss: 0.1016, Accuracy: 96.69%\n",
      "Validation Epoch 86: Loss: 1.1151, Accuracy: 72.95%\n",
      "Training Epoch 87: Loss: 0.0989, Accuracy: 96.67%\n",
      "Validation Epoch 87: Loss: 1.1283, Accuracy: 72.65%\n",
      "Training Epoch 88: Loss: 0.1070, Accuracy: 96.42%\n",
      "Validation Epoch 88: Loss: 1.1469, Accuracy: 72.97%\n",
      "Training Epoch 89: Loss: 0.0983, Accuracy: 96.72%\n",
      "Validation Epoch 89: Loss: 1.2349, Accuracy: 71.01%\n",
      "Training Epoch 90: Loss: 0.1046, Accuracy: 96.57%\n",
      "Validation Epoch 90: Loss: 1.1164, Accuracy: 73.92%\n",
      "Training Epoch 91: Loss: 0.1095, Accuracy: 96.32%\n",
      "Validation Epoch 91: Loss: 1.1073, Accuracy: 73.05%\n",
      "Training Epoch 92: Loss: 0.1003, Accuracy: 96.61%\n",
      "Validation Epoch 92: Loss: 1.0987, Accuracy: 73.56%\n",
      "Training Epoch 93: Loss: 0.1022, Accuracy: 96.54%\n",
      "Validation Epoch 93: Loss: 1.0830, Accuracy: 72.86%\n",
      "Training Epoch 94: Loss: 0.1050, Accuracy: 96.49%\n",
      "Validation Epoch 94: Loss: 1.1455, Accuracy: 72.58%\n",
      "Training Epoch 95: Loss: 0.1112, Accuracy: 96.21%\n",
      "Validation Epoch 95: Loss: 1.0946, Accuracy: 73.26%\n",
      "Training Epoch 96: Loss: 0.0955, Accuracy: 96.82%\n",
      "Validation Epoch 96: Loss: 1.1786, Accuracy: 72.16%\n",
      "Training Epoch 97: Loss: 0.1086, Accuracy: 96.38%\n",
      "Validation Epoch 97: Loss: 1.1818, Accuracy: 72.40%\n",
      "Training Epoch 98: Loss: 0.0969, Accuracy: 96.78%\n",
      "Validation Epoch 98: Loss: 1.1222, Accuracy: 72.84%\n",
      "Training Epoch 99: Loss: 0.0903, Accuracy: 97.07%\n",
      "Validation Epoch 99: Loss: 1.0709, Accuracy: 74.37%\n",
      "Training Epoch 100: Loss: 0.0969, Accuracy: 96.79%\n",
      "Validation Epoch 100: Loss: 1.0961, Accuracy: 74.33%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▆▇▇▇████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▅▆▇▇▇▇▆▇▇▇█████████████████▇▇▇▇▇▇▇█▇▇█</td></tr><tr><td>validation_loss</td><td>█▆▃▁▁▁▂▄▆▅▅▆▆▇▇▇▇▇▇▆▇▇▇▇█▇▆▆▅▆▅▄▅▄▄▅▃▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>train_accuracy</td><td>96.7875</td></tr><tr><td>train_loss</td><td>0.09685</td></tr><tr><td>validation_accuracy</td><td>74.33</td></tr><tr><td>validation_loss</td><td>1.09606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_first_train</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/lw7bzoas' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/lw7bzoas</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241018_230241-lw7bzoas/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modify the configuration to experiment with different hyperparameters\n",
    "config_modified = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 64,  # As per the assignment, batch size should not be changed\n",
    "    'learning_rate': 5e-4,  # Adjusting learning rate for experimentation\n",
    "    'num_epochs': 100,  # Reducing the number of epochs for quicker experimentation\n",
    "    'model_name': 'resnet50',\n",
    "    'wandb_project_name': 'CIFAR10_training_with_various_models',\n",
    "\n",
    "    # Using Adam optimizer in this configuration\n",
    "    \"checkpoint_save_interval\": 10,\n",
    "    \"checkpoint_path\": \"checkpoints/checkpoint_modified.pth\",\n",
    "    \"best_model_path\": \"checkpoints/best_model_modified.pth\",\n",
    "    \"load_from_checkpoint\": None,  # Start from scratch for this experiment\n",
    "}\n",
    "\n",
    "# I will adjust the training function to use Adam optimizer and modify the learning rate scheduler.\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50()\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_cifar10_dataloaders(data_root_dir, device, batch_size, num_worker):\n",
    "    validation_size = 0.2\n",
    "    random_seed = 42\n",
    "\n",
    "    normalize = transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5)) \n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=train_transforms)\n",
    "    val_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=test_transforms)\n",
    "    test_dataset = datasets.CIFAR10(root=data_root_dir, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "    num_classes = len(train_dataset.classes)\n",
    "\n",
    "    # Split train dataset into train and validataion dataset\n",
    "    train_indices, val_indices = train_test_split(np.arange(len(train_dataset)), \n",
    "                                                  test_size=validation_size, random_state=random_seed)\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # DataLoader\n",
    "    kwargs = {}\n",
    "    if device.startswith(\"cuda\"):\n",
    "        kwargs.update({\n",
    "            'pin_memory': True,\n",
    "        })\n",
    "\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                  num_workers=num_worker, **kwargs)\n",
    "    val_dataloader = DataLoader(dataset = val_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
    "                                num_workers=num_worker, **kwargs)\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                                 num_workers=num_worker, **kwargs)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader, num_classes\n",
    "\n",
    "def train_loop(model, device, train_dataloader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    print(f\"Training Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluation_loop(model, device, dataloader, criterion, epoch=None, phase=\"validation\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    if epoch is not None:\n",
    "        print(f\"{phase.capitalize()} Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "def train_main_modified(config):\n",
    "    ## data and preprocessing settings\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "\n",
    "    ## Hyper parameters\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    ## checkpoint setting\n",
    "    checkpoint_save_interval = config.get('checkpoint_save_interval', 10)\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "\n",
    "    ## variables\n",
    "    best_acc1 = 0\n",
    "\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config,\n",
    "        name=\"ResNet50_first_train\"\n",
    "    )\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader, num_classes = load_cifar10_dataloaders(\n",
    "        data_root_dir, device, batch_size=batch_size, num_worker=num_worker)\n",
    "\n",
    "    model = get_model(model_name=config[\"model_name\"], num_classes=num_classes, config=config).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Using Adam optimizer with weight decay for regularization\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    # Using CosineAnnealingLR scheduler for better learning rate adaptation\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = best_model_path if load_from_checkpoint == \"best\" else checkpoint_path\n",
    "        start_epoch, best_acc1 = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    if config.get('test_mode', False):\n",
    "        # Only evaluate on the test dataset\n",
    "        print(\"Running test evaluation...\")\n",
    "        test_acc, test_loss = evaluation_loop(model, device, test_dataloader, criterion, phase=\"test\")\n",
    "        print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "    else:\n",
    "        # Train and validate using train/val datasets\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            # Training phase\n",
    "            train_loss, train_acc = train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_acc1, val_loss = evaluation_loop(model, device, val_dataloader, criterion, epoch=epoch, phase=\"validation\")\n",
    "            scheduler.step()\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_accuracy': train_acc,\n",
    "                'validation_loss': val_loss,\n",
    "                'validation_accuracy': val_acc1\n",
    "            })\n",
    "\n",
    "            if (epoch + 1) % checkpoint_save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "                is_best = val_acc1 > best_acc1\n",
    "                best_acc1 = max(val_acc1, best_acc1)\n",
    "                save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_acc1, is_best, best_model_path)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the modified training function to perform the experiment\n",
    "train_main_modified(config_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc02d4-fb01-4a35-acb4-65ab8414def6",
   "metadata": {},
   "source": [
    "## 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bb7a7a4-77fa-4e3b-981f-606088e87cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241018_232805-srv6vlri</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/srv6vlri' target=\"_blank\">ResNet50_first_train</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/srv6vlri' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/srv6vlri</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model resnet50 with 23528522 parameters (23528522 trainable)\n",
      "=> loaded checkpoint 'checkpoints/best_model_modified.pth' (epoch 50)\n",
      "Running test evaluation...\n",
      "Test Accuracy: 74.93\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_first_train</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/srv6vlri' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/srv6vlri</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241018_232805-srv6vlri/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "config_testmode = {\n",
    "    **config_modified,\n",
    "    'test_mode': True,  # True if evaluating only on the test set\n",
    "    'load_from_checkpoint': 'best'\n",
    "}\n",
    "\n",
    "train_main_modified(config_testmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10967c-2859-4f3b-9d1c-346c0df4b832",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 두번째 모델(ResNet50_model_second_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9acb9339-6d5a-4caf-b64f-2e96d130ee3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/offline-run-20241113_093809-kni32ogg<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20241113_093809-kni32ogg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33munknownlimitless0301\u001b[0m (\u001b[33munknownlimitless0301-university-of-suwon6591\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241113_094330-1iivam3z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/1iivam3z' target=\"_blank\">ResNet50_model_second_train</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/1iivam3z' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/1iivam3z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model resnet50 with 24562250 parameters (24562250 trainable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: Loss: 2.2207, Accuracy: 15.93%\n",
      "Validation Epoch 1: Loss: 2.0510, Accuracy: 23.16%\n",
      "Training Epoch 2: Loss: 1.9817, Accuracy: 25.68%\n",
      "Validation Epoch 2: Loss: 1.8194, Accuracy: 32.34%\n",
      "Training Epoch 3: Loss: 1.8209, Accuracy: 31.92%\n",
      "Validation Epoch 3: Loss: 1.7006, Accuracy: 35.75%\n",
      "Training Epoch 4: Loss: 1.7346, Accuracy: 35.30%\n",
      "Validation Epoch 4: Loss: 1.6252, Accuracy: 39.39%\n",
      "Training Epoch 5: Loss: 1.6678, Accuracy: 37.96%\n",
      "Validation Epoch 5: Loss: 1.5842, Accuracy: 40.84%\n",
      "Training Epoch 6: Loss: 1.6107, Accuracy: 40.23%\n",
      "Validation Epoch 6: Loss: 1.5049, Accuracy: 43.89%\n",
      "Training Epoch 7: Loss: 1.5675, Accuracy: 42.27%\n",
      "Validation Epoch 7: Loss: 1.4611, Accuracy: 45.49%\n",
      "Training Epoch 8: Loss: 1.5258, Accuracy: 43.79%\n",
      "Validation Epoch 8: Loss: 1.4178, Accuracy: 47.56%\n",
      "Training Epoch 9: Loss: 1.4858, Accuracy: 45.75%\n",
      "Validation Epoch 9: Loss: 1.3905, Accuracy: 48.72%\n",
      "Training Epoch 10: Loss: 1.4482, Accuracy: 46.91%\n",
      "Validation Epoch 10: Loss: 1.3513, Accuracy: 50.41%\n",
      "Training Epoch 11: Loss: 1.4231, Accuracy: 48.38%\n",
      "Validation Epoch 11: Loss: 1.3236, Accuracy: 52.19%\n",
      "Training Epoch 12: Loss: 1.3915, Accuracy: 49.73%\n",
      "Validation Epoch 12: Loss: 1.2859, Accuracy: 52.92%\n",
      "Training Epoch 13: Loss: 1.3617, Accuracy: 50.91%\n",
      "Validation Epoch 13: Loss: 1.2629, Accuracy: 53.86%\n",
      "Training Epoch 14: Loss: 1.3328, Accuracy: 51.77%\n",
      "Validation Epoch 14: Loss: 1.2334, Accuracy: 55.42%\n",
      "Training Epoch 15: Loss: 1.3083, Accuracy: 52.99%\n",
      "Validation Epoch 15: Loss: 1.2061, Accuracy: 56.03%\n",
      "Training Epoch 16: Loss: 1.2819, Accuracy: 54.05%\n",
      "Validation Epoch 16: Loss: 1.2003, Accuracy: 56.71%\n",
      "Training Epoch 17: Loss: 1.2644, Accuracy: 55.09%\n",
      "Validation Epoch 17: Loss: 1.1817, Accuracy: 57.50%\n",
      "Training Epoch 18: Loss: 1.2405, Accuracy: 55.76%\n",
      "Validation Epoch 18: Loss: 1.1481, Accuracy: 58.52%\n",
      "Training Epoch 19: Loss: 1.2158, Accuracy: 56.46%\n",
      "Validation Epoch 19: Loss: 1.1497, Accuracy: 59.09%\n",
      "Training Epoch 20: Loss: 1.1938, Accuracy: 57.33%\n",
      "Validation Epoch 20: Loss: 1.1272, Accuracy: 59.38%\n",
      "Training Epoch 21: Loss: 1.1700, Accuracy: 58.50%\n",
      "Validation Epoch 21: Loss: 1.1386, Accuracy: 59.06%\n",
      "Training Epoch 22: Loss: 1.1478, Accuracy: 59.30%\n",
      "Validation Epoch 22: Loss: 1.0770, Accuracy: 61.45%\n",
      "Training Epoch 23: Loss: 1.1279, Accuracy: 59.77%\n",
      "Validation Epoch 23: Loss: 1.0615, Accuracy: 62.10%\n",
      "Training Epoch 24: Loss: 1.1152, Accuracy: 60.52%\n",
      "Validation Epoch 24: Loss: 1.0554, Accuracy: 62.23%\n",
      "Training Epoch 25: Loss: 1.0907, Accuracy: 61.38%\n",
      "Validation Epoch 25: Loss: 1.0199, Accuracy: 63.89%\n",
      "Training Epoch 26: Loss: 1.0751, Accuracy: 61.79%\n",
      "Validation Epoch 26: Loss: 1.0170, Accuracy: 64.15%\n",
      "Training Epoch 27: Loss: 1.0577, Accuracy: 62.57%\n",
      "Validation Epoch 27: Loss: 0.9997, Accuracy: 64.58%\n",
      "Training Epoch 28: Loss: 1.0413, Accuracy: 62.95%\n",
      "Validation Epoch 28: Loss: 1.0012, Accuracy: 64.82%\n",
      "Training Epoch 29: Loss: 1.0223, Accuracy: 63.82%\n",
      "Validation Epoch 29: Loss: 0.9896, Accuracy: 64.62%\n",
      "Training Epoch 30: Loss: 1.0061, Accuracy: 64.41%\n",
      "Validation Epoch 30: Loss: 0.9512, Accuracy: 66.29%\n",
      "Training Epoch 31: Loss: 0.9969, Accuracy: 64.77%\n",
      "Validation Epoch 31: Loss: 0.9663, Accuracy: 65.85%\n",
      "Training Epoch 32: Loss: 0.9782, Accuracy: 65.49%\n",
      "Validation Epoch 32: Loss: 0.9445, Accuracy: 65.99%\n",
      "Training Epoch 33: Loss: 0.9657, Accuracy: 65.88%\n",
      "Validation Epoch 33: Loss: 0.9174, Accuracy: 67.48%\n",
      "Training Epoch 34: Loss: 0.9511, Accuracy: 66.59%\n",
      "Validation Epoch 34: Loss: 0.9193, Accuracy: 67.49%\n",
      "Training Epoch 35: Loss: 0.9364, Accuracy: 66.88%\n",
      "Validation Epoch 35: Loss: 0.9010, Accuracy: 67.78%\n",
      "Training Epoch 36: Loss: 0.9162, Accuracy: 67.42%\n",
      "Validation Epoch 36: Loss: 0.8784, Accuracy: 69.26%\n",
      "Training Epoch 37: Loss: 0.9039, Accuracy: 68.08%\n",
      "Validation Epoch 37: Loss: 0.9002, Accuracy: 68.56%\n",
      "Training Epoch 38: Loss: 0.8874, Accuracy: 68.50%\n",
      "Validation Epoch 38: Loss: 0.8672, Accuracy: 69.55%\n",
      "Training Epoch 39: Loss: 0.8748, Accuracy: 69.07%\n",
      "Validation Epoch 39: Loss: 0.8811, Accuracy: 68.92%\n",
      "Training Epoch 40: Loss: 0.8643, Accuracy: 69.35%\n",
      "Validation Epoch 40: Loss: 0.8672, Accuracy: 69.51%\n",
      "Training Epoch 41: Loss: 0.8529, Accuracy: 69.98%\n",
      "Validation Epoch 41: Loss: 0.8563, Accuracy: 70.23%\n",
      "Training Epoch 42: Loss: 0.8353, Accuracy: 70.53%\n",
      "Validation Epoch 42: Loss: 0.8613, Accuracy: 70.12%\n",
      "Training Epoch 43: Loss: 0.8392, Accuracy: 70.39%\n",
      "Validation Epoch 43: Loss: 0.8696, Accuracy: 69.77%\n",
      "Training Epoch 44: Loss: 0.8195, Accuracy: 70.98%\n",
      "Validation Epoch 44: Loss: 0.8497, Accuracy: 70.20%\n",
      "Training Epoch 45: Loss: 0.8070, Accuracy: 71.25%\n",
      "Validation Epoch 45: Loss: 0.8501, Accuracy: 70.74%\n",
      "Training Epoch 46: Loss: 0.7985, Accuracy: 71.93%\n",
      "Validation Epoch 46: Loss: 0.8137, Accuracy: 71.44%\n",
      "Training Epoch 47: Loss: 0.7890, Accuracy: 72.10%\n",
      "Validation Epoch 47: Loss: 0.8342, Accuracy: 71.02%\n",
      "Training Epoch 48: Loss: 0.7733, Accuracy: 72.71%\n",
      "Validation Epoch 48: Loss: 0.8160, Accuracy: 71.68%\n",
      "Training Epoch 49: Loss: 0.7587, Accuracy: 73.09%\n",
      "Validation Epoch 49: Loss: 0.8210, Accuracy: 71.85%\n",
      "Training Epoch 50: Loss: 0.7513, Accuracy: 73.40%\n",
      "Validation Epoch 50: Loss: 0.8172, Accuracy: 71.44%\n",
      "Training Epoch 51: Loss: 0.7414, Accuracy: 73.72%\n",
      "Validation Epoch 51: Loss: 0.8033, Accuracy: 71.95%\n",
      "Training Epoch 52: Loss: 0.7275, Accuracy: 74.27%\n",
      "Validation Epoch 52: Loss: 0.7994, Accuracy: 72.47%\n",
      "Training Epoch 53: Loss: 0.7247, Accuracy: 74.47%\n",
      "Validation Epoch 53: Loss: 0.8005, Accuracy: 72.17%\n",
      "Training Epoch 54: Loss: 0.7137, Accuracy: 74.84%\n",
      "Validation Epoch 54: Loss: 0.8039, Accuracy: 72.45%\n",
      "Training Epoch 55: Loss: 0.7099, Accuracy: 75.00%\n",
      "Validation Epoch 55: Loss: 0.8044, Accuracy: 72.61%\n",
      "Training Epoch 56: Loss: 0.6994, Accuracy: 75.29%\n",
      "Validation Epoch 56: Loss: 0.7871, Accuracy: 73.30%\n",
      "Training Epoch 57: Loss: 0.6869, Accuracy: 75.54%\n",
      "Validation Epoch 57: Loss: 0.7885, Accuracy: 72.80%\n",
      "Training Epoch 58: Loss: 0.6838, Accuracy: 75.82%\n",
      "Validation Epoch 58: Loss: 0.7862, Accuracy: 72.69%\n",
      "Training Epoch 59: Loss: 0.6756, Accuracy: 76.23%\n",
      "Validation Epoch 59: Loss: 0.7650, Accuracy: 73.70%\n",
      "Training Epoch 60: Loss: 0.6632, Accuracy: 76.57%\n",
      "Validation Epoch 60: Loss: 0.7837, Accuracy: 73.28%\n",
      "Training Epoch 61: Loss: 0.6609, Accuracy: 76.73%\n",
      "Validation Epoch 61: Loss: 0.7835, Accuracy: 73.18%\n",
      "Training Epoch 62: Loss: 0.6436, Accuracy: 77.34%\n",
      "Validation Epoch 62: Loss: 0.7651, Accuracy: 73.57%\n",
      "Training Epoch 63: Loss: 0.6392, Accuracy: 77.34%\n",
      "Validation Epoch 63: Loss: 0.7819, Accuracy: 73.48%\n",
      "Training Epoch 64: Loss: 0.6288, Accuracy: 77.83%\n",
      "Validation Epoch 64: Loss: 0.7729, Accuracy: 73.99%\n",
      "Training Epoch 65: Loss: 0.6294, Accuracy: 77.66%\n",
      "Validation Epoch 65: Loss: 0.7739, Accuracy: 73.45%\n",
      "Training Epoch 66: Loss: 0.6168, Accuracy: 78.17%\n",
      "Validation Epoch 66: Loss: 0.7516, Accuracy: 74.85%\n",
      "Training Epoch 67: Loss: 0.6072, Accuracy: 78.22%\n",
      "Validation Epoch 67: Loss: 0.7673, Accuracy: 73.99%\n",
      "Training Epoch 68: Loss: 0.5998, Accuracy: 79.08%\n",
      "Validation Epoch 68: Loss: 0.7748, Accuracy: 73.94%\n",
      "Training Epoch 69: Loss: 0.5988, Accuracy: 78.93%\n",
      "Validation Epoch 69: Loss: 0.7572, Accuracy: 74.90%\n",
      "Training Epoch 70: Loss: 0.5867, Accuracy: 79.41%\n",
      "Validation Epoch 70: Loss: 0.7606, Accuracy: 74.97%\n",
      "Training Epoch 71: Loss: 0.5791, Accuracy: 79.50%\n",
      "Validation Epoch 71: Loss: 0.7759, Accuracy: 74.05%\n",
      "Training Epoch 72: Loss: 0.5692, Accuracy: 79.77%\n",
      "Validation Epoch 72: Loss: 0.7972, Accuracy: 74.44%\n",
      "Training Epoch 73: Loss: 0.5702, Accuracy: 79.88%\n",
      "Validation Epoch 73: Loss: 0.7555, Accuracy: 75.35%\n",
      "Training Epoch 74: Loss: 0.5559, Accuracy: 80.34%\n",
      "Validation Epoch 74: Loss: 0.7627, Accuracy: 74.92%\n",
      "Training Epoch 75: Loss: 0.5512, Accuracy: 80.36%\n",
      "Validation Epoch 75: Loss: 0.7586, Accuracy: 75.39%\n",
      "Training Epoch 76: Loss: 0.5472, Accuracy: 80.53%\n",
      "Validation Epoch 76: Loss: 0.7601, Accuracy: 75.24%\n",
      "Training Epoch 77: Loss: 0.5454, Accuracy: 80.58%\n",
      "Validation Epoch 77: Loss: 0.7513, Accuracy: 75.09%\n",
      "Training Epoch 78: Loss: 0.5379, Accuracy: 81.00%\n",
      "Validation Epoch 78: Loss: 0.7518, Accuracy: 75.72%\n",
      "Training Epoch 79: Loss: 0.5286, Accuracy: 81.25%\n",
      "Validation Epoch 79: Loss: 0.7667, Accuracy: 74.65%\n",
      "Training Epoch 80: Loss: 0.5206, Accuracy: 81.69%\n",
      "Validation Epoch 80: Loss: 0.7522, Accuracy: 75.70%\n",
      "Training Epoch 81: Loss: 0.5127, Accuracy: 81.91%\n",
      "Validation Epoch 81: Loss: 0.7525, Accuracy: 75.79%\n",
      "Training Epoch 82: Loss: 0.5106, Accuracy: 81.89%\n",
      "Validation Epoch 82: Loss: 0.7427, Accuracy: 76.52%\n",
      "Training Epoch 83: Loss: 0.4983, Accuracy: 82.31%\n",
      "Validation Epoch 83: Loss: 0.7518, Accuracy: 75.60%\n",
      "Training Epoch 84: Loss: 0.4967, Accuracy: 82.63%\n",
      "Validation Epoch 84: Loss: 0.7688, Accuracy: 75.60%\n",
      "Training Epoch 85: Loss: 0.4892, Accuracy: 82.84%\n",
      "Validation Epoch 85: Loss: 0.7421, Accuracy: 76.51%\n",
      "Training Epoch 86: Loss: 0.4794, Accuracy: 82.89%\n",
      "Validation Epoch 86: Loss: 0.7513, Accuracy: 75.98%\n",
      "Training Epoch 87: Loss: 0.4732, Accuracy: 83.28%\n",
      "Validation Epoch 87: Loss: 0.7585, Accuracy: 76.15%\n",
      "Training Epoch 88: Loss: 0.4741, Accuracy: 83.22%\n",
      "Validation Epoch 88: Loss: 0.7758, Accuracy: 75.26%\n",
      "Training Epoch 89: Loss: 0.4728, Accuracy: 83.12%\n",
      "Validation Epoch 89: Loss: 0.7382, Accuracy: 76.13%\n",
      "Training Epoch 90: Loss: 0.4580, Accuracy: 83.86%\n",
      "Validation Epoch 90: Loss: 0.7522, Accuracy: 76.12%\n",
      "Training Epoch 91: Loss: 0.4538, Accuracy: 83.88%\n",
      "Validation Epoch 91: Loss: 0.7565, Accuracy: 76.29%\n",
      "Training Epoch 92: Loss: 0.4500, Accuracy: 84.05%\n",
      "Validation Epoch 92: Loss: 0.7335, Accuracy: 77.02%\n",
      "Training Epoch 93: Loss: 0.4470, Accuracy: 84.18%\n",
      "Validation Epoch 93: Loss: 0.7554, Accuracy: 76.23%\n",
      "Training Epoch 94: Loss: 0.4361, Accuracy: 84.62%\n",
      "Validation Epoch 94: Loss: 0.7555, Accuracy: 76.05%\n",
      "Training Epoch 95: Loss: 0.4379, Accuracy: 84.41%\n",
      "Validation Epoch 95: Loss: 0.7416, Accuracy: 76.40%\n",
      "Training Epoch 96: Loss: 0.4320, Accuracy: 84.64%\n",
      "Validation Epoch 96: Loss: 0.7379, Accuracy: 77.05%\n",
      "Training Epoch 97: Loss: 0.4240, Accuracy: 85.08%\n",
      "Validation Epoch 97: Loss: 0.7675, Accuracy: 76.64%\n",
      "Training Epoch 98: Loss: 0.4199, Accuracy: 85.00%\n",
      "Validation Epoch 98: Loss: 0.7537, Accuracy: 76.42%\n",
      "Training Epoch 99: Loss: 0.4129, Accuracy: 85.48%\n",
      "Validation Epoch 99: Loss: 0.7523, Accuracy: 77.19%\n",
      "Training Epoch 100: Loss: 0.4031, Accuracy: 85.68%\n",
      "Validation Epoch 100: Loss: 0.7977, Accuracy: 75.98%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>validation_loss</td><td>█▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>train_accuracy</td><td>85.6825</td></tr><tr><td>train_loss</td><td>0.40311</td></tr><tr><td>validation_accuracy</td><td>75.98</td></tr><tr><td>validation_loss</td><td>0.79772</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_model_second_train</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/1iivam3z' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/1iivam3z</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241113_094330-1iivam3z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration for improved experiment\n",
    "config_modified = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 64,  # Fixed as per assignment\n",
    "    'learning_rate': 1e-3,  # Higher initial learning rate\n",
    "    'num_epochs': 100,\n",
    "    'model_name': 'resnet50',\n",
    "    'wandb_project_name': 'CIFAR10_training_with_various_models',\n",
    "    \"checkpoint_save_interval\": 10,\n",
    "    \"checkpoint_path\": \"checkpoints/checkpoint_modified.pth\",\n",
    "    \"best_model_path\": \"checkpoints/best_model_modified.pth\",\n",
    "    \"load_from_checkpoint\": None,\n",
    "}\n",
    "\n",
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50()\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Linear(model.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "    return model\n",
    "\n",
    "def load_cifar10_dataloaders(data_root_dir, device, batch_size, num_worker):\n",
    "    validation_size = 0.2\n",
    "    random_seed = 42\n",
    "    normalize = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "\n",
    "    # Data Augmentation for training set\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    test_transforms = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=train_transforms)\n",
    "    val_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=test_transforms)\n",
    "    test_dataset = datasets.CIFAR10(root=data_root_dir, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    train_indices, val_indices = train_test_split(np.arange(len(train_dataset)), test_size=validation_size, random_state=random_seed)\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    kwargs = {'pin_memory': True} if device.startswith(\"cuda\") else {}\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_worker, **kwargs)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_worker, **kwargs)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_worker, **kwargs)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader, num_classes\n",
    "\n",
    "def train_loop(model, device, train_dataloader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Training Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluation_loop(model, device, dataloader, criterion, epoch=None, phase=\"validation\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    if epoch is not None:\n",
    "        print(f\"{phase.capitalize()} Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "def train_main_modified(config):\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "    checkpoint_save_interval = config.get('checkpoint_save_interval', 10)\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "    best_acc1 = 0\n",
    "\n",
    "    wandb.finish()\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config,\n",
    "        name=\"ResNet50_model_second_train\"\n",
    "    )\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader, num_classes = load_cifar10_dataloaders(\n",
    "        data_root_dir, device, batch_size=batch_size, num_worker=num_worker)\n",
    "    model = get_model(model_name=config[\"model_name\"], num_classes=num_classes, config=config).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Using SGD with momentum\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = best_model_path if load_from_checkpoint == \"best\" else checkpoint_path\n",
    "        start_epoch, best_acc1 = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    if config.get('test_mode', False):\n",
    "        print(\"Running test evaluation...\")\n",
    "        test_acc, test_loss = evaluation_loop(model, device, test_dataloader, criterion, phase=\"test\")\n",
    "        print(f\"Test Accuracy: {test_acc}\")\n",
    "    else:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            train_loss, train_acc = train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "            val_acc1, val_loss = evaluation_loop(model, device, val_dataloader, criterion, epoch=epoch, phase=\"validation\")\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_accuracy': train_acc,\n",
    "                'validation_loss': val_loss,\n",
    "                'validation_accuracy': val_acc1\n",
    "            })\n",
    "\n",
    "            if (epoch + 1) % checkpoint_save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "                is_best = val_acc1 > best_acc1\n",
    "                best_acc1 = max(val_acc1, best_acc1)\n",
    "                save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_acc1, is_best, best_model_path)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the modified training function to perform the experiment\n",
    "train_main_modified(config_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577519b7-5a33-4d00-bf22-bf3179954dcf",
   "metadata": {},
   "source": [
    "## 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ad4713-b22c-4cb7-a08b-baf2a8a28fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241113_095638-p53kh7su</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/p53kh7su' target=\"_blank\">ResNet50_model_second_train</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/p53kh7su' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/p53kh7su</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model resnet50 with 24562250 parameters (24562250 trainable)\n",
      "=> loaded checkpoint 'checkpoints/best_model_modified.pth' (epoch 90)\n",
      "Running test evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_model_second_train</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/p53kh7su' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/p53kh7su</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241113_095638-p53kh7su/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "config_testmode = {\n",
    "    **config_modified,\n",
    "    'test_mode': True,  # True if evaluating only on the test set\n",
    "    'load_from_checkpoint': 'best'\n",
    "}\n",
    "\n",
    "train_main_modified(config_testmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59070c5-aee6-4423-8f84-de29a30baf91",
   "metadata": {},
   "source": [
    "# 세 번째 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b4c9906-bfde-43b4-b36f-140355aa17ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241113_110915-6manek5w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/6manek5w' target=\"_blank\">ResNet50_model_3rd_train</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/6manek5w' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/6manek5w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model resnet50 with 24562250 parameters (24562250 trainable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: Loss: 2.1913, Accuracy: 16.98%\n",
      "Validation Epoch 1: Loss: 1.9664, Accuracy: 26.28%\n",
      "Training Epoch 2: Loss: 1.9172, Accuracy: 27.75%\n",
      "Validation Epoch 2: Loss: 1.7702, Accuracy: 33.29%\n",
      "Training Epoch 3: Loss: 1.7872, Accuracy: 33.19%\n",
      "Validation Epoch 3: Loss: 1.6731, Accuracy: 35.79%\n",
      "Training Epoch 4: Loss: 1.7009, Accuracy: 36.64%\n",
      "Validation Epoch 4: Loss: 1.5841, Accuracy: 41.05%\n",
      "Training Epoch 5: Loss: 1.6283, Accuracy: 40.12%\n",
      "Validation Epoch 5: Loss: 1.5244, Accuracy: 43.75%\n",
      "Training Epoch 6: Loss: 1.5725, Accuracy: 42.23%\n",
      "Validation Epoch 6: Loss: 1.4624, Accuracy: 45.78%\n",
      "Training Epoch 7: Loss: 1.5261, Accuracy: 44.10%\n",
      "Validation Epoch 7: Loss: 1.4323, Accuracy: 46.72%\n",
      "Training Epoch 8: Loss: 1.4905, Accuracy: 45.75%\n",
      "Validation Epoch 8: Loss: 1.3726, Accuracy: 49.79%\n",
      "Training Epoch 9: Loss: 1.4499, Accuracy: 47.03%\n",
      "Validation Epoch 9: Loss: 1.3411, Accuracy: 51.22%\n",
      "Training Epoch 10: Loss: 1.4151, Accuracy: 48.73%\n",
      "Validation Epoch 10: Loss: 1.3025, Accuracy: 52.60%\n",
      "Training Epoch 11: Loss: 1.3805, Accuracy: 49.81%\n",
      "Validation Epoch 11: Loss: 1.2730, Accuracy: 53.54%\n",
      "Training Epoch 12: Loss: 1.3497, Accuracy: 51.48%\n",
      "Validation Epoch 12: Loss: 1.2614, Accuracy: 54.24%\n",
      "Training Epoch 13: Loss: 1.3216, Accuracy: 52.48%\n",
      "Validation Epoch 13: Loss: 1.2253, Accuracy: 55.31%\n",
      "Training Epoch 14: Loss: 1.2917, Accuracy: 53.93%\n",
      "Validation Epoch 14: Loss: 1.2024, Accuracy: 56.44%\n",
      "Training Epoch 15: Loss: 1.2667, Accuracy: 54.74%\n",
      "Validation Epoch 15: Loss: 1.1705, Accuracy: 57.30%\n",
      "Training Epoch 16: Loss: 1.2466, Accuracy: 56.03%\n",
      "Validation Epoch 16: Loss: 1.2139, Accuracy: 56.65%\n",
      "Training Epoch 17: Loss: 1.2195, Accuracy: 56.72%\n",
      "Validation Epoch 17: Loss: 1.1418, Accuracy: 58.53%\n",
      "Training Epoch 18: Loss: 1.1997, Accuracy: 57.43%\n",
      "Validation Epoch 18: Loss: 1.1100, Accuracy: 59.86%\n",
      "Training Epoch 19: Loss: 1.1754, Accuracy: 58.40%\n",
      "Validation Epoch 19: Loss: 1.1061, Accuracy: 59.76%\n",
      "Training Epoch 20: Loss: 1.1543, Accuracy: 59.23%\n",
      "Validation Epoch 20: Loss: 1.0851, Accuracy: 60.47%\n",
      "Training Epoch 21: Loss: 1.1309, Accuracy: 59.80%\n",
      "Validation Epoch 21: Loss: 1.0777, Accuracy: 61.47%\n",
      "Training Epoch 22: Loss: 1.1159, Accuracy: 60.65%\n",
      "Validation Epoch 22: Loss: 1.0337, Accuracy: 63.18%\n",
      "Training Epoch 23: Loss: 1.0935, Accuracy: 61.48%\n",
      "Validation Epoch 23: Loss: 1.0187, Accuracy: 63.40%\n",
      "Training Epoch 24: Loss: 1.0745, Accuracy: 61.83%\n",
      "Validation Epoch 24: Loss: 1.0134, Accuracy: 64.02%\n",
      "Training Epoch 25: Loss: 1.0538, Accuracy: 62.83%\n",
      "Validation Epoch 25: Loss: 1.0079, Accuracy: 64.02%\n",
      "Training Epoch 26: Loss: 1.0353, Accuracy: 63.25%\n",
      "Validation Epoch 26: Loss: 0.9749, Accuracy: 65.28%\n",
      "Training Epoch 27: Loss: 1.0158, Accuracy: 64.26%\n",
      "Validation Epoch 27: Loss: 0.9815, Accuracy: 65.27%\n",
      "Training Epoch 28: Loss: 1.0024, Accuracy: 64.57%\n",
      "Validation Epoch 28: Loss: 0.9541, Accuracy: 66.17%\n",
      "Training Epoch 29: Loss: 0.9854, Accuracy: 65.12%\n",
      "Validation Epoch 29: Loss: 0.9677, Accuracy: 65.80%\n",
      "Training Epoch 30: Loss: 0.9768, Accuracy: 65.61%\n",
      "Validation Epoch 30: Loss: 0.9255, Accuracy: 66.98%\n",
      "Training Epoch 31: Loss: 0.9553, Accuracy: 66.32%\n",
      "Validation Epoch 31: Loss: 0.9452, Accuracy: 66.19%\n",
      "Training Epoch 32: Loss: 0.9394, Accuracy: 66.94%\n",
      "Validation Epoch 32: Loss: 0.9141, Accuracy: 67.82%\n",
      "Training Epoch 33: Loss: 0.9256, Accuracy: 67.32%\n",
      "Validation Epoch 33: Loss: 0.8995, Accuracy: 68.28%\n",
      "Training Epoch 34: Loss: 0.9081, Accuracy: 68.20%\n",
      "Validation Epoch 34: Loss: 0.8857, Accuracy: 68.47%\n",
      "Training Epoch 35: Loss: 0.8949, Accuracy: 68.42%\n",
      "Validation Epoch 35: Loss: 0.8780, Accuracy: 68.45%\n",
      "Training Epoch 36: Loss: 0.8797, Accuracy: 69.06%\n",
      "Validation Epoch 36: Loss: 0.8745, Accuracy: 69.11%\n",
      "Training Epoch 37: Loss: 0.8642, Accuracy: 69.58%\n",
      "Validation Epoch 37: Loss: 0.8712, Accuracy: 69.15%\n",
      "Training Epoch 38: Loss: 0.8535, Accuracy: 69.82%\n",
      "Validation Epoch 38: Loss: 0.8535, Accuracy: 70.40%\n",
      "Training Epoch 39: Loss: 0.8426, Accuracy: 70.20%\n",
      "Validation Epoch 39: Loss: 0.8356, Accuracy: 71.14%\n",
      "Training Epoch 40: Loss: 0.8256, Accuracy: 70.93%\n",
      "Validation Epoch 40: Loss: 0.8228, Accuracy: 71.07%\n",
      "Training Epoch 41: Loss: 0.8176, Accuracy: 71.32%\n",
      "Validation Epoch 41: Loss: 0.8270, Accuracy: 70.92%\n",
      "Training Epoch 42: Loss: 0.8054, Accuracy: 71.74%\n",
      "Validation Epoch 42: Loss: 0.8144, Accuracy: 71.44%\n",
      "Training Epoch 43: Loss: 0.7929, Accuracy: 72.47%\n",
      "Validation Epoch 43: Loss: 0.8251, Accuracy: 71.34%\n",
      "Training Epoch 44: Loss: 0.7838, Accuracy: 72.37%\n",
      "Validation Epoch 44: Loss: 0.8099, Accuracy: 71.91%\n",
      "Training Epoch 45: Loss: 0.7709, Accuracy: 72.96%\n",
      "Validation Epoch 45: Loss: 0.7944, Accuracy: 72.14%\n",
      "Training Epoch 46: Loss: 0.7648, Accuracy: 73.02%\n",
      "Validation Epoch 46: Loss: 0.8201, Accuracy: 72.53%\n",
      "Training Epoch 47: Loss: 0.7503, Accuracy: 73.49%\n",
      "Validation Epoch 47: Loss: 0.7819, Accuracy: 73.17%\n",
      "Training Epoch 48: Loss: 0.7448, Accuracy: 73.70%\n",
      "Validation Epoch 48: Loss: 0.7745, Accuracy: 73.03%\n",
      "Training Epoch 49: Loss: 0.7285, Accuracy: 74.35%\n",
      "Validation Epoch 49: Loss: 0.7778, Accuracy: 73.72%\n",
      "Training Epoch 50: Loss: 0.7250, Accuracy: 74.45%\n",
      "Validation Epoch 50: Loss: 0.7681, Accuracy: 73.04%\n",
      "Training Epoch 51: Loss: 0.7163, Accuracy: 74.92%\n",
      "Validation Epoch 51: Loss: 0.7559, Accuracy: 73.75%\n",
      "Training Epoch 52: Loss: 0.7035, Accuracy: 75.17%\n",
      "Validation Epoch 52: Loss: 0.7585, Accuracy: 74.37%\n",
      "Training Epoch 53: Loss: 0.6918, Accuracy: 75.50%\n",
      "Validation Epoch 53: Loss: 0.7763, Accuracy: 73.30%\n",
      "Training Epoch 54: Loss: 0.6832, Accuracy: 75.81%\n",
      "Validation Epoch 54: Loss: 0.7689, Accuracy: 73.47%\n",
      "Training Epoch 55: Loss: 0.6809, Accuracy: 75.97%\n",
      "Validation Epoch 55: Loss: 0.7575, Accuracy: 73.66%\n",
      "Training Epoch 56: Loss: 0.6739, Accuracy: 76.31%\n",
      "Validation Epoch 56: Loss: 0.7585, Accuracy: 73.98%\n",
      "Training Epoch 57: Loss: 0.6547, Accuracy: 76.78%\n",
      "Validation Epoch 57: Loss: 0.7648, Accuracy: 74.19%\n",
      "Training Epoch 58: Loss: 0.6490, Accuracy: 77.23%\n",
      "Validation Epoch 58: Loss: 0.7425, Accuracy: 74.89%\n",
      "Training Epoch 59: Loss: 0.6452, Accuracy: 77.25%\n",
      "Validation Epoch 59: Loss: 0.7460, Accuracy: 74.51%\n",
      "Training Epoch 60: Loss: 0.6366, Accuracy: 77.80%\n",
      "Validation Epoch 60: Loss: 0.7307, Accuracy: 75.15%\n",
      "Training Epoch 61: Loss: 0.6291, Accuracy: 78.01%\n",
      "Validation Epoch 61: Loss: 0.7406, Accuracy: 74.79%\n",
      "Training Epoch 62: Loss: 0.6213, Accuracy: 77.92%\n",
      "Validation Epoch 62: Loss: 0.7444, Accuracy: 75.29%\n",
      "Training Epoch 63: Loss: 0.6070, Accuracy: 78.55%\n",
      "Validation Epoch 63: Loss: 0.7400, Accuracy: 75.05%\n",
      "Training Epoch 64: Loss: 0.6046, Accuracy: 78.69%\n",
      "Validation Epoch 64: Loss: 0.7162, Accuracy: 76.12%\n",
      "Training Epoch 65: Loss: 0.5999, Accuracy: 78.63%\n",
      "Validation Epoch 65: Loss: 0.7322, Accuracy: 75.25%\n",
      "Training Epoch 66: Loss: 0.5894, Accuracy: 79.36%\n",
      "Validation Epoch 66: Loss: 0.7168, Accuracy: 75.33%\n",
      "Training Epoch 67: Loss: 0.5855, Accuracy: 79.43%\n",
      "Validation Epoch 67: Loss: 0.7163, Accuracy: 76.21%\n",
      "Training Epoch 68: Loss: 0.5775, Accuracy: 79.72%\n",
      "Validation Epoch 68: Loss: 0.7300, Accuracy: 75.42%\n",
      "Training Epoch 69: Loss: 0.5643, Accuracy: 79.84%\n",
      "Validation Epoch 69: Loss: 0.7277, Accuracy: 76.22%\n",
      "Training Epoch 70: Loss: 0.5589, Accuracy: 80.16%\n",
      "Validation Epoch 70: Loss: 0.7267, Accuracy: 75.75%\n",
      "Training Epoch 71: Loss: 0.5551, Accuracy: 80.37%\n",
      "Validation Epoch 71: Loss: 0.7415, Accuracy: 75.66%\n",
      "Training Epoch 72: Loss: 0.5485, Accuracy: 80.25%\n",
      "Validation Epoch 72: Loss: 0.7192, Accuracy: 76.21%\n",
      "Training Epoch 73: Loss: 0.5446, Accuracy: 80.64%\n",
      "Validation Epoch 73: Loss: 0.7107, Accuracy: 76.57%\n",
      "Training Epoch 74: Loss: 0.5276, Accuracy: 81.23%\n",
      "Validation Epoch 74: Loss: 0.7189, Accuracy: 76.45%\n",
      "Training Epoch 75: Loss: 0.5306, Accuracy: 81.31%\n",
      "Validation Epoch 75: Loss: 0.7344, Accuracy: 76.08%\n",
      "Training Epoch 76: Loss: 0.5201, Accuracy: 81.54%\n",
      "Validation Epoch 76: Loss: 0.7326, Accuracy: 76.21%\n",
      "Training Epoch 77: Loss: 0.5143, Accuracy: 81.89%\n",
      "Validation Epoch 77: Loss: 0.7111, Accuracy: 77.22%\n",
      "Training Epoch 78: Loss: 0.5094, Accuracy: 82.11%\n",
      "Validation Epoch 78: Loss: 0.7255, Accuracy: 76.68%\n",
      "Training Epoch 79: Loss: 0.5066, Accuracy: 82.06%\n",
      "Validation Epoch 79: Loss: 0.7207, Accuracy: 76.93%\n",
      "Training Epoch 80: Loss: 0.5051, Accuracy: 82.04%\n",
      "Validation Epoch 80: Loss: 0.7099, Accuracy: 77.19%\n",
      "Training Epoch 81: Loss: 0.4921, Accuracy: 82.83%\n",
      "Validation Epoch 81: Loss: 0.7132, Accuracy: 76.83%\n",
      "Training Epoch 82: Loss: 0.4829, Accuracy: 82.81%\n",
      "Validation Epoch 82: Loss: 0.7312, Accuracy: 76.40%\n",
      "Training Epoch 83: Loss: 0.4781, Accuracy: 82.99%\n",
      "Validation Epoch 83: Loss: 0.7247, Accuracy: 77.22%\n",
      "Training Epoch 84: Loss: 0.4750, Accuracy: 83.18%\n",
      "Validation Epoch 84: Loss: 0.7208, Accuracy: 76.94%\n",
      "Training Epoch 85: Loss: 0.4710, Accuracy: 83.33%\n",
      "Validation Epoch 85: Loss: 0.7219, Accuracy: 77.08%\n",
      "Training Epoch 86: Loss: 0.4653, Accuracy: 83.55%\n",
      "Validation Epoch 86: Loss: 0.7154, Accuracy: 77.36%\n",
      "Training Epoch 87: Loss: 0.4611, Accuracy: 83.57%\n",
      "Validation Epoch 87: Loss: 0.7092, Accuracy: 78.00%\n",
      "Training Epoch 88: Loss: 0.4526, Accuracy: 83.90%\n",
      "Validation Epoch 88: Loss: 0.7227, Accuracy: 77.11%\n",
      "Training Epoch 89: Loss: 0.4432, Accuracy: 84.35%\n",
      "Validation Epoch 89: Loss: 0.7216, Accuracy: 77.26%\n",
      "Training Epoch 90: Loss: 0.4354, Accuracy: 84.42%\n",
      "Validation Epoch 90: Loss: 0.7113, Accuracy: 77.83%\n",
      "Training Epoch 91: Loss: 0.4385, Accuracy: 84.43%\n",
      "Validation Epoch 91: Loss: 0.7283, Accuracy: 76.98%\n",
      "Training Epoch 92: Loss: 0.4371, Accuracy: 84.51%\n",
      "Validation Epoch 92: Loss: 0.7360, Accuracy: 76.78%\n",
      "Training Epoch 93: Loss: 0.4281, Accuracy: 84.72%\n",
      "Validation Epoch 93: Loss: 0.7253, Accuracy: 77.27%\n",
      "Training Epoch 94: Loss: 0.4160, Accuracy: 85.21%\n",
      "Validation Epoch 94: Loss: 0.7262, Accuracy: 77.66%\n",
      "Training Epoch 95: Loss: 0.4166, Accuracy: 85.18%\n",
      "Validation Epoch 95: Loss: 0.7262, Accuracy: 77.56%\n",
      "Training Epoch 96: Loss: 0.4170, Accuracy: 85.16%\n",
      "Validation Epoch 96: Loss: 0.7408, Accuracy: 77.93%\n",
      "Training Epoch 97: Loss: 0.4006, Accuracy: 85.67%\n",
      "Validation Epoch 97: Loss: 0.7227, Accuracy: 78.19%\n",
      "Training Epoch 98: Loss: 0.3989, Accuracy: 85.78%\n",
      "Validation Epoch 98: Loss: 0.7286, Accuracy: 77.87%\n",
      "Training Epoch 99: Loss: 0.3514, Accuracy: 87.47%\n",
      "Validation Epoch 99: Loss: 0.7041, Accuracy: 78.67%\n",
      "Training Epoch 100: Loss: 0.3332, Accuracy: 88.28%\n",
      "Validation Epoch 100: Loss: 0.7089, Accuracy: 79.06%\n",
      "Training Epoch 101: Loss: 0.3183, Accuracy: 88.54%\n",
      "Validation Epoch 101: Loss: 0.7188, Accuracy: 79.11%\n",
      "Training Epoch 102: Loss: 0.3262, Accuracy: 88.55%\n",
      "Validation Epoch 102: Loss: 0.7110, Accuracy: 78.94%\n",
      "Training Epoch 103: Loss: 0.3096, Accuracy: 88.94%\n",
      "Validation Epoch 103: Loss: 0.7212, Accuracy: 78.68%\n",
      "Training Epoch 104: Loss: 0.3094, Accuracy: 89.14%\n",
      "Validation Epoch 104: Loss: 0.7296, Accuracy: 79.11%\n",
      "Training Epoch 105: Loss: 0.3091, Accuracy: 89.08%\n",
      "Validation Epoch 105: Loss: 0.7342, Accuracy: 79.10%\n",
      "Training Epoch 106: Loss: 0.3002, Accuracy: 89.53%\n",
      "Validation Epoch 106: Loss: 0.7319, Accuracy: 79.03%\n",
      "Training Epoch 107: Loss: 0.3006, Accuracy: 89.20%\n",
      "Validation Epoch 107: Loss: 0.7332, Accuracy: 78.96%\n",
      "Training Epoch 108: Loss: 0.3004, Accuracy: 89.60%\n",
      "Validation Epoch 108: Loss: 0.7355, Accuracy: 79.05%\n",
      "Training Epoch 109: Loss: 0.2938, Accuracy: 89.51%\n",
      "Validation Epoch 109: Loss: 0.7389, Accuracy: 78.99%\n",
      "Training Epoch 110: Loss: 0.2955, Accuracy: 89.33%\n",
      "Validation Epoch 110: Loss: 0.7264, Accuracy: 79.25%\n",
      "Training Epoch 111: Loss: 0.2795, Accuracy: 90.05%\n",
      "Validation Epoch 111: Loss: 0.7330, Accuracy: 79.22%\n",
      "Training Epoch 112: Loss: 0.2814, Accuracy: 89.82%\n",
      "Validation Epoch 112: Loss: 0.7376, Accuracy: 79.21%\n",
      "Training Epoch 113: Loss: 0.2851, Accuracy: 89.86%\n",
      "Validation Epoch 113: Loss: 0.7307, Accuracy: 79.28%\n",
      "Training Epoch 114: Loss: 0.2842, Accuracy: 89.95%\n",
      "Validation Epoch 114: Loss: 0.7292, Accuracy: 79.30%\n",
      "Training Epoch 115: Loss: 0.2799, Accuracy: 90.14%\n",
      "Validation Epoch 115: Loss: 0.7339, Accuracy: 79.16%\n",
      "Training Epoch 116: Loss: 0.2805, Accuracy: 89.97%\n",
      "Validation Epoch 116: Loss: 0.7343, Accuracy: 79.15%\n",
      "Training Epoch 117: Loss: 0.2824, Accuracy: 90.06%\n",
      "Validation Epoch 117: Loss: 0.7343, Accuracy: 79.22%\n",
      "Training Epoch 118: Loss: 0.2840, Accuracy: 90.08%\n",
      "Validation Epoch 118: Loss: 0.7405, Accuracy: 79.33%\n",
      "Training Epoch 119: Loss: 0.2802, Accuracy: 89.97%\n",
      "Validation Epoch 119: Loss: 0.7359, Accuracy: 79.31%\n",
      "Training Epoch 120: Loss: 0.2798, Accuracy: 90.03%\n",
      "Validation Epoch 120: Loss: 0.7340, Accuracy: 79.27%\n",
      "Training Epoch 121: Loss: 0.2792, Accuracy: 89.97%\n",
      "Validation Epoch 121: Loss: 0.7373, Accuracy: 79.11%\n",
      "Training Epoch 122: Loss: 0.2792, Accuracy: 90.04%\n",
      "Validation Epoch 122: Loss: 0.7401, Accuracy: 79.14%\n",
      "Training Epoch 123: Loss: 0.2816, Accuracy: 89.94%\n",
      "Validation Epoch 123: Loss: 0.7344, Accuracy: 79.11%\n",
      "Training Epoch 124: Loss: 0.2859, Accuracy: 89.98%\n",
      "Validation Epoch 124: Loss: 0.7351, Accuracy: 79.34%\n",
      "Training Epoch 125: Loss: 0.2791, Accuracy: 90.20%\n",
      "Validation Epoch 125: Loss: 0.7394, Accuracy: 79.26%\n",
      "Training Epoch 126: Loss: 0.2794, Accuracy: 90.15%\n",
      "Validation Epoch 126: Loss: 0.7362, Accuracy: 79.26%\n",
      "Training Epoch 127: Loss: 0.2802, Accuracy: 89.96%\n",
      "Validation Epoch 127: Loss: 0.7381, Accuracy: 79.35%\n",
      "Training Epoch 128: Loss: 0.2791, Accuracy: 90.28%\n",
      "Validation Epoch 128: Loss: 0.7400, Accuracy: 79.22%\n",
      "Training Epoch 129: Loss: 0.2829, Accuracy: 90.10%\n",
      "Validation Epoch 129: Loss: 0.7343, Accuracy: 79.15%\n",
      "Training Epoch 130: Loss: 0.2845, Accuracy: 90.04%\n",
      "Validation Epoch 130: Loss: 0.7406, Accuracy: 79.23%\n",
      "Training Epoch 131: Loss: 0.2756, Accuracy: 90.22%\n",
      "Validation Epoch 131: Loss: 0.7372, Accuracy: 79.12%\n",
      "Training Epoch 132: Loss: 0.2807, Accuracy: 89.94%\n",
      "Validation Epoch 132: Loss: 0.7343, Accuracy: 79.40%\n",
      "Training Epoch 133: Loss: 0.2773, Accuracy: 90.03%\n",
      "Validation Epoch 133: Loss: 0.7423, Accuracy: 78.94%\n",
      "Training Epoch 134: Loss: 0.2797, Accuracy: 90.11%\n",
      "Validation Epoch 134: Loss: 0.7440, Accuracy: 79.13%\n",
      "Training Epoch 135: Loss: 0.2828, Accuracy: 89.92%\n",
      "Validation Epoch 135: Loss: 0.7414, Accuracy: 79.24%\n",
      "Training Epoch 136: Loss: 0.2789, Accuracy: 90.06%\n",
      "Validation Epoch 136: Loss: 0.7444, Accuracy: 79.12%\n",
      "Training Epoch 137: Loss: 0.2802, Accuracy: 90.04%\n",
      "Validation Epoch 137: Loss: 0.7376, Accuracy: 79.19%\n",
      "Training Epoch 138: Loss: 0.2769, Accuracy: 90.30%\n",
      "Validation Epoch 138: Loss: 0.7368, Accuracy: 79.20%\n",
      "Training Epoch 139: Loss: 0.2772, Accuracy: 90.02%\n",
      "Validation Epoch 139: Loss: 0.7423, Accuracy: 79.23%\n",
      "Training Epoch 140: Loss: 0.2798, Accuracy: 90.04%\n",
      "Validation Epoch 140: Loss: 0.7429, Accuracy: 79.16%\n",
      "Training Epoch 141: Loss: 0.2797, Accuracy: 89.97%\n",
      "Validation Epoch 141: Loss: 0.7381, Accuracy: 79.32%\n",
      "Training Epoch 142: Loss: 0.2808, Accuracy: 89.94%\n",
      "Validation Epoch 142: Loss: 0.7399, Accuracy: 79.38%\n",
      "Training Epoch 143: Loss: 0.2831, Accuracy: 90.01%\n",
      "Validation Epoch 143: Loss: 0.7350, Accuracy: 79.26%\n",
      "Training Epoch 144: Loss: 0.2805, Accuracy: 89.75%\n",
      "Validation Epoch 144: Loss: 0.7359, Accuracy: 79.30%\n",
      "Training Epoch 145: Loss: 0.2807, Accuracy: 90.09%\n",
      "Validation Epoch 145: Loss: 0.7434, Accuracy: 79.06%\n",
      "Training Epoch 146: Loss: 0.2813, Accuracy: 89.92%\n",
      "Validation Epoch 146: Loss: 0.7423, Accuracy: 79.10%\n",
      "Training Epoch 147: Loss: 0.2755, Accuracy: 90.18%\n",
      "Validation Epoch 147: Loss: 0.7428, Accuracy: 79.17%\n",
      "Training Epoch 148: Loss: 0.2768, Accuracy: 90.11%\n",
      "Validation Epoch 148: Loss: 0.7354, Accuracy: 79.42%\n",
      "Training Epoch 149: Loss: 0.2813, Accuracy: 90.06%\n",
      "Validation Epoch 149: Loss: 0.7446, Accuracy: 79.20%\n",
      "Training Epoch 150: Loss: 0.2789, Accuracy: 90.28%\n",
      "Validation Epoch 150: Loss: 0.7294, Accuracy: 79.38%\n",
      "Training Epoch 151: Loss: 0.2758, Accuracy: 90.18%\n",
      "Validation Epoch 151: Loss: 0.7319, Accuracy: 79.31%\n",
      "Training Epoch 152: Loss: 0.2795, Accuracy: 90.05%\n",
      "Validation Epoch 152: Loss: 0.7427, Accuracy: 79.23%\n",
      "Training Epoch 153: Loss: 0.2766, Accuracy: 89.99%\n",
      "Validation Epoch 153: Loss: 0.7379, Accuracy: 79.33%\n",
      "Training Epoch 154: Loss: 0.2765, Accuracy: 90.23%\n",
      "Validation Epoch 154: Loss: 0.7421, Accuracy: 79.14%\n",
      "Training Epoch 155: Loss: 0.2779, Accuracy: 90.16%\n",
      "Validation Epoch 155: Loss: 0.7368, Accuracy: 79.28%\n",
      "Training Epoch 156: Loss: 0.2781, Accuracy: 90.14%\n",
      "Validation Epoch 156: Loss: 0.7348, Accuracy: 79.06%\n",
      "Training Epoch 157: Loss: 0.2815, Accuracy: 89.93%\n",
      "Validation Epoch 157: Loss: 0.7373, Accuracy: 79.21%\n",
      "Training Epoch 158: Loss: 0.2787, Accuracy: 90.13%\n",
      "Validation Epoch 158: Loss: 0.7321, Accuracy: 79.41%\n",
      "Training Epoch 159: Loss: 0.2815, Accuracy: 89.99%\n",
      "Validation Epoch 159: Loss: 0.7404, Accuracy: 79.07%\n",
      "Training Epoch 160: Loss: 0.2790, Accuracy: 90.20%\n",
      "Validation Epoch 160: Loss: 0.7427, Accuracy: 79.01%\n",
      "Training Epoch 161: Loss: 0.2773, Accuracy: 90.02%\n",
      "Validation Epoch 161: Loss: 0.7431, Accuracy: 79.13%\n",
      "Training Epoch 162: Loss: 0.2794, Accuracy: 89.96%\n",
      "Validation Epoch 162: Loss: 0.7420, Accuracy: 79.14%\n",
      "Training Epoch 163: Loss: 0.2769, Accuracy: 90.14%\n",
      "Validation Epoch 163: Loss: 0.7401, Accuracy: 79.02%\n",
      "Training Epoch 164: Loss: 0.2773, Accuracy: 90.22%\n",
      "Validation Epoch 164: Loss: 0.7473, Accuracy: 79.27%\n",
      "Training Epoch 165: Loss: 0.2809, Accuracy: 89.97%\n",
      "Validation Epoch 165: Loss: 0.7335, Accuracy: 79.25%\n",
      "Training Epoch 166: Loss: 0.2816, Accuracy: 89.95%\n",
      "Validation Epoch 166: Loss: 0.7361, Accuracy: 79.26%\n",
      "Training Epoch 167: Loss: 0.2796, Accuracy: 90.20%\n",
      "Validation Epoch 167: Loss: 0.7401, Accuracy: 79.13%\n",
      "Training Epoch 168: Loss: 0.2767, Accuracy: 90.21%\n",
      "Validation Epoch 168: Loss: 0.7402, Accuracy: 79.29%\n",
      "Training Epoch 169: Loss: 0.2762, Accuracy: 90.08%\n",
      "Validation Epoch 169: Loss: 0.7442, Accuracy: 79.36%\n",
      "Training Epoch 170: Loss: 0.2771, Accuracy: 90.12%\n",
      "Validation Epoch 170: Loss: 0.7378, Accuracy: 79.18%\n",
      "Training Epoch 171: Loss: 0.2796, Accuracy: 90.05%\n",
      "Validation Epoch 171: Loss: 0.7372, Accuracy: 79.09%\n",
      "Training Epoch 172: Loss: 0.2778, Accuracy: 90.00%\n",
      "Validation Epoch 172: Loss: 0.7425, Accuracy: 79.16%\n",
      "Training Epoch 173: Loss: 0.2739, Accuracy: 90.19%\n",
      "Validation Epoch 173: Loss: 0.7374, Accuracy: 79.20%\n",
      "Training Epoch 174: Loss: 0.2777, Accuracy: 90.34%\n",
      "Validation Epoch 174: Loss: 0.7400, Accuracy: 79.13%\n",
      "Training Epoch 175: Loss: 0.2853, Accuracy: 89.92%\n",
      "Validation Epoch 175: Loss: 0.7344, Accuracy: 79.23%\n",
      "Training Epoch 176: Loss: 0.2813, Accuracy: 89.77%\n",
      "Validation Epoch 176: Loss: 0.7406, Accuracy: 79.13%\n",
      "Training Epoch 177: Loss: 0.2745, Accuracy: 90.26%\n",
      "Validation Epoch 177: Loss: 0.7367, Accuracy: 79.25%\n",
      "Training Epoch 178: Loss: 0.2787, Accuracy: 90.06%\n",
      "Validation Epoch 178: Loss: 0.7385, Accuracy: 79.05%\n",
      "Training Epoch 179: Loss: 0.2812, Accuracy: 90.09%\n",
      "Validation Epoch 179: Loss: 0.7441, Accuracy: 79.16%\n",
      "Training Epoch 180: Loss: 0.2830, Accuracy: 90.01%\n",
      "Validation Epoch 180: Loss: 0.7409, Accuracy: 79.14%\n",
      "Training Epoch 181: Loss: 0.2725, Accuracy: 90.40%\n",
      "Validation Epoch 181: Loss: 0.7381, Accuracy: 79.40%\n",
      "Training Epoch 182: Loss: 0.2782, Accuracy: 90.09%\n",
      "Validation Epoch 182: Loss: 0.7405, Accuracy: 79.17%\n",
      "Training Epoch 183: Loss: 0.2755, Accuracy: 90.16%\n",
      "Validation Epoch 183: Loss: 0.7396, Accuracy: 79.16%\n",
      "Training Epoch 184: Loss: 0.2773, Accuracy: 90.12%\n",
      "Validation Epoch 184: Loss: 0.7350, Accuracy: 79.26%\n",
      "Training Epoch 185: Loss: 0.2773, Accuracy: 90.19%\n",
      "Validation Epoch 185: Loss: 0.7365, Accuracy: 79.38%\n",
      "Training Epoch 186: Loss: 0.2853, Accuracy: 89.89%\n",
      "Validation Epoch 186: Loss: 0.7380, Accuracy: 79.06%\n",
      "Training Epoch 187: Loss: 0.2762, Accuracy: 90.09%\n",
      "Validation Epoch 187: Loss: 0.7365, Accuracy: 79.28%\n",
      "Training Epoch 188: Loss: 0.2739, Accuracy: 90.15%\n",
      "Validation Epoch 188: Loss: 0.7401, Accuracy: 79.07%\n",
      "Training Epoch 189: Loss: 0.2758, Accuracy: 90.20%\n",
      "Validation Epoch 189: Loss: 0.7361, Accuracy: 79.29%\n",
      "Training Epoch 190: Loss: 0.2813, Accuracy: 90.07%\n",
      "Validation Epoch 190: Loss: 0.7366, Accuracy: 79.22%\n",
      "Training Epoch 191: Loss: 0.2777, Accuracy: 90.27%\n",
      "Validation Epoch 191: Loss: 0.7385, Accuracy: 79.27%\n",
      "Training Epoch 192: Loss: 0.2789, Accuracy: 90.13%\n",
      "Validation Epoch 192: Loss: 0.7369, Accuracy: 79.15%\n",
      "Training Epoch 193: Loss: 0.2800, Accuracy: 90.10%\n",
      "Validation Epoch 193: Loss: 0.7455, Accuracy: 79.19%\n",
      "Training Epoch 194: Loss: 0.2755, Accuracy: 90.19%\n",
      "Validation Epoch 194: Loss: 0.7387, Accuracy: 79.30%\n",
      "Training Epoch 195: Loss: 0.2774, Accuracy: 90.17%\n",
      "Validation Epoch 195: Loss: 0.7417, Accuracy: 79.06%\n",
      "Training Epoch 196: Loss: 0.2787, Accuracy: 90.20%\n",
      "Validation Epoch 196: Loss: 0.7322, Accuracy: 79.33%\n",
      "Training Epoch 197: Loss: 0.2838, Accuracy: 90.06%\n",
      "Validation Epoch 197: Loss: 0.7423, Accuracy: 79.15%\n",
      "Training Epoch 198: Loss: 0.2818, Accuracy: 89.94%\n",
      "Validation Epoch 198: Loss: 0.7425, Accuracy: 79.18%\n",
      "Training Epoch 199: Loss: 0.2766, Accuracy: 90.08%\n",
      "Validation Epoch 199: Loss: 0.7345, Accuracy: 79.32%\n",
      "Training Epoch 200: Loss: 0.2798, Accuracy: 90.06%\n",
      "Validation Epoch 200: Loss: 0.7421, Accuracy: 79.17%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▄▅▆▆▆▆▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>validation_loss</td><td>█▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>200</td></tr><tr><td>train_accuracy</td><td>90.0625</td></tr><tr><td>train_loss</td><td>0.27984</td></tr><tr><td>validation_accuracy</td><td>79.17</td></tr><tr><td>validation_loss</td><td>0.74208</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_model_3rd_train</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/6manek5w' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/6manek5w</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241113_110915-6manek5w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration for improved experiment\n",
    "config_modified = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 64,  # Fixed as per assignment\n",
    "    'learning_rate': 1e-3,  # Higher initial learning rate\n",
    "    'num_epochs': 200,\n",
    "    'model_name': 'resnet50',\n",
    "    'wandb_project_name': 'CIFAR10_training_with_various_models',\n",
    "    \"checkpoint_save_interval\": 10,\n",
    "    \"checkpoint_path\": \"checkpoints/checkpoint_modified.pth\",\n",
    "    \"best_model_path\": \"checkpoints/best_model_modified.pth\",\n",
    "    \"load_from_checkpoint\": None,\n",
    "}\n",
    "\n",
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50()\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Linear(model.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "    return model\n",
    "\n",
    "def load_cifar10_dataloaders(data_root_dir, device, batch_size, num_worker):\n",
    "    validation_size = 0.2\n",
    "    random_seed = 42\n",
    "    normalize = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "\n",
    "    # Data Augmentation for training set\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    test_transforms = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=train_transforms)\n",
    "    val_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=test_transforms)\n",
    "    test_dataset = datasets.CIFAR10(root=data_root_dir, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    train_indices, val_indices = train_test_split(np.arange(len(train_dataset)), test_size=validation_size, random_state=random_seed)\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    kwargs = {'pin_memory': True} if device.startswith(\"cuda\") else {}\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_worker, **kwargs)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_worker, **kwargs)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_worker, **kwargs)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader, num_classes\n",
    "\n",
    "def train_loop(model, device, train_dataloader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Training Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluation_loop(model, device, dataloader, criterion, epoch=None, phase=\"validation\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    if epoch is not None:\n",
    "        print(f\"{phase.capitalize()} Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "def train_main_modified(config):\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "    checkpoint_save_interval = config.get('checkpoint_save_interval', 10)\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "    best_acc1 = 0\n",
    "\n",
    "    wandb.finish()\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config,\n",
    "        name=\"ResNet50_model_3rd_train\"\n",
    "    )\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader, num_classes = load_cifar10_dataloaders(\n",
    "        data_root_dir, device, batch_size=batch_size, num_worker=num_worker)\n",
    "    model = get_model(model_name=config[\"model_name\"], num_classes=num_classes, config=config).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Using SGD with momentum\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = best_model_path if load_from_checkpoint == \"best\" else checkpoint_path\n",
    "        start_epoch, best_acc1 = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    if config.get('test_mode', False):\n",
    "        print(\"Running test evaluation...\")\n",
    "        test_acc, test_loss = evaluation_loop(model, device, test_dataloader, criterion, phase=\"test\")\n",
    "        print(f\"Test Accuracy: {test_acc}\")\n",
    "    else:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            train_loss, train_acc = train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "            val_acc1, val_loss = evaluation_loop(model, device, val_dataloader, criterion, epoch=epoch, phase=\"validation\")\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_accuracy': train_acc,\n",
    "                'validation_loss': val_loss,\n",
    "                'validation_accuracy': val_acc1\n",
    "            })\n",
    "\n",
    "            if (epoch + 1) % checkpoint_save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "                is_best = val_acc1 > best_acc1\n",
    "                best_acc1 = max(val_acc1, best_acc1)\n",
    "                save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_acc1, is_best, best_model_path)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the modified training function to perform the experiment\n",
    "train_main_modified(config_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39914c59-f8c6-428c-a992-2b8bc00a8af4",
   "metadata": {},
   "source": [
    "## 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e52cac8-5764-4974-ac97-40b825bb87b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241113_113113-ivqe3fbg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/ivqe3fbg' target=\"_blank\">ResNet50_model_3rd_train</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/ivqe3fbg' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/ivqe3fbg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model resnet50 with 24562250 parameters (24562250 trainable)\n",
      "=> loaded checkpoint 'checkpoints/best_model_modified.pth' (epoch 150)\n",
      "Running test evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_model_3rd_train</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/ivqe3fbg' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/ivqe3fbg</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241113_113113-ivqe3fbg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "config_testmode = {\n",
    "    **config_modified,\n",
    "    'test_mode': True,  # True if evaluating only on the test set\n",
    "    'load_from_checkpoint': 'best'\n",
    "}\n",
    "\n",
    "train_main_modified(config_testmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a9681-09bb-454d-9a10-461dc4d11446",
   "metadata": {},
   "source": [
    "# 네 번째 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6d2ac2-81d2-4ad9-9a7a-50e80c3ecbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "# SAM Optimizer Implementation\n",
    "class SAM(Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, \"SAM requires non-negative rho.\"\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        scale = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (scale + 1e-12)\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * (torch.abs(p) if group[\"adaptive\"] else 1.0) * scale.to(p)\n",
    "                p.add_(e_w)  # Ascent step\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # Descent step\n",
    "        self.base_optimizer.step()  # Perform actual update\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device\n",
    "        norm = torch.norm(\n",
    "            torch.stack([\n",
    "                p.grad.norm(p=2).to(shared_device)\n",
    "                for group in self.param_groups for p in group[\"params\"]\n",
    "                if p.grad is not None\n",
    "            ]),\n",
    "            p=2\n",
    "        )\n",
    "        return norm\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"SAM requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # Get loss for first step\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ebe10ce-86c8-46d4-9ce7-e126d21a989e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33munknownlimitless0301\u001b[0m (\u001b[33munknownlimitless0301-university-of-suwon6591\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241113_122745-xsk86wzn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/xsk86wzn' target=\"_blank\">ResNet50_model_4th_train_with_SAM</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/xsk86wzn' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/xsk86wzn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model resnet50 with 24563274 parameters (24563274 trainable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: Loss: 2.2473, Accuracy: 16.18%\n",
      "Validation Epoch 1: Loss: 2.0942, Accuracy: 22.94%\n",
      "Training Epoch 2: Loss: 2.0611, Accuracy: 22.73%\n",
      "Validation Epoch 2: Loss: 1.9491, Accuracy: 27.39%\n",
      "Training Epoch 3: Loss: 1.9372, Accuracy: 27.24%\n",
      "Validation Epoch 3: Loss: 1.8485, Accuracy: 31.05%\n",
      "Training Epoch 4: Loss: 1.8287, Accuracy: 31.19%\n",
      "Validation Epoch 4: Loss: 1.7389, Accuracy: 34.81%\n",
      "Training Epoch 5: Loss: 1.7452, Accuracy: 34.58%\n",
      "Validation Epoch 5: Loss: 1.6658, Accuracy: 37.26%\n",
      "Training Epoch 6: Loss: 1.6821, Accuracy: 37.38%\n",
      "Validation Epoch 6: Loss: 1.5999, Accuracy: 40.01%\n",
      "Training Epoch 7: Loss: 1.6350, Accuracy: 38.61%\n",
      "Validation Epoch 7: Loss: 1.5509, Accuracy: 42.59%\n",
      "Training Epoch 8: Loss: 1.5913, Accuracy: 40.60%\n",
      "Validation Epoch 8: Loss: 1.5160, Accuracy: 43.75%\n",
      "Training Epoch 9: Loss: 1.5578, Accuracy: 42.00%\n",
      "Validation Epoch 9: Loss: 1.4630, Accuracy: 45.35%\n",
      "Training Epoch 10: Loss: 1.5236, Accuracy: 43.52%\n",
      "Validation Epoch 10: Loss: 1.4376, Accuracy: 46.70%\n",
      "Training Epoch 11: Loss: 1.4920, Accuracy: 44.49%\n",
      "Validation Epoch 11: Loss: 1.4198, Accuracy: 47.13%\n",
      "Training Epoch 12: Loss: 1.4658, Accuracy: 45.77%\n",
      "Validation Epoch 12: Loss: 1.3866, Accuracy: 48.70%\n",
      "Training Epoch 13: Loss: 1.4381, Accuracy: 47.38%\n",
      "Validation Epoch 13: Loss: 1.3543, Accuracy: 49.44%\n",
      "Training Epoch 14: Loss: 1.4157, Accuracy: 48.19%\n",
      "Validation Epoch 14: Loss: 1.3288, Accuracy: 51.42%\n",
      "Training Epoch 15: Loss: 1.3926, Accuracy: 48.83%\n",
      "Validation Epoch 15: Loss: 1.3146, Accuracy: 51.92%\n",
      "Training Epoch 16: Loss: 1.3712, Accuracy: 49.88%\n",
      "Validation Epoch 16: Loss: 1.2910, Accuracy: 52.28%\n",
      "Training Epoch 17: Loss: 1.3522, Accuracy: 50.98%\n",
      "Validation Epoch 17: Loss: 1.2621, Accuracy: 54.09%\n",
      "Training Epoch 18: Loss: 1.3271, Accuracy: 52.00%\n",
      "Validation Epoch 18: Loss: 1.2527, Accuracy: 54.02%\n",
      "Training Epoch 19: Loss: 1.3085, Accuracy: 52.53%\n",
      "Validation Epoch 19: Loss: 1.2297, Accuracy: 55.90%\n",
      "Training Epoch 20: Loss: 1.2903, Accuracy: 53.51%\n",
      "Validation Epoch 20: Loss: 1.2109, Accuracy: 56.31%\n",
      "Training Epoch 21: Loss: 1.2732, Accuracy: 54.53%\n",
      "Validation Epoch 21: Loss: 1.1854, Accuracy: 57.09%\n",
      "Training Epoch 22: Loss: 1.2532, Accuracy: 54.81%\n",
      "Validation Epoch 22: Loss: 1.1722, Accuracy: 57.69%\n",
      "Training Epoch 23: Loss: 1.2340, Accuracy: 55.94%\n",
      "Validation Epoch 23: Loss: 1.1664, Accuracy: 58.41%\n",
      "Training Epoch 24: Loss: 1.2174, Accuracy: 56.52%\n",
      "Validation Epoch 24: Loss: 1.1434, Accuracy: 58.85%\n",
      "Training Epoch 25: Loss: 1.1955, Accuracy: 57.22%\n",
      "Validation Epoch 25: Loss: 1.1258, Accuracy: 59.49%\n",
      "Training Epoch 26: Loss: 1.1795, Accuracy: 57.49%\n",
      "Validation Epoch 26: Loss: 1.1146, Accuracy: 60.29%\n",
      "Training Epoch 27: Loss: 1.1655, Accuracy: 58.56%\n",
      "Validation Epoch 27: Loss: 1.0957, Accuracy: 60.94%\n",
      "Training Epoch 28: Loss: 1.1416, Accuracy: 59.31%\n",
      "Validation Epoch 28: Loss: 1.0652, Accuracy: 61.49%\n",
      "Training Epoch 29: Loss: 1.1362, Accuracy: 59.73%\n",
      "Validation Epoch 29: Loss: 1.0704, Accuracy: 61.48%\n",
      "Training Epoch 30: Loss: 1.1152, Accuracy: 60.27%\n",
      "Validation Epoch 30: Loss: 1.0519, Accuracy: 61.60%\n",
      "Training Epoch 31: Loss: 1.1005, Accuracy: 61.02%\n",
      "Validation Epoch 31: Loss: 1.0363, Accuracy: 62.88%\n",
      "Training Epoch 32: Loss: 1.0843, Accuracy: 61.57%\n",
      "Validation Epoch 32: Loss: 1.0176, Accuracy: 62.91%\n",
      "Training Epoch 33: Loss: 1.0688, Accuracy: 61.99%\n",
      "Validation Epoch 33: Loss: 1.0130, Accuracy: 63.06%\n",
      "Training Epoch 34: Loss: 1.0583, Accuracy: 62.50%\n",
      "Validation Epoch 34: Loss: 1.0098, Accuracy: 63.46%\n",
      "Training Epoch 35: Loss: 1.0492, Accuracy: 62.74%\n",
      "Validation Epoch 35: Loss: 0.9838, Accuracy: 64.60%\n",
      "Training Epoch 36: Loss: 1.0313, Accuracy: 63.53%\n",
      "Validation Epoch 36: Loss: 0.9709, Accuracy: 65.02%\n",
      "Training Epoch 37: Loss: 1.0168, Accuracy: 64.08%\n",
      "Validation Epoch 37: Loss: 0.9610, Accuracy: 65.08%\n",
      "Training Epoch 38: Loss: 1.0019, Accuracy: 64.64%\n",
      "Validation Epoch 38: Loss: 0.9443, Accuracy: 66.09%\n",
      "Training Epoch 39: Loss: 0.9874, Accuracy: 65.31%\n",
      "Validation Epoch 39: Loss: 0.9347, Accuracy: 65.96%\n",
      "Training Epoch 40: Loss: 0.9721, Accuracy: 65.83%\n",
      "Validation Epoch 40: Loss: 0.9231, Accuracy: 66.47%\n",
      "Training Epoch 41: Loss: 0.9592, Accuracy: 66.01%\n",
      "Validation Epoch 41: Loss: 0.9091, Accuracy: 67.31%\n",
      "Training Epoch 42: Loss: 0.9520, Accuracy: 66.53%\n",
      "Validation Epoch 42: Loss: 0.9066, Accuracy: 67.28%\n",
      "Training Epoch 43: Loss: 0.9362, Accuracy: 66.99%\n",
      "Validation Epoch 43: Loss: 0.8953, Accuracy: 67.88%\n",
      "Training Epoch 44: Loss: 0.9257, Accuracy: 67.32%\n",
      "Validation Epoch 44: Loss: 0.8841, Accuracy: 68.36%\n",
      "Training Epoch 45: Loss: 0.9188, Accuracy: 67.67%\n",
      "Validation Epoch 45: Loss: 0.8798, Accuracy: 68.46%\n",
      "Training Epoch 46: Loss: 0.8984, Accuracy: 68.67%\n",
      "Validation Epoch 46: Loss: 0.8574, Accuracy: 69.20%\n",
      "Training Epoch 47: Loss: 0.8912, Accuracy: 68.82%\n",
      "Validation Epoch 47: Loss: 0.8552, Accuracy: 69.34%\n",
      "Training Epoch 48: Loss: 0.8788, Accuracy: 69.37%\n",
      "Validation Epoch 48: Loss: 0.8302, Accuracy: 70.33%\n",
      "Training Epoch 49: Loss: 0.8675, Accuracy: 69.77%\n",
      "Validation Epoch 49: Loss: 0.8297, Accuracy: 70.03%\n",
      "Training Epoch 50: Loss: 0.8552, Accuracy: 70.25%\n",
      "Validation Epoch 50: Loss: 0.8214, Accuracy: 70.82%\n",
      "Training Epoch 51: Loss: 0.8474, Accuracy: 70.12%\n",
      "Validation Epoch 51: Loss: 0.8187, Accuracy: 71.01%\n",
      "Training Epoch 52: Loss: 0.8373, Accuracy: 70.56%\n",
      "Validation Epoch 52: Loss: 0.8146, Accuracy: 71.42%\n",
      "Training Epoch 53: Loss: 0.8320, Accuracy: 70.97%\n",
      "Validation Epoch 53: Loss: 0.8071, Accuracy: 71.29%\n",
      "Training Epoch 54: Loss: 0.8161, Accuracy: 71.41%\n",
      "Validation Epoch 54: Loss: 0.7981, Accuracy: 72.03%\n",
      "Training Epoch 55: Loss: 0.8132, Accuracy: 71.82%\n",
      "Validation Epoch 55: Loss: 0.7930, Accuracy: 71.81%\n",
      "Training Epoch 56: Loss: 0.7991, Accuracy: 72.22%\n",
      "Validation Epoch 56: Loss: 0.7807, Accuracy: 72.74%\n",
      "Training Epoch 57: Loss: 0.7866, Accuracy: 72.49%\n",
      "Validation Epoch 57: Loss: 0.7800, Accuracy: 72.29%\n",
      "Training Epoch 58: Loss: 0.7846, Accuracy: 72.69%\n",
      "Validation Epoch 58: Loss: 0.7670, Accuracy: 73.08%\n",
      "Training Epoch 59: Loss: 0.7729, Accuracy: 73.05%\n",
      "Validation Epoch 59: Loss: 0.7637, Accuracy: 73.30%\n",
      "Training Epoch 60: Loss: 0.7628, Accuracy: 73.33%\n",
      "Validation Epoch 60: Loss: 0.7575, Accuracy: 73.22%\n",
      "Training Epoch 61: Loss: 0.7610, Accuracy: 73.34%\n",
      "Validation Epoch 61: Loss: 0.7461, Accuracy: 73.96%\n",
      "Training Epoch 62: Loss: 0.7474, Accuracy: 73.81%\n",
      "Validation Epoch 62: Loss: 0.7430, Accuracy: 74.17%\n",
      "Training Epoch 63: Loss: 0.7478, Accuracy: 74.12%\n",
      "Validation Epoch 63: Loss: 0.7416, Accuracy: 73.80%\n",
      "Training Epoch 64: Loss: 0.7343, Accuracy: 74.44%\n",
      "Validation Epoch 64: Loss: 0.7332, Accuracy: 74.16%\n",
      "Training Epoch 65: Loss: 0.7305, Accuracy: 74.52%\n",
      "Validation Epoch 65: Loss: 0.7315, Accuracy: 74.36%\n",
      "Training Epoch 66: Loss: 0.7232, Accuracy: 74.86%\n",
      "Validation Epoch 66: Loss: 0.7261, Accuracy: 74.58%\n",
      "Training Epoch 67: Loss: 0.7134, Accuracy: 75.33%\n",
      "Validation Epoch 67: Loss: 0.7179, Accuracy: 74.72%\n",
      "Training Epoch 68: Loss: 0.7104, Accuracy: 75.55%\n",
      "Validation Epoch 68: Loss: 0.7174, Accuracy: 74.71%\n",
      "Training Epoch 69: Loss: 0.7008, Accuracy: 75.60%\n",
      "Validation Epoch 69: Loss: 0.7070, Accuracy: 75.35%\n",
      "Training Epoch 70: Loss: 0.6918, Accuracy: 76.03%\n",
      "Validation Epoch 70: Loss: 0.7078, Accuracy: 74.80%\n",
      "Training Epoch 71: Loss: 0.6826, Accuracy: 75.99%\n",
      "Validation Epoch 71: Loss: 0.6958, Accuracy: 75.60%\n",
      "Training Epoch 72: Loss: 0.6870, Accuracy: 76.22%\n",
      "Validation Epoch 72: Loss: 0.6962, Accuracy: 75.76%\n",
      "Training Epoch 73: Loss: 0.6754, Accuracy: 76.57%\n",
      "Validation Epoch 73: Loss: 0.6904, Accuracy: 76.03%\n",
      "Training Epoch 74: Loss: 0.6692, Accuracy: 76.60%\n",
      "Validation Epoch 74: Loss: 0.6888, Accuracy: 75.79%\n",
      "Training Epoch 75: Loss: 0.6634, Accuracy: 76.93%\n",
      "Validation Epoch 75: Loss: 0.6881, Accuracy: 75.82%\n",
      "Training Epoch 76: Loss: 0.6528, Accuracy: 77.38%\n",
      "Validation Epoch 76: Loss: 0.6732, Accuracy: 76.17%\n",
      "Training Epoch 77: Loss: 0.6528, Accuracy: 77.29%\n",
      "Validation Epoch 77: Loss: 0.6795, Accuracy: 76.01%\n",
      "Training Epoch 78: Loss: 0.6420, Accuracy: 77.78%\n",
      "Validation Epoch 78: Loss: 0.6729, Accuracy: 76.56%\n",
      "Training Epoch 79: Loss: 0.6420, Accuracy: 77.81%\n",
      "Validation Epoch 79: Loss: 0.6606, Accuracy: 76.69%\n",
      "Training Epoch 80: Loss: 0.6320, Accuracy: 78.12%\n",
      "Validation Epoch 80: Loss: 0.6613, Accuracy: 76.91%\n",
      "Training Epoch 81: Loss: 0.6306, Accuracy: 78.16%\n",
      "Validation Epoch 81: Loss: 0.6752, Accuracy: 76.32%\n",
      "Training Epoch 82: Loss: 0.6249, Accuracy: 78.20%\n",
      "Validation Epoch 82: Loss: 0.6636, Accuracy: 76.94%\n",
      "Training Epoch 83: Loss: 0.6162, Accuracy: 78.60%\n",
      "Validation Epoch 83: Loss: 0.6535, Accuracy: 77.20%\n",
      "Training Epoch 84: Loss: 0.6128, Accuracy: 78.83%\n",
      "Validation Epoch 84: Loss: 0.6538, Accuracy: 76.94%\n",
      "Training Epoch 85: Loss: 0.6089, Accuracy: 78.81%\n",
      "Validation Epoch 85: Loss: 0.6478, Accuracy: 77.33%\n",
      "Training Epoch 86: Loss: 0.6034, Accuracy: 79.11%\n",
      "Validation Epoch 86: Loss: 0.6483, Accuracy: 77.26%\n",
      "Training Epoch 87: Loss: 0.5978, Accuracy: 79.41%\n",
      "Validation Epoch 87: Loss: 0.6473, Accuracy: 77.40%\n",
      "Training Epoch 88: Loss: 0.5911, Accuracy: 79.40%\n",
      "Validation Epoch 88: Loss: 0.6442, Accuracy: 77.62%\n",
      "Training Epoch 89: Loss: 0.5829, Accuracy: 79.79%\n",
      "Validation Epoch 89: Loss: 0.6369, Accuracy: 77.94%\n",
      "Training Epoch 90: Loss: 0.5800, Accuracy: 79.88%\n",
      "Validation Epoch 90: Loss: 0.6423, Accuracy: 77.65%\n",
      "Training Epoch 91: Loss: 0.5771, Accuracy: 80.04%\n",
      "Validation Epoch 91: Loss: 0.6342, Accuracy: 77.57%\n",
      "Training Epoch 92: Loss: 0.5721, Accuracy: 80.28%\n",
      "Validation Epoch 92: Loss: 0.6337, Accuracy: 77.83%\n",
      "Training Epoch 93: Loss: 0.5691, Accuracy: 80.35%\n",
      "Validation Epoch 93: Loss: 0.6325, Accuracy: 77.61%\n",
      "Training Epoch 94: Loss: 0.5635, Accuracy: 80.47%\n",
      "Validation Epoch 94: Loss: 0.6297, Accuracy: 77.50%\n",
      "Training Epoch 95: Loss: 0.5609, Accuracy: 80.58%\n",
      "Validation Epoch 95: Loss: 0.6335, Accuracy: 77.80%\n",
      "Training Epoch 96: Loss: 0.5510, Accuracy: 81.01%\n",
      "Validation Epoch 96: Loss: 0.6309, Accuracy: 77.73%\n",
      "Training Epoch 97: Loss: 0.5479, Accuracy: 81.25%\n",
      "Validation Epoch 97: Loss: 0.6179, Accuracy: 78.60%\n",
      "Training Epoch 98: Loss: 0.5460, Accuracy: 81.19%\n",
      "Validation Epoch 98: Loss: 0.6211, Accuracy: 78.50%\n",
      "Training Epoch 99: Loss: 0.5335, Accuracy: 81.72%\n",
      "Validation Epoch 99: Loss: 0.6243, Accuracy: 78.13%\n",
      "Training Epoch 100: Loss: 0.5324, Accuracy: 81.70%\n",
      "Validation Epoch 100: Loss: 0.6145, Accuracy: 78.68%\n",
      "Training Epoch 101: Loss: 0.5308, Accuracy: 81.72%\n",
      "Validation Epoch 101: Loss: 0.6106, Accuracy: 78.81%\n",
      "Training Epoch 102: Loss: 0.5226, Accuracy: 81.99%\n",
      "Validation Epoch 102: Loss: 0.6163, Accuracy: 78.53%\n",
      "Training Epoch 103: Loss: 0.5269, Accuracy: 81.81%\n",
      "Validation Epoch 103: Loss: 0.6124, Accuracy: 78.90%\n",
      "Training Epoch 104: Loss: 0.5198, Accuracy: 81.91%\n",
      "Validation Epoch 104: Loss: 0.6146, Accuracy: 78.40%\n",
      "Training Epoch 105: Loss: 0.5155, Accuracy: 82.39%\n",
      "Validation Epoch 105: Loss: 0.6034, Accuracy: 79.01%\n",
      "Training Epoch 106: Loss: 0.5060, Accuracy: 82.52%\n",
      "Validation Epoch 106: Loss: 0.6035, Accuracy: 78.80%\n",
      "Training Epoch 107: Loss: 0.5088, Accuracy: 82.69%\n",
      "Validation Epoch 107: Loss: 0.6072, Accuracy: 78.55%\n",
      "Training Epoch 108: Loss: 0.5038, Accuracy: 82.64%\n",
      "Validation Epoch 108: Loss: 0.6032, Accuracy: 79.27%\n",
      "Training Epoch 109: Loss: 0.4994, Accuracy: 82.89%\n",
      "Validation Epoch 109: Loss: 0.5994, Accuracy: 79.16%\n",
      "Training Epoch 110: Loss: 0.4976, Accuracy: 82.87%\n",
      "Validation Epoch 110: Loss: 0.5930, Accuracy: 79.28%\n",
      "Training Epoch 111: Loss: 0.4857, Accuracy: 83.14%\n",
      "Validation Epoch 111: Loss: 0.6000, Accuracy: 79.10%\n",
      "Training Epoch 112: Loss: 0.4858, Accuracy: 83.11%\n",
      "Validation Epoch 112: Loss: 0.5913, Accuracy: 79.28%\n",
      "Training Epoch 113: Loss: 0.4868, Accuracy: 83.23%\n",
      "Validation Epoch 113: Loss: 0.5982, Accuracy: 79.13%\n",
      "Training Epoch 114: Loss: 0.4761, Accuracy: 83.55%\n",
      "Validation Epoch 114: Loss: 0.5906, Accuracy: 79.43%\n",
      "Training Epoch 115: Loss: 0.4768, Accuracy: 83.61%\n",
      "Validation Epoch 115: Loss: 0.5978, Accuracy: 79.01%\n",
      "Training Epoch 116: Loss: 0.4666, Accuracy: 84.08%\n",
      "Validation Epoch 116: Loss: 0.5905, Accuracy: 79.68%\n",
      "Training Epoch 117: Loss: 0.4680, Accuracy: 83.77%\n",
      "Validation Epoch 117: Loss: 0.5860, Accuracy: 79.48%\n",
      "Training Epoch 118: Loss: 0.4605, Accuracy: 84.25%\n",
      "Validation Epoch 118: Loss: 0.5923, Accuracy: 79.34%\n",
      "Training Epoch 119: Loss: 0.4583, Accuracy: 84.17%\n",
      "Validation Epoch 119: Loss: 0.5804, Accuracy: 79.90%\n",
      "Training Epoch 120: Loss: 0.4557, Accuracy: 84.30%\n",
      "Validation Epoch 120: Loss: 0.5829, Accuracy: 79.56%\n",
      "Training Epoch 121: Loss: 0.4526, Accuracy: 84.47%\n",
      "Validation Epoch 121: Loss: 0.5822, Accuracy: 79.78%\n",
      "Training Epoch 122: Loss: 0.4499, Accuracy: 84.42%\n",
      "Validation Epoch 122: Loss: 0.5880, Accuracy: 79.31%\n",
      "Training Epoch 123: Loss: 0.4474, Accuracy: 84.59%\n",
      "Validation Epoch 123: Loss: 0.5849, Accuracy: 79.87%\n",
      "Training Epoch 124: Loss: 0.4361, Accuracy: 85.03%\n",
      "Validation Epoch 124: Loss: 0.5876, Accuracy: 79.40%\n",
      "Training Epoch 125: Loss: 0.4389, Accuracy: 85.00%\n",
      "Validation Epoch 125: Loss: 0.5766, Accuracy: 79.78%\n",
      "Training Epoch 126: Loss: 0.4325, Accuracy: 85.17%\n",
      "Validation Epoch 126: Loss: 0.5830, Accuracy: 79.22%\n",
      "Training Epoch 127: Loss: 0.4295, Accuracy: 85.24%\n",
      "Validation Epoch 127: Loss: 0.5870, Accuracy: 79.51%\n",
      "Training Epoch 128: Loss: 0.4280, Accuracy: 85.38%\n",
      "Validation Epoch 128: Loss: 0.5751, Accuracy: 80.03%\n",
      "Training Epoch 129: Loss: 0.4205, Accuracy: 85.64%\n",
      "Validation Epoch 129: Loss: 0.5758, Accuracy: 80.22%\n",
      "Training Epoch 130: Loss: 0.4208, Accuracy: 85.40%\n",
      "Validation Epoch 130: Loss: 0.5760, Accuracy: 80.06%\n",
      "Training Epoch 131: Loss: 0.4143, Accuracy: 85.76%\n",
      "Validation Epoch 131: Loss: 0.5662, Accuracy: 80.40%\n",
      "Training Epoch 132: Loss: 0.4115, Accuracy: 85.93%\n",
      "Validation Epoch 132: Loss: 0.5726, Accuracy: 80.23%\n",
      "Training Epoch 133: Loss: 0.4068, Accuracy: 86.02%\n",
      "Validation Epoch 133: Loss: 0.5643, Accuracy: 80.34%\n",
      "Training Epoch 134: Loss: 0.4033, Accuracy: 86.35%\n",
      "Validation Epoch 134: Loss: 0.5731, Accuracy: 80.10%\n",
      "Training Epoch 135: Loss: 0.4049, Accuracy: 86.13%\n",
      "Validation Epoch 135: Loss: 0.5696, Accuracy: 80.26%\n",
      "Training Epoch 136: Loss: 0.3969, Accuracy: 86.27%\n",
      "Validation Epoch 136: Loss: 0.5668, Accuracy: 80.29%\n",
      "Training Epoch 137: Loss: 0.3971, Accuracy: 86.35%\n",
      "Validation Epoch 137: Loss: 0.5676, Accuracy: 80.49%\n",
      "Training Epoch 138: Loss: 0.3935, Accuracy: 86.51%\n",
      "Validation Epoch 138: Loss: 0.5682, Accuracy: 80.19%\n",
      "Training Epoch 139: Loss: 0.3897, Accuracy: 86.71%\n",
      "Validation Epoch 139: Loss: 0.5650, Accuracy: 80.33%\n",
      "Training Epoch 140: Loss: 0.3847, Accuracy: 87.05%\n",
      "Validation Epoch 140: Loss: 0.5623, Accuracy: 80.39%\n",
      "Training Epoch 141: Loss: 0.3889, Accuracy: 86.73%\n",
      "Validation Epoch 141: Loss: 0.5609, Accuracy: 80.89%\n",
      "Training Epoch 142: Loss: 0.3781, Accuracy: 87.23%\n",
      "Validation Epoch 142: Loss: 0.5644, Accuracy: 80.47%\n",
      "Training Epoch 143: Loss: 0.3745, Accuracy: 87.46%\n",
      "Validation Epoch 143: Loss: 0.5701, Accuracy: 80.34%\n",
      "Training Epoch 144: Loss: 0.3734, Accuracy: 87.14%\n",
      "Validation Epoch 144: Loss: 0.5551, Accuracy: 80.75%\n",
      "Training Epoch 145: Loss: 0.3687, Accuracy: 87.69%\n",
      "Validation Epoch 145: Loss: 0.5587, Accuracy: 80.75%\n",
      "Training Epoch 146: Loss: 0.3658, Accuracy: 87.58%\n",
      "Validation Epoch 146: Loss: 0.5527, Accuracy: 81.09%\n",
      "Training Epoch 147: Loss: 0.3605, Accuracy: 87.91%\n",
      "Validation Epoch 147: Loss: 0.5510, Accuracy: 80.88%\n",
      "Training Epoch 148: Loss: 0.3603, Accuracy: 87.64%\n",
      "Validation Epoch 148: Loss: 0.5535, Accuracy: 80.98%\n",
      "Training Epoch 149: Loss: 0.3608, Accuracy: 87.78%\n",
      "Validation Epoch 149: Loss: 0.5546, Accuracy: 80.94%\n",
      "Training Epoch 150: Loss: 0.3538, Accuracy: 88.14%\n",
      "Validation Epoch 150: Loss: 0.5544, Accuracy: 80.78%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>validation_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>150</td></tr><tr><td>train_accuracy</td><td>88.1425</td></tr><tr><td>train_loss</td><td>0.35381</td></tr><tr><td>validation_accuracy</td><td>80.78</td></tr><tr><td>validation_loss</td><td>0.55443</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_model_4th_train_with_SAM</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/xsk86wzn' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/xsk86wzn</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241113_122745-xsk86wzn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration for improved experiment with GELU, BatchNorm, and SAM optimizer\n",
    "config_modified = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 64,  # Fixed as per assignment\n",
    "    'learning_rate': 1e-3,  # Higher initial learning rate\n",
    "    'num_epochs': 150,  # Updated to 150 epochs\n",
    "    'model_name': 'resnet50',\n",
    "    'wandb_project_name': 'CIFAR10_training_with_various_models',\n",
    "    \"checkpoint_save_interval\": 10,\n",
    "    \"checkpoint_path\": \"checkpoints/checkpoint_modified.pth\",\n",
    "    \"best_model_path\": \"checkpoints/best_model_modified.pth\",\n",
    "    \"load_from_checkpoint\": None,\n",
    "}\n",
    "\n",
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50()\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Linear(model.fc.in_features, 512),\n",
    "            nn.BatchNorm1d(512),  # Batch Normalization 추가\n",
    "            nn.GELU(),            # GELU 활성화 함수로 변경\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "    return model\n",
    "\n",
    "def load_cifar10_dataloaders(data_root_dir, device, batch_size, num_worker):\n",
    "    validation_size = 0.2\n",
    "    random_seed = 42\n",
    "    normalize = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "\n",
    "    # Data Augmentation for training set\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    test_transforms = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=train_transforms)\n",
    "    val_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=test_transforms)\n",
    "    test_dataset = datasets.CIFAR10(root=data_root_dir, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    train_indices, val_indices = train_test_split(np.arange(len(train_dataset)), test_size=validation_size, random_state=random_seed)\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    kwargs = {'pin_memory': True} if device.startswith(\"cuda\") else {}\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_worker, **kwargs)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_worker, **kwargs)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_worker, **kwargs)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader, num_classes\n",
    "\n",
    "def train_loop(model, device, train_dataloader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # SAM optimizer requires two steps: forward-backward for sharpness-aware gradient adjustment\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step(closure)  # SAM의 두 단계 업데이트\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Training Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluation_loop(model, device, dataloader, criterion, epoch=None, phase=\"validation\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    if epoch is not None:\n",
    "        print(f\"{phase.capitalize()} Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "def train_main_modified(config):\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "    checkpoint_save_interval = config.get('checkpoint_save_interval', 10)\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "    best_acc1 = 0\n",
    "\n",
    "    wandb.finish()\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config,\n",
    "        name=\"ResNet50_model_4th_train_with_SAM\"\n",
    "    )\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader, num_classes = load_cifar10_dataloaders(\n",
    "        data_root_dir, device, batch_size=batch_size, num_worker=num_worker)\n",
    "    model = get_model(model_name=config[\"model_name\"], num_classes=num_classes, config=config).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Using SAM with 'SGD with momentum'\n",
    "    optimizer = SAM(model.parameters(), base_optimizer=torch.optim.SGD, lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = best_model_path if load_from_checkpoint == \"best\" else checkpoint_path\n",
    "        start_epoch, best_acc1 = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    if config.get('test_mode', False):\n",
    "        print(\"Running test evaluation...\")\n",
    "        test_acc, test_loss = evaluation_loop(model, device, test_dataloader, criterion, phase=\"test\")\n",
    "        print(f\"Test Accuracy: {test_acc}\")\n",
    "    else:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            train_loss, train_acc = train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "            val_acc1, val_loss = evaluation_loop(model, device, val_dataloader, criterion, epoch=epoch, phase=\"validation\")\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_accuracy': train_acc,\n",
    "                'validation_loss': val_loss,\n",
    "                'validation_accuracy': val_acc1\n",
    "            })\n",
    "\n",
    "            if (epoch + 1) % checkpoint_save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "                is_best = val_acc1 > best_acc1\n",
    "                best_acc1 = max(val_acc1, best_acc1)\n",
    "                save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_acc1, is_best, best_model_path)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the modified training function to perform the experiment\n",
    "train_main_modified(config_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6680795c-a090-4e62-88cd-9d9cf9416b51",
   "metadata": {},
   "source": [
    "## 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "120e9b61-6cfb-4282-b0da-52bd1c4790c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241113_125556-duonhlt8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/duonhlt8' target=\"_blank\">ResNet50_model_4th_train_with_SAM</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/duonhlt8' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/duonhlt8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model resnet50 with 24563274 parameters (24563274 trainable)\n",
      "=> loaded checkpoint 'checkpoints/best_model_modified.pth' (epoch 150)\n",
      "Running test evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_model_4th_train_with_SAM</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/duonhlt8' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/duonhlt8</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241113_125556-duonhlt8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "config_testmode = {\n",
    "    **config_modified,\n",
    "    'test_mode': True,  # True if evaluating only on the test set\n",
    "    'load_from_checkpoint': 'best'\n",
    "}\n",
    "\n",
    "train_main_modified(config_testmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f850166-a355-49b2-86ed-9a6e8464f178",
   "metadata": {},
   "source": [
    "# 다섯번째 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f3371f-9939-416e-952d-e0b2942437bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Optimizer\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import AutoAugmentPolicy\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# SAM Optimizer Implementation (이미 작성된 SAM 코드 그대로 사용)\n",
    "class SAM(Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, \"SAM requires non-negative rho.\"\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        scale = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (scale + 1e-12)\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * (torch.abs(p) if group[\"adaptive\"] else 1.0) * scale.to(p)\n",
    "                p.add_(e_w)\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])\n",
    "        self.base_optimizer.step()\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device\n",
    "        norm = torch.norm(\n",
    "            torch.stack([\n",
    "                p.grad.norm(p=2).to(shared_device)\n",
    "                for group in self.param_groups for p in group[\"params\"]\n",
    "                if p.grad is not None\n",
    "            ]),\n",
    "            p=2\n",
    "        )\n",
    "        return norm\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"SAM requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a38776-cac4-4357-a64a-9899824ca947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33munknownlimitless0301\u001b[0m (\u001b[33munknownlimitless0301-university-of-suwon6591\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241115_095818-eeydko88</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/eeydko88' target=\"_blank\">ResNet50_5th_train_with_extra_tuning</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/eeydko88' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/eeydko88</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model resnet50 with 26139210 parameters (26139210 trainable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: Loss: 2.3229, Accuracy: 12.20%\n",
      "Validation Loss: 2.2072, Accuracy: 18.16%\n",
      "Training Epoch 2: Loss: 2.2451, Accuracy: 15.05%\n",
      "Validation Loss: 2.1041, Accuracy: 22.03%\n",
      "Training Epoch 3: Loss: 2.1759, Accuracy: 17.61%\n",
      "Validation Loss: 2.0175, Accuracy: 23.71%\n",
      "Training Epoch 4: Loss: 2.1138, Accuracy: 19.96%\n",
      "Validation Loss: 1.9415, Accuracy: 26.72%\n",
      "Training Epoch 5: Loss: 2.0529, Accuracy: 22.75%\n",
      "Validation Loss: 1.8722, Accuracy: 29.98%\n",
      "Training Epoch 6: Loss: 2.0045, Accuracy: 24.45%\n",
      "Validation Loss: 1.8103, Accuracy: 32.41%\n",
      "Training Epoch 7: Loss: 1.9633, Accuracy: 25.79%\n",
      "Validation Loss: 1.7623, Accuracy: 34.98%\n",
      "Training Epoch 8: Loss: 1.9354, Accuracy: 27.04%\n",
      "Validation Loss: 1.7180, Accuracy: 36.21%\n",
      "Training Epoch 9: Loss: 1.9014, Accuracy: 28.87%\n",
      "Validation Loss: 1.6968, Accuracy: 36.93%\n",
      "Training Epoch 10: Loss: 1.8719, Accuracy: 29.86%\n",
      "Validation Loss: 1.6563, Accuracy: 38.77%\n",
      "Training Epoch 11: Loss: 1.8536, Accuracy: 30.87%\n",
      "Validation Loss: 1.6220, Accuracy: 39.88%\n",
      "Training Epoch 12: Loss: 1.8374, Accuracy: 31.61%\n",
      "Validation Loss: 1.5936, Accuracy: 40.88%\n",
      "Training Epoch 13: Loss: 1.8178, Accuracy: 32.30%\n",
      "Validation Loss: 1.5906, Accuracy: 42.47%\n",
      "Training Epoch 14: Loss: 1.7969, Accuracy: 33.08%\n",
      "Validation Loss: 1.5560, Accuracy: 42.61%\n",
      "Training Epoch 15: Loss: 1.7751, Accuracy: 34.36%\n",
      "Validation Loss: 1.5335, Accuracy: 43.95%\n",
      "Training Epoch 16: Loss: 1.7631, Accuracy: 35.00%\n",
      "Validation Loss: 1.5195, Accuracy: 45.00%\n",
      "Training Epoch 17: Loss: 1.7385, Accuracy: 36.01%\n",
      "Validation Loss: 1.4941, Accuracy: 44.69%\n",
      "Training Epoch 18: Loss: 1.7259, Accuracy: 36.35%\n",
      "Validation Loss: 1.4827, Accuracy: 46.74%\n",
      "Training Epoch 19: Loss: 1.7153, Accuracy: 36.73%\n",
      "Validation Loss: 1.4619, Accuracy: 45.91%\n",
      "Training Epoch 20: Loss: 1.6963, Accuracy: 37.90%\n",
      "Validation Loss: 1.4541, Accuracy: 46.51%\n",
      "Training Epoch 21: Loss: 1.6883, Accuracy: 38.11%\n",
      "Validation Loss: 1.4137, Accuracy: 48.76%\n",
      "Training Epoch 22: Loss: 1.6667, Accuracy: 38.73%\n",
      "Validation Loss: 1.4253, Accuracy: 48.50%\n",
      "Training Epoch 23: Loss: 1.6540, Accuracy: 39.48%\n",
      "Validation Loss: 1.3944, Accuracy: 50.23%\n",
      "Training Epoch 24: Loss: 1.6409, Accuracy: 40.12%\n",
      "Validation Loss: 1.3755, Accuracy: 50.39%\n",
      "Training Epoch 25: Loss: 1.6273, Accuracy: 40.73%\n",
      "Validation Loss: 1.3589, Accuracy: 51.06%\n",
      "Training Epoch 26: Loss: 1.6136, Accuracy: 41.13%\n",
      "Validation Loss: 1.3390, Accuracy: 52.07%\n",
      "Training Epoch 27: Loss: 1.6004, Accuracy: 41.44%\n",
      "Validation Loss: 1.3253, Accuracy: 51.36%\n",
      "Training Epoch 28: Loss: 1.5840, Accuracy: 42.53%\n",
      "Validation Loss: 1.3205, Accuracy: 52.06%\n",
      "Training Epoch 29: Loss: 1.5780, Accuracy: 42.73%\n",
      "Validation Loss: 1.2961, Accuracy: 53.26%\n",
      "Training Epoch 30: Loss: 1.5661, Accuracy: 43.25%\n",
      "Validation Loss: 1.3045, Accuracy: 52.86%\n",
      "Training Epoch 31: Loss: 1.5487, Accuracy: 43.68%\n",
      "Validation Loss: 1.2602, Accuracy: 54.81%\n",
      "Training Epoch 32: Loss: 1.5350, Accuracy: 44.35%\n",
      "Validation Loss: 1.2408, Accuracy: 55.06%\n",
      "Training Epoch 33: Loss: 1.5253, Accuracy: 45.06%\n",
      "Validation Loss: 1.2316, Accuracy: 56.30%\n",
      "Training Epoch 34: Loss: 1.5112, Accuracy: 45.42%\n",
      "Validation Loss: 1.2350, Accuracy: 55.37%\n",
      "Training Epoch 35: Loss: 1.4819, Accuracy: 46.27%\n",
      "Validation Loss: 1.2105, Accuracy: 56.66%\n",
      "Training Epoch 36: Loss: 1.4748, Accuracy: 46.98%\n",
      "Validation Loss: 1.1892, Accuracy: 57.24%\n",
      "Training Epoch 37: Loss: 1.4609, Accuracy: 47.26%\n",
      "Validation Loss: 1.1868, Accuracy: 57.38%\n",
      "Training Epoch 38: Loss: 1.4522, Accuracy: 47.74%\n",
      "Validation Loss: 1.1658, Accuracy: 58.05%\n",
      "Training Epoch 39: Loss: 1.4387, Accuracy: 47.98%\n",
      "Validation Loss: 1.1388, Accuracy: 58.91%\n",
      "Training Epoch 40: Loss: 1.4261, Accuracy: 48.85%\n",
      "Validation Loss: 1.1215, Accuracy: 59.90%\n",
      "Training Epoch 41: Loss: 1.4125, Accuracy: 49.27%\n",
      "Validation Loss: 1.1155, Accuracy: 60.35%\n",
      "Training Epoch 42: Loss: 1.4044, Accuracy: 49.71%\n",
      "Validation Loss: 1.1061, Accuracy: 60.11%\n",
      "Training Epoch 43: Loss: 1.3860, Accuracy: 50.07%\n",
      "Validation Loss: 1.0907, Accuracy: 60.83%\n",
      "Training Epoch 44: Loss: 1.3740, Accuracy: 50.41%\n",
      "Validation Loss: 1.0881, Accuracy: 61.37%\n",
      "Training Epoch 45: Loss: 1.3618, Accuracy: 51.27%\n",
      "Validation Loss: 1.0806, Accuracy: 61.82%\n",
      "Training Epoch 46: Loss: 1.3576, Accuracy: 51.29%\n",
      "Validation Loss: 1.0529, Accuracy: 62.61%\n",
      "Training Epoch 47: Loss: 1.3424, Accuracy: 51.88%\n",
      "Validation Loss: 1.0377, Accuracy: 62.94%\n",
      "Training Epoch 48: Loss: 1.3413, Accuracy: 52.19%\n",
      "Validation Loss: 1.0225, Accuracy: 63.38%\n",
      "Training Epoch 49: Loss: 1.3242, Accuracy: 52.79%\n",
      "Validation Loss: 1.0209, Accuracy: 63.83%\n",
      "Training Epoch 50: Loss: 1.3070, Accuracy: 53.07%\n",
      "Validation Loss: 1.0224, Accuracy: 63.79%\n",
      "Training Epoch 51: Loss: 1.3023, Accuracy: 53.51%\n",
      "Validation Loss: 1.0026, Accuracy: 64.15%\n",
      "Training Epoch 52: Loss: 1.2878, Accuracy: 54.26%\n",
      "Validation Loss: 0.9843, Accuracy: 65.10%\n",
      "Training Epoch 53: Loss: 1.2796, Accuracy: 54.46%\n",
      "Validation Loss: 0.9794, Accuracy: 65.36%\n",
      "Training Epoch 54: Loss: 1.2706, Accuracy: 54.91%\n",
      "Validation Loss: 0.9703, Accuracy: 65.55%\n",
      "Training Epoch 55: Loss: 1.2600, Accuracy: 55.32%\n",
      "Validation Loss: 0.9486, Accuracy: 66.38%\n",
      "Training Epoch 56: Loss: 1.2512, Accuracy: 55.37%\n",
      "Validation Loss: 0.9424, Accuracy: 66.26%\n",
      "Training Epoch 57: Loss: 1.2320, Accuracy: 56.41%\n",
      "Validation Loss: 0.9322, Accuracy: 66.20%\n",
      "Training Epoch 58: Loss: 1.2406, Accuracy: 56.02%\n",
      "Validation Loss: 0.9270, Accuracy: 67.10%\n",
      "Training Epoch 59: Loss: 1.2234, Accuracy: 56.56%\n",
      "Validation Loss: 0.9121, Accuracy: 67.75%\n",
      "Training Epoch 60: Loss: 1.2122, Accuracy: 57.01%\n",
      "Validation Loss: 0.9118, Accuracy: 67.41%\n",
      "Training Epoch 61: Loss: 1.2147, Accuracy: 56.86%\n",
      "Validation Loss: 0.9211, Accuracy: 67.35%\n",
      "Training Epoch 62: Loss: 1.1890, Accuracy: 57.87%\n",
      "Validation Loss: 0.8899, Accuracy: 68.25%\n",
      "Training Epoch 63: Loss: 1.1931, Accuracy: 57.89%\n",
      "Validation Loss: 0.8972, Accuracy: 68.34%\n",
      "Training Epoch 64: Loss: 1.1783, Accuracy: 58.50%\n",
      "Validation Loss: 0.8797, Accuracy: 68.82%\n",
      "Training Epoch 65: Loss: 1.1712, Accuracy: 58.91%\n",
      "Validation Loss: 0.8763, Accuracy: 68.62%\n",
      "Training Epoch 66: Loss: 1.1661, Accuracy: 58.78%\n",
      "Validation Loss: 0.8636, Accuracy: 69.36%\n",
      "Training Epoch 67: Loss: 1.1569, Accuracy: 59.11%\n",
      "Validation Loss: 0.8695, Accuracy: 69.11%\n",
      "Training Epoch 68: Loss: 1.1565, Accuracy: 59.22%\n",
      "Validation Loss: 0.8658, Accuracy: 70.05%\n",
      "Training Epoch 69: Loss: 1.1445, Accuracy: 59.64%\n",
      "Validation Loss: 0.8348, Accuracy: 70.83%\n",
      "Training Epoch 70: Loss: 1.1370, Accuracy: 59.77%\n",
      "Validation Loss: 0.8294, Accuracy: 70.75%\n",
      "Training Epoch 71: Loss: 1.1284, Accuracy: 60.23%\n",
      "Validation Loss: 0.8325, Accuracy: 70.64%\n",
      "Training Epoch 72: Loss: 1.1254, Accuracy: 60.38%\n",
      "Validation Loss: 0.8224, Accuracy: 70.91%\n",
      "Training Epoch 73: Loss: 1.1177, Accuracy: 60.67%\n",
      "Validation Loss: 0.8278, Accuracy: 71.28%\n",
      "Training Epoch 74: Loss: 1.1092, Accuracy: 60.81%\n",
      "Validation Loss: 0.8148, Accuracy: 70.89%\n",
      "Training Epoch 75: Loss: 1.0987, Accuracy: 61.61%\n",
      "Validation Loss: 0.8093, Accuracy: 71.53%\n",
      "Training Epoch 76: Loss: 1.0941, Accuracy: 61.57%\n",
      "Validation Loss: 0.8001, Accuracy: 71.48%\n",
      "Training Epoch 77: Loss: 1.0923, Accuracy: 61.59%\n",
      "Validation Loss: 0.8009, Accuracy: 71.77%\n",
      "Training Epoch 78: Loss: 1.0812, Accuracy: 62.02%\n",
      "Validation Loss: 0.7932, Accuracy: 72.66%\n",
      "Training Epoch 79: Loss: 1.0844, Accuracy: 61.98%\n",
      "Validation Loss: 0.7826, Accuracy: 72.66%\n",
      "Training Epoch 80: Loss: 1.0722, Accuracy: 62.24%\n",
      "Validation Loss: 0.7766, Accuracy: 73.13%\n",
      "Training Epoch 81: Loss: 1.0663, Accuracy: 62.92%\n",
      "Validation Loss: 0.7656, Accuracy: 73.16%\n",
      "Training Epoch 82: Loss: 1.0628, Accuracy: 62.57%\n",
      "Validation Loss: 0.7785, Accuracy: 72.85%\n",
      "Training Epoch 83: Loss: 1.0624, Accuracy: 62.86%\n",
      "Validation Loss: 0.7664, Accuracy: 73.32%\n",
      "Training Epoch 84: Loss: 1.0520, Accuracy: 63.20%\n",
      "Validation Loss: 0.7761, Accuracy: 73.18%\n",
      "Training Epoch 85: Loss: 1.0449, Accuracy: 63.34%\n",
      "Validation Loss: 0.7677, Accuracy: 73.42%\n",
      "Training Epoch 86: Loss: 1.0411, Accuracy: 63.52%\n",
      "Validation Loss: 0.7623, Accuracy: 73.18%\n",
      "Training Epoch 87: Loss: 1.0346, Accuracy: 63.92%\n",
      "Validation Loss: 0.7554, Accuracy: 73.78%\n",
      "Training Epoch 88: Loss: 1.0233, Accuracy: 64.35%\n",
      "Validation Loss: 0.7514, Accuracy: 74.72%\n",
      "Training Epoch 89: Loss: 1.0208, Accuracy: 64.14%\n",
      "Validation Loss: 0.7432, Accuracy: 74.19%\n",
      "Training Epoch 90: Loss: 1.0155, Accuracy: 64.40%\n",
      "Validation Loss: 0.7346, Accuracy: 74.42%\n",
      "Training Epoch 91: Loss: 1.0134, Accuracy: 64.36%\n",
      "Validation Loss: 0.7342, Accuracy: 74.78%\n",
      "Training Epoch 92: Loss: 1.0021, Accuracy: 65.16%\n",
      "Validation Loss: 0.7269, Accuracy: 74.84%\n",
      "Training Epoch 93: Loss: 1.0116, Accuracy: 64.84%\n",
      "Validation Loss: 0.7302, Accuracy: 74.28%\n",
      "Training Epoch 94: Loss: 1.0009, Accuracy: 65.29%\n",
      "Validation Loss: 0.7182, Accuracy: 75.29%\n",
      "Training Epoch 95: Loss: 0.9952, Accuracy: 65.47%\n",
      "Validation Loss: 0.7154, Accuracy: 75.36%\n",
      "Training Epoch 96: Loss: 0.9922, Accuracy: 65.42%\n",
      "Validation Loss: 0.7186, Accuracy: 75.13%\n",
      "Training Epoch 97: Loss: 0.9871, Accuracy: 65.49%\n",
      "Validation Loss: 0.7064, Accuracy: 75.59%\n",
      "Training Epoch 98: Loss: 0.9729, Accuracy: 65.89%\n",
      "Validation Loss: 0.6991, Accuracy: 75.93%\n",
      "Training Epoch 99: Loss: 0.9882, Accuracy: 65.54%\n",
      "Validation Loss: 0.7051, Accuracy: 75.69%\n",
      "Training Epoch 100: Loss: 0.9718, Accuracy: 65.92%\n",
      "Validation Loss: 0.6947, Accuracy: 75.56%\n",
      "Training Epoch 101: Loss: 0.9705, Accuracy: 66.28%\n",
      "Validation Loss: 0.6962, Accuracy: 75.89%\n",
      "Training Epoch 102: Loss: 0.9646, Accuracy: 66.45%\n",
      "Validation Loss: 0.6921, Accuracy: 76.22%\n",
      "Training Epoch 103: Loss: 0.9630, Accuracy: 66.31%\n",
      "Validation Loss: 0.6876, Accuracy: 75.88%\n",
      "Training Epoch 104: Loss: 0.9611, Accuracy: 66.73%\n",
      "Validation Loss: 0.6918, Accuracy: 76.21%\n",
      "Training Epoch 105: Loss: 0.9484, Accuracy: 66.83%\n",
      "Validation Loss: 0.6875, Accuracy: 76.14%\n",
      "Training Epoch 106: Loss: 0.9518, Accuracy: 66.86%\n",
      "Validation Loss: 0.6757, Accuracy: 76.40%\n",
      "Training Epoch 107: Loss: 0.9445, Accuracy: 66.97%\n",
      "Validation Loss: 0.6693, Accuracy: 76.90%\n",
      "Training Epoch 108: Loss: 0.9469, Accuracy: 66.92%\n",
      "Validation Loss: 0.6781, Accuracy: 76.44%\n",
      "Training Epoch 109: Loss: 0.9356, Accuracy: 67.53%\n",
      "Validation Loss: 0.6698, Accuracy: 76.96%\n",
      "Training Epoch 110: Loss: 0.9298, Accuracy: 67.66%\n",
      "Validation Loss: 0.6795, Accuracy: 76.18%\n",
      "Training Epoch 111: Loss: 0.9273, Accuracy: 67.75%\n",
      "Validation Loss: 0.6695, Accuracy: 76.77%\n",
      "Training Epoch 112: Loss: 0.9244, Accuracy: 67.97%\n",
      "Validation Loss: 0.6611, Accuracy: 77.55%\n",
      "Training Epoch 113: Loss: 0.9237, Accuracy: 67.86%\n",
      "Validation Loss: 0.6578, Accuracy: 77.55%\n",
      "Training Epoch 114: Loss: 0.9168, Accuracy: 68.11%\n",
      "Validation Loss: 0.6495, Accuracy: 77.80%\n",
      "Training Epoch 115: Loss: 0.9145, Accuracy: 68.22%\n",
      "Validation Loss: 0.6567, Accuracy: 77.53%\n",
      "Training Epoch 116: Loss: 0.9145, Accuracy: 68.20%\n",
      "Validation Loss: 0.6491, Accuracy: 77.88%\n",
      "Training Epoch 117: Loss: 0.9086, Accuracy: 68.38%\n",
      "Validation Loss: 0.6462, Accuracy: 77.83%\n",
      "Training Epoch 118: Loss: 0.9035, Accuracy: 68.60%\n",
      "Validation Loss: 0.6478, Accuracy: 78.05%\n",
      "Training Epoch 119: Loss: 0.9027, Accuracy: 68.71%\n",
      "Validation Loss: 0.6471, Accuracy: 78.11%\n",
      "Training Epoch 120: Loss: 0.8972, Accuracy: 68.91%\n",
      "Validation Loss: 0.6476, Accuracy: 77.87%\n",
      "Training Epoch 121: Loss: 0.8963, Accuracy: 68.94%\n",
      "Validation Loss: 0.6377, Accuracy: 77.77%\n",
      "Training Epoch 122: Loss: 0.8896, Accuracy: 69.11%\n",
      "Validation Loss: 0.6375, Accuracy: 78.10%\n",
      "Training Epoch 123: Loss: 0.8931, Accuracy: 69.00%\n",
      "Validation Loss: 0.6372, Accuracy: 78.21%\n",
      "Training Epoch 124: Loss: 0.8884, Accuracy: 69.16%\n",
      "Validation Loss: 0.6275, Accuracy: 78.50%\n",
      "Training Epoch 125: Loss: 0.8814, Accuracy: 69.14%\n",
      "Validation Loss: 0.6198, Accuracy: 78.48%\n",
      "Training Epoch 126: Loss: 0.8777, Accuracy: 69.36%\n",
      "Validation Loss: 0.6299, Accuracy: 78.43%\n",
      "Training Epoch 127: Loss: 0.8726, Accuracy: 69.80%\n",
      "Validation Loss: 0.6215, Accuracy: 78.74%\n",
      "Training Epoch 128: Loss: 0.8667, Accuracy: 69.95%\n",
      "Validation Loss: 0.6180, Accuracy: 78.71%\n",
      "Training Epoch 129: Loss: 0.8650, Accuracy: 70.05%\n",
      "Validation Loss: 0.6290, Accuracy: 78.28%\n",
      "Training Epoch 130: Loss: 0.8645, Accuracy: 70.20%\n",
      "Validation Loss: 0.6208, Accuracy: 78.54%\n",
      "Training Epoch 131: Loss: 0.8611, Accuracy: 70.20%\n",
      "Validation Loss: 0.6151, Accuracy: 78.94%\n",
      "Training Epoch 132: Loss: 0.8513, Accuracy: 70.44%\n",
      "Validation Loss: 0.6135, Accuracy: 79.47%\n",
      "Training Epoch 133: Loss: 0.8538, Accuracy: 70.62%\n",
      "Validation Loss: 0.6116, Accuracy: 78.80%\n",
      "Training Epoch 134: Loss: 0.8574, Accuracy: 70.22%\n",
      "Validation Loss: 0.6151, Accuracy: 78.76%\n",
      "Training Epoch 135: Loss: 0.8546, Accuracy: 70.44%\n",
      "Validation Loss: 0.6059, Accuracy: 79.77%\n",
      "Training Epoch 136: Loss: 0.8491, Accuracy: 70.79%\n",
      "Validation Loss: 0.6072, Accuracy: 79.22%\n",
      "Training Epoch 137: Loss: 0.8446, Accuracy: 70.69%\n",
      "Validation Loss: 0.5959, Accuracy: 79.64%\n",
      "Training Epoch 138: Loss: 0.8435, Accuracy: 70.87%\n",
      "Validation Loss: 0.5964, Accuracy: 79.76%\n",
      "Training Epoch 139: Loss: 0.8392, Accuracy: 70.71%\n",
      "Validation Loss: 0.5988, Accuracy: 79.52%\n",
      "Training Epoch 140: Loss: 0.8321, Accuracy: 71.20%\n",
      "Validation Loss: 0.5945, Accuracy: 79.62%\n",
      "Training Epoch 141: Loss: 0.8298, Accuracy: 71.20%\n",
      "Validation Loss: 0.5911, Accuracy: 79.54%\n",
      "Training Epoch 142: Loss: 0.8283, Accuracy: 71.27%\n",
      "Validation Loss: 0.5909, Accuracy: 79.73%\n",
      "Training Epoch 143: Loss: 0.8237, Accuracy: 71.58%\n",
      "Validation Loss: 0.5815, Accuracy: 79.71%\n",
      "Training Epoch 144: Loss: 0.8240, Accuracy: 71.58%\n",
      "Validation Loss: 0.5861, Accuracy: 79.59%\n",
      "Training Epoch 145: Loss: 0.8238, Accuracy: 71.61%\n",
      "Validation Loss: 0.5837, Accuracy: 80.21%\n",
      "Training Epoch 146: Loss: 0.8201, Accuracy: 71.55%\n",
      "Validation Loss: 0.5833, Accuracy: 79.64%\n",
      "Training Epoch 147: Loss: 0.8143, Accuracy: 71.65%\n",
      "Validation Loss: 0.5852, Accuracy: 79.98%\n",
      "Training Epoch 148: Loss: 0.8132, Accuracy: 71.71%\n",
      "Validation Loss: 0.5774, Accuracy: 79.97%\n",
      "Training Epoch 149: Loss: 0.8138, Accuracy: 71.92%\n",
      "Validation Loss: 0.5846, Accuracy: 79.91%\n",
      "Training Epoch 150: Loss: 0.8086, Accuracy: 72.08%\n",
      "Validation Loss: 0.5764, Accuracy: 80.28%\n",
      "Training Epoch 151: Loss: 0.8072, Accuracy: 72.17%\n",
      "Validation Loss: 0.5758, Accuracy: 80.61%\n",
      "Training Epoch 152: Loss: 0.8070, Accuracy: 72.04%\n",
      "Validation Loss: 0.5792, Accuracy: 80.09%\n",
      "Training Epoch 153: Loss: 0.8002, Accuracy: 72.28%\n",
      "Validation Loss: 0.5737, Accuracy: 80.63%\n",
      "Training Epoch 154: Loss: 0.7961, Accuracy: 72.55%\n",
      "Validation Loss: 0.5685, Accuracy: 80.82%\n",
      "Training Epoch 155: Loss: 0.7956, Accuracy: 72.72%\n",
      "Validation Loss: 0.5767, Accuracy: 80.54%\n",
      "Training Epoch 156: Loss: 0.7975, Accuracy: 72.55%\n",
      "Validation Loss: 0.5659, Accuracy: 80.46%\n",
      "Training Epoch 157: Loss: 0.7890, Accuracy: 72.78%\n",
      "Validation Loss: 0.5647, Accuracy: 80.62%\n",
      "Training Epoch 158: Loss: 0.7901, Accuracy: 72.77%\n",
      "Validation Loss: 0.5656, Accuracy: 80.63%\n",
      "Training Epoch 159: Loss: 0.7885, Accuracy: 72.81%\n",
      "Validation Loss: 0.5652, Accuracy: 80.55%\n",
      "Training Epoch 160: Loss: 0.7889, Accuracy: 72.94%\n",
      "Validation Loss: 0.5627, Accuracy: 80.46%\n",
      "Training Epoch 161: Loss: 0.7842, Accuracy: 72.95%\n",
      "Validation Loss: 0.5582, Accuracy: 80.70%\n",
      "Training Epoch 162: Loss: 0.7697, Accuracy: 73.66%\n",
      "Validation Loss: 0.5574, Accuracy: 80.79%\n",
      "Training Epoch 163: Loss: 0.7796, Accuracy: 73.08%\n",
      "Validation Loss: 0.5602, Accuracy: 80.94%\n",
      "Training Epoch 164: Loss: 0.7708, Accuracy: 73.45%\n",
      "Validation Loss: 0.5537, Accuracy: 80.83%\n",
      "Training Epoch 165: Loss: 0.7745, Accuracy: 73.42%\n",
      "Validation Loss: 0.5486, Accuracy: 81.10%\n",
      "Training Epoch 166: Loss: 0.7692, Accuracy: 73.62%\n",
      "Validation Loss: 0.5526, Accuracy: 80.91%\n",
      "Training Epoch 167: Loss: 0.7683, Accuracy: 73.61%\n",
      "Validation Loss: 0.5543, Accuracy: 80.93%\n",
      "Training Epoch 168: Loss: 0.7679, Accuracy: 73.54%\n",
      "Validation Loss: 0.5462, Accuracy: 81.30%\n",
      "Training Epoch 169: Loss: 0.7627, Accuracy: 73.77%\n",
      "Validation Loss: 0.5424, Accuracy: 81.27%\n",
      "Training Epoch 170: Loss: 0.7619, Accuracy: 73.81%\n",
      "Validation Loss: 0.5475, Accuracy: 81.34%\n",
      "Training Epoch 171: Loss: 0.7657, Accuracy: 73.79%\n",
      "Validation Loss: 0.5438, Accuracy: 81.35%\n",
      "Training Epoch 172: Loss: 0.7557, Accuracy: 73.83%\n",
      "Validation Loss: 0.5402, Accuracy: 81.51%\n",
      "Training Epoch 173: Loss: 0.7498, Accuracy: 74.05%\n",
      "Validation Loss: 0.5454, Accuracy: 81.63%\n",
      "Training Epoch 174: Loss: 0.7491, Accuracy: 74.14%\n",
      "Validation Loss: 0.5411, Accuracy: 81.82%\n",
      "Training Epoch 175: Loss: 0.7568, Accuracy: 73.96%\n",
      "Validation Loss: 0.5352, Accuracy: 81.89%\n",
      "Training Epoch 176: Loss: 0.7491, Accuracy: 74.25%\n",
      "Validation Loss: 0.5360, Accuracy: 81.51%\n",
      "Training Epoch 177: Loss: 0.7485, Accuracy: 74.20%\n",
      "Validation Loss: 0.5357, Accuracy: 81.61%\n",
      "Training Epoch 178: Loss: 0.7443, Accuracy: 74.38%\n",
      "Validation Loss: 0.5329, Accuracy: 81.89%\n",
      "Training Epoch 179: Loss: 0.7420, Accuracy: 74.46%\n",
      "Validation Loss: 0.5414, Accuracy: 81.63%\n",
      "Training Epoch 180: Loss: 0.7449, Accuracy: 74.39%\n",
      "Validation Loss: 0.5535, Accuracy: 81.07%\n",
      "Training Epoch 181: Loss: 0.7395, Accuracy: 74.38%\n",
      "Validation Loss: 0.5341, Accuracy: 81.58%\n",
      "Training Epoch 182: Loss: 0.7345, Accuracy: 74.71%\n",
      "Validation Loss: 0.5284, Accuracy: 82.18%\n",
      "Training Epoch 183: Loss: 0.7305, Accuracy: 74.75%\n",
      "Validation Loss: 0.5275, Accuracy: 81.79%\n",
      "Training Epoch 184: Loss: 0.7326, Accuracy: 74.74%\n",
      "Validation Loss: 0.5376, Accuracy: 81.28%\n",
      "Training Epoch 185: Loss: 0.7311, Accuracy: 74.81%\n",
      "Validation Loss: 0.5282, Accuracy: 81.84%\n",
      "Training Epoch 186: Loss: 0.7215, Accuracy: 74.95%\n",
      "Validation Loss: 0.5238, Accuracy: 81.99%\n",
      "Training Epoch 187: Loss: 0.7211, Accuracy: 75.09%\n",
      "Validation Loss: 0.5182, Accuracy: 81.99%\n",
      "Training Epoch 188: Loss: 0.7287, Accuracy: 74.70%\n",
      "Validation Loss: 0.5227, Accuracy: 82.06%\n",
      "Training Epoch 189: Loss: 0.7211, Accuracy: 75.19%\n",
      "Validation Loss: 0.5209, Accuracy: 82.22%\n",
      "Training Epoch 190: Loss: 0.7149, Accuracy: 75.23%\n",
      "Validation Loss: 0.5186, Accuracy: 82.30%\n",
      "Training Epoch 191: Loss: 0.7191, Accuracy: 75.16%\n",
      "Validation Loss: 0.5154, Accuracy: 82.60%\n",
      "Training Epoch 192: Loss: 0.7122, Accuracy: 75.54%\n",
      "Validation Loss: 0.5127, Accuracy: 82.60%\n",
      "Training Epoch 193: Loss: 0.7125, Accuracy: 75.28%\n",
      "Validation Loss: 0.5167, Accuracy: 82.09%\n",
      "Training Epoch 194: Loss: 0.7122, Accuracy: 75.41%\n",
      "Validation Loss: 0.5133, Accuracy: 82.24%\n",
      "Training Epoch 195: Loss: 0.7123, Accuracy: 75.44%\n",
      "Validation Loss: 0.5182, Accuracy: 82.38%\n",
      "Training Epoch 196: Loss: 0.7088, Accuracy: 75.47%\n",
      "Validation Loss: 0.5124, Accuracy: 82.41%\n",
      "Training Epoch 197: Loss: 0.7046, Accuracy: 75.75%\n",
      "Validation Loss: 0.5159, Accuracy: 82.60%\n",
      "Training Epoch 198: Loss: 0.7068, Accuracy: 75.76%\n",
      "Validation Loss: 0.5094, Accuracy: 82.41%\n",
      "Training Epoch 199: Loss: 0.7037, Accuracy: 76.00%\n",
      "Validation Loss: 0.5145, Accuracy: 82.54%\n",
      "Training Epoch 200: Loss: 0.7040, Accuracy: 75.78%\n",
      "Validation Loss: 0.5012, Accuracy: 82.85%\n",
      "Training Epoch 201: Loss: 0.6970, Accuracy: 76.02%\n",
      "Validation Loss: 0.5057, Accuracy: 82.54%\n",
      "Training Epoch 202: Loss: 0.6983, Accuracy: 76.08%\n",
      "Validation Loss: 0.5071, Accuracy: 82.72%\n",
      "Training Epoch 203: Loss: 0.6982, Accuracy: 76.02%\n",
      "Validation Loss: 0.5125, Accuracy: 82.40%\n",
      "Training Epoch 204: Loss: 0.6972, Accuracy: 76.11%\n",
      "Validation Loss: 0.5068, Accuracy: 82.44%\n",
      "Training Epoch 205: Loss: 0.6898, Accuracy: 76.33%\n",
      "Validation Loss: 0.5054, Accuracy: 82.41%\n",
      "Training Epoch 206: Loss: 0.6895, Accuracy: 76.27%\n",
      "Validation Loss: 0.4974, Accuracy: 82.90%\n",
      "Training Epoch 207: Loss: 0.6864, Accuracy: 76.63%\n",
      "Validation Loss: 0.5016, Accuracy: 82.86%\n",
      "Training Epoch 208: Loss: 0.6849, Accuracy: 76.41%\n",
      "Validation Loss: 0.4926, Accuracy: 83.09%\n",
      "Training Epoch 209: Loss: 0.6778, Accuracy: 76.44%\n",
      "Validation Loss: 0.4929, Accuracy: 82.79%\n",
      "Training Epoch 210: Loss: 0.6873, Accuracy: 76.49%\n",
      "Validation Loss: 0.4973, Accuracy: 83.00%\n",
      "Training Epoch 211: Loss: 0.6804, Accuracy: 76.51%\n",
      "Validation Loss: 0.4958, Accuracy: 82.70%\n",
      "Training Epoch 212: Loss: 0.6859, Accuracy: 76.58%\n",
      "Validation Loss: 0.4994, Accuracy: 82.70%\n",
      "Training Epoch 213: Loss: 0.6827, Accuracy: 76.62%\n",
      "Validation Loss: 0.4929, Accuracy: 82.89%\n",
      "Training Epoch 214: Loss: 0.6802, Accuracy: 76.73%\n",
      "Validation Loss: 0.4991, Accuracy: 82.88%\n",
      "Training Epoch 215: Loss: 0.6779, Accuracy: 76.64%\n",
      "Validation Loss: 0.4941, Accuracy: 83.25%\n",
      "Training Epoch 216: Loss: 0.6748, Accuracy: 76.83%\n",
      "Validation Loss: 0.4963, Accuracy: 82.85%\n",
      "Training Epoch 217: Loss: 0.6679, Accuracy: 77.06%\n",
      "Validation Loss: 0.4896, Accuracy: 83.21%\n",
      "Training Epoch 218: Loss: 0.6794, Accuracy: 76.93%\n",
      "Validation Loss: 0.4915, Accuracy: 82.97%\n",
      "Training Epoch 219: Loss: 0.6670, Accuracy: 77.20%\n",
      "Validation Loss: 0.4946, Accuracy: 82.66%\n",
      "Training Epoch 220: Loss: 0.6722, Accuracy: 77.03%\n",
      "Validation Loss: 0.4852, Accuracy: 83.39%\n",
      "Training Epoch 221: Loss: 0.6624, Accuracy: 77.22%\n",
      "Validation Loss: 0.4903, Accuracy: 82.79%\n",
      "Training Epoch 222: Loss: 0.6640, Accuracy: 77.24%\n",
      "Validation Loss: 0.4873, Accuracy: 83.07%\n",
      "Training Epoch 223: Loss: 0.6666, Accuracy: 77.09%\n",
      "Validation Loss: 0.4867, Accuracy: 82.93%\n",
      "Training Epoch 224: Loss: 0.6561, Accuracy: 77.51%\n",
      "Validation Loss: 0.4871, Accuracy: 83.04%\n",
      "Training Epoch 225: Loss: 0.6591, Accuracy: 77.32%\n",
      "Validation Loss: 0.4835, Accuracy: 83.08%\n",
      "Training Epoch 226: Loss: 0.6585, Accuracy: 77.32%\n",
      "Validation Loss: 0.4900, Accuracy: 82.91%\n",
      "Training Epoch 227: Loss: 0.6550, Accuracy: 77.64%\n",
      "Validation Loss: 0.4810, Accuracy: 83.55%\n",
      "Training Epoch 228: Loss: 0.6508, Accuracy: 77.84%\n",
      "Validation Loss: 0.4818, Accuracy: 83.27%\n",
      "Training Epoch 229: Loss: 0.6520, Accuracy: 77.69%\n",
      "Validation Loss: 0.4765, Accuracy: 83.34%\n",
      "Training Epoch 230: Loss: 0.6569, Accuracy: 77.38%\n",
      "Validation Loss: 0.4830, Accuracy: 83.11%\n",
      "Training Epoch 231: Loss: 0.6485, Accuracy: 77.86%\n",
      "Validation Loss: 0.4803, Accuracy: 83.05%\n",
      "Training Epoch 232: Loss: 0.6499, Accuracy: 77.89%\n",
      "Validation Loss: 0.4784, Accuracy: 83.34%\n",
      "Training Epoch 233: Loss: 0.6428, Accuracy: 77.92%\n",
      "Validation Loss: 0.4763, Accuracy: 83.82%\n",
      "Training Epoch 234: Loss: 0.6473, Accuracy: 77.91%\n",
      "Validation Loss: 0.4808, Accuracy: 83.23%\n",
      "Training Epoch 235: Loss: 0.6418, Accuracy: 78.08%\n",
      "Validation Loss: 0.4749, Accuracy: 83.80%\n",
      "Training Epoch 236: Loss: 0.6421, Accuracy: 78.00%\n",
      "Validation Loss: 0.4811, Accuracy: 83.29%\n",
      "Training Epoch 237: Loss: 0.6326, Accuracy: 78.14%\n",
      "Validation Loss: 0.4736, Accuracy: 83.65%\n",
      "Training Epoch 238: Loss: 0.6425, Accuracy: 77.81%\n",
      "Validation Loss: 0.4790, Accuracy: 83.53%\n",
      "Training Epoch 239: Loss: 0.6395, Accuracy: 78.03%\n",
      "Validation Loss: 0.4781, Accuracy: 83.53%\n",
      "Training Epoch 240: Loss: 0.6359, Accuracy: 78.23%\n",
      "Validation Loss: 0.4766, Accuracy: 83.34%\n",
      "Training Epoch 241: Loss: 0.6298, Accuracy: 78.32%\n",
      "Validation Loss: 0.4711, Accuracy: 83.85%\n",
      "Training Epoch 242: Loss: 0.6326, Accuracy: 78.14%\n",
      "Validation Loss: 0.4715, Accuracy: 83.82%\n",
      "Training Epoch 243: Loss: 0.6389, Accuracy: 78.23%\n",
      "Validation Loss: 0.4743, Accuracy: 83.79%\n",
      "Training Epoch 244: Loss: 0.6322, Accuracy: 78.64%\n",
      "Validation Loss: 0.4663, Accuracy: 83.82%\n",
      "Training Epoch 245: Loss: 0.6296, Accuracy: 78.42%\n",
      "Validation Loss: 0.4638, Accuracy: 83.88%\n",
      "Training Epoch 246: Loss: 0.6208, Accuracy: 78.85%\n",
      "Validation Loss: 0.4673, Accuracy: 83.99%\n",
      "Training Epoch 247: Loss: 0.6268, Accuracy: 78.53%\n",
      "Validation Loss: 0.4733, Accuracy: 83.69%\n",
      "Training Epoch 248: Loss: 0.6279, Accuracy: 78.33%\n",
      "Validation Loss: 0.4684, Accuracy: 83.95%\n",
      "Training Epoch 249: Loss: 0.6232, Accuracy: 78.75%\n",
      "Validation Loss: 0.4643, Accuracy: 84.06%\n",
      "Training Epoch 250: Loss: 0.6252, Accuracy: 78.50%\n",
      "Validation Loss: 0.4643, Accuracy: 83.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▃▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>validation_loss</td><td>█▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>250</td></tr><tr><td>train_accuracy</td><td>78.5025</td></tr><tr><td>train_loss</td><td>0.62523</td></tr><tr><td>validation_accuracy</td><td>83.97</td></tr><tr><td>validation_loss</td><td>0.46428</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_5th_train_with_extra_tuning</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/eeydko88' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/eeydko88</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241115_095818-eeydko88/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration with AutoAugment, increased weight_decay, and other techniques\n",
    "config_modified = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-3,\n",
    "    'num_epochs': 250,\n",
    "    'model_name': 'resnet50',\n",
    "    'wandb_project_name': 'CIFAR10_training_with_various_models',\n",
    "    \"checkpoint_save_interval\": 10,\n",
    "    \"checkpoint_path\": \"checkpoints/checkpoint_modified.pth\",\n",
    "    \"best_model_path\": \"checkpoints/best_model_modified.pth\",\n",
    "    \"load_from_checkpoint\": None,\n",
    "    'test_mode': False  # 기본적으로 학습 모드\n",
    "}\n",
    "\n",
    "# Model with extended hidden dimensions, additional layer, and stochastic depth\n",
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50()\n",
    "        model.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(model.fc.in_features, 1024),  # 확장된 hidden dimension\n",
    "            torch.nn.BatchNorm1d(1024),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(1024, 512),                   # 추가된 hidden layer\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(512, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "    return model\n",
    "\n",
    "def load_cifar10_dataloaders(data_root_dir, device, batch_size, num_worker):\n",
    "    validation_size = 0.2\n",
    "    random_seed = 42\n",
    "    normalize = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "\n",
    "    # Data Augmentation for training set with AutoAugment\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    test_transforms = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=train_transforms)\n",
    "    val_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=test_transforms)\n",
    "    test_dataset = datasets.CIFAR10(root=data_root_dir, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    train_indices, val_indices = train_test_split(np.arange(len(train_dataset)), test_size=validation_size, random_state=random_seed)\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    kwargs = {'pin_memory': True} if device.startswith(\"cuda\") else {}\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_worker, **kwargs)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_worker, **kwargs)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_worker, **kwargs)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader, num_classes\n",
    "\n",
    "# Training loop without Gradient Clipping\n",
    "def train_loop(model, device, train_dataloader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step(closure) \n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Training Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluation_loop(model, device, dataloader, criterion, phase=\"validation\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"{phase.capitalize()} Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    if phase == \"test\":\n",
    "        print(f\"Test Accuracy: {accuracy:.2f}\")  # Add summary test accuracy output\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "def train_main_modified(config):\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "    checkpoint_save_interval = config.get('checkpoint_save_interval', 10)\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "    test_mode = config.get('test_mode', False)\n",
    "    best_acc = 0\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.finish()\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config,\n",
    "        name=\"ResNet50_5th_train_with_extra_tuning\"\n",
    "    )\n",
    "\n",
    "    # Set Device\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Model Setup\n",
    "    model = get_model(config[\"model_name\"], 10, config).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Optimizer and Scheduler Setup\n",
    "    optimizer = SAM(model.parameters(), base_optimizer=torch.optim.SGD, lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "    # Load Data\n",
    "    train_dataloader, val_dataloader, test_dataloader, _ = load_cifar10_dataloaders(data_root_dir, device, batch_size, num_worker)\n",
    "\n",
    "    # Load checkpoint if needed\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = best_model_path if load_from_checkpoint == \"best\" else checkpoint_path\n",
    "        start_epoch, best_acc = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    # Test mode: evaluate only on the test set and skip training\n",
    "    if test_mode:\n",
    "        print(\"Running in test mode...\")\n",
    "        test_acc, test_loss = evaluation_loop(model, device, test_dataloader, criterion, phase=\"test\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "        wandb.log({\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_acc\n",
    "        })\n",
    "        wandb.finish()\n",
    "        return\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_loss, train_acc = train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "        val_acc, val_loss = evaluation_loop(model, device, val_dataloader, criterion, phase=\"validation\")\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_acc,\n",
    "            'validation_loss': val_loss,\n",
    "            'validation_accuracy': val_acc\n",
    "        })\n",
    "\n",
    "        # Save checkpoint periodically or if it's the final epoch\n",
    "        if (epoch + 1) % checkpoint_save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "            is_best = val_acc > best_acc\n",
    "            best_acc = max(val_acc, best_acc)\n",
    "            save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_acc, is_best, best_model_path)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the modified training function to perform the experiment with AutoAugment\n",
    "train_main_modified(config_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0dd183-ad95-4862-bfca-3c2f9bf013d4",
   "metadata": {},
   "source": [
    "## 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ff84aba-ee90-492b-9f43-fc530e7a8130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241115_104634-7sqt52tf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/7sqt52tf' target=\"_blank\">ResNet50_5th_train_with_extra_tuning</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/7sqt52tf' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/7sqt52tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model resnet50 with 26139210 parameters (26139210 trainable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "=> loaded checkpoint 'checkpoints/best_model_modified.pth' (epoch 250)\n",
      "Running in test mode...\n",
      "Test Loss: 0.4631, Accuracy: 83.83%\n",
      "Test Accuracy: 83.83\n",
      "Test Loss: 0.4631, Test Accuracy: 83.83%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>83.83</td></tr><tr><td>test_loss</td><td>0.46308</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_5th_train_with_extra_tuning</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/7sqt52tf' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/7sqt52tf</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241115_104634-7sqt52tf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration for Test Mode\n",
    "config_testmode = {\n",
    "    **config_modified,\n",
    "    'test_mode': True,  # 평가 모드 설정\n",
    "    'load_from_checkpoint': 'best'\n",
    "}\n",
    "\n",
    "# Run the model in test mode for evaluation\n",
    "train_main_modified(config_testmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088e7c2-f916-4aa6-a6a6-d6d4228dcb08",
   "metadata": {},
   "source": [
    "# 여섯 번째 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a629d7-df71-462d-a03b-b4c99ff4f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Optimizer\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import AutoAugmentPolicy\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# SAM Optimizer Implementation (이미 작성된 SAM 코드 그대로 사용)\n",
    "class SAM(Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, \"SAM requires non-negative rho.\"\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        scale = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (scale + 1e-12)\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * (torch.abs(p) if group[\"adaptive\"] else 1.0) * scale.to(p)\n",
    "                p.add_(e_w)\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])\n",
    "        self.base_optimizer.step()\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device\n",
    "        norm = torch.norm(\n",
    "            torch.stack([\n",
    "                p.grad.norm(p=2).to(shared_device)\n",
    "                for group in self.param_groups for p in group[\"params\"]\n",
    "                if p.grad is not None\n",
    "            ]),\n",
    "            p=2\n",
    "        )\n",
    "        return norm\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"SAM requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d9cb51-f331-4dd6-afc1-8f843883937d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241115_110550-e33sknvp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/e33sknvp' target=\"_blank\">ResNet50_6th_train</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/e33sknvp' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/e33sknvp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model resnet50 with 26139210 parameters (26139210 trainable)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training Epoch 1: Loss: 2.1264, Accuracy: 20.65%\n",
      "Validation Loss: 2.0000, Accuracy: 30.28%\n",
      "Training Epoch 2: Loss: 1.8828, Accuracy: 30.15%\n",
      "Validation Loss: 1.6168, Accuracy: 42.90%\n",
      "Training Epoch 3: Loss: 1.7640, Accuracy: 35.32%\n",
      "Validation Loss: 1.4353, Accuracy: 46.29%\n",
      "Training Epoch 4: Loss: 1.6590, Accuracy: 39.39%\n",
      "Validation Loss: 1.3147, Accuracy: 50.59%\n",
      "Training Epoch 5: Loss: 1.5688, Accuracy: 43.25%\n",
      "Validation Loss: 1.2845, Accuracy: 52.83%\n",
      "Training Epoch 6: Loss: 1.5334, Accuracy: 44.33%\n",
      "Validation Loss: 1.1753, Accuracy: 57.10%\n",
      "Training Epoch 7: Loss: 1.4028, Accuracy: 49.73%\n",
      "Validation Loss: 1.0753, Accuracy: 61.48%\n",
      "Training Epoch 8: Loss: 1.3727, Accuracy: 51.22%\n",
      "Validation Loss: 0.9925, Accuracy: 64.09%\n",
      "Training Epoch 9: Loss: 1.2705, Accuracy: 54.88%\n",
      "Validation Loss: 0.9762, Accuracy: 66.13%\n",
      "Training Epoch 10: Loss: 1.2125, Accuracy: 57.21%\n",
      "Validation Loss: 0.8907, Accuracy: 68.23%\n",
      "Training Epoch 11: Loss: 1.1716, Accuracy: 59.09%\n",
      "Validation Loss: 0.8372, Accuracy: 70.69%\n",
      "Training Epoch 12: Loss: 1.1110, Accuracy: 61.31%\n",
      "Validation Loss: 0.7833, Accuracy: 72.71%\n",
      "Training Epoch 13: Loss: 1.0688, Accuracy: 62.79%\n",
      "Validation Loss: 0.7630, Accuracy: 74.19%\n",
      "Training Epoch 14: Loss: 1.0285, Accuracy: 64.58%\n",
      "Validation Loss: 0.7426, Accuracy: 74.36%\n",
      "Training Epoch 15: Loss: 1.0042, Accuracy: 65.31%\n",
      "Validation Loss: 0.7169, Accuracy: 75.67%\n",
      "Training Epoch 16: Loss: 0.9752, Accuracy: 66.47%\n",
      "Validation Loss: 0.6783, Accuracy: 77.08%\n",
      "Training Epoch 17: Loss: 0.9363, Accuracy: 67.76%\n",
      "Validation Loss: 0.7096, Accuracy: 75.54%\n",
      "Training Epoch 18: Loss: 0.9183, Accuracy: 68.47%\n",
      "Validation Loss: 0.6428, Accuracy: 77.77%\n",
      "Training Epoch 19: Loss: 0.8871, Accuracy: 69.60%\n",
      "Validation Loss: 0.6358, Accuracy: 77.97%\n",
      "Training Epoch 20: Loss: 0.8748, Accuracy: 70.31%\n",
      "Validation Loss: 0.6204, Accuracy: 78.45%\n",
      "Training Epoch 21: Loss: 0.8623, Accuracy: 70.53%\n",
      "Validation Loss: 0.6097, Accuracy: 79.30%\n",
      "Training Epoch 22: Loss: 0.8276, Accuracy: 71.79%\n",
      "Validation Loss: 0.6027, Accuracy: 79.54%\n",
      "Training Epoch 23: Loss: 0.8144, Accuracy: 72.27%\n",
      "Validation Loss: 0.5782, Accuracy: 80.29%\n",
      "Training Epoch 24: Loss: 0.7922, Accuracy: 72.73%\n",
      "Validation Loss: 0.5577, Accuracy: 80.91%\n",
      "Training Epoch 25: Loss: 0.7791, Accuracy: 73.20%\n",
      "Validation Loss: 0.5602, Accuracy: 81.15%\n",
      "Training Epoch 26: Loss: 0.7672, Accuracy: 73.62%\n",
      "Validation Loss: 0.5281, Accuracy: 82.31%\n",
      "Training Epoch 27: Loss: 0.7511, Accuracy: 74.25%\n",
      "Validation Loss: 0.5298, Accuracy: 81.66%\n",
      "Training Epoch 28: Loss: 0.7427, Accuracy: 74.71%\n",
      "Validation Loss: 0.5093, Accuracy: 82.49%\n",
      "Training Epoch 29: Loss: 0.7261, Accuracy: 75.25%\n",
      "Validation Loss: 0.5167, Accuracy: 82.25%\n",
      "Training Epoch 30: Loss: 0.7097, Accuracy: 75.55%\n",
      "Validation Loss: 0.5194, Accuracy: 82.82%\n",
      "Training Epoch 31: Loss: 0.7060, Accuracy: 76.03%\n",
      "Validation Loss: 0.4977, Accuracy: 83.11%\n",
      "Training Epoch 32: Loss: 0.6981, Accuracy: 76.24%\n",
      "Validation Loss: 0.4891, Accuracy: 83.48%\n",
      "Training Epoch 33: Loss: 0.6877, Accuracy: 76.38%\n",
      "Validation Loss: 0.4774, Accuracy: 83.85%\n",
      "Training Epoch 34: Loss: 0.6728, Accuracy: 77.05%\n",
      "Validation Loss: 0.5332, Accuracy: 81.63%\n",
      "Training Epoch 35: Loss: 0.6621, Accuracy: 77.36%\n",
      "Validation Loss: 0.4655, Accuracy: 84.11%\n",
      "Training Epoch 36: Loss: 0.6560, Accuracy: 77.69%\n",
      "Validation Loss: 0.4845, Accuracy: 83.11%\n",
      "Training Epoch 37: Loss: 0.6515, Accuracy: 77.81%\n",
      "Validation Loss: 0.4697, Accuracy: 84.16%\n",
      "Training Epoch 38: Loss: 0.6457, Accuracy: 78.10%\n",
      "Validation Loss: 0.4861, Accuracy: 83.55%\n",
      "Training Epoch 39: Loss: 0.6234, Accuracy: 78.55%\n",
      "Validation Loss: 0.4498, Accuracy: 84.53%\n",
      "Training Epoch 40: Loss: 0.6509, Accuracy: 77.77%\n",
      "Validation Loss: 0.4552, Accuracy: 84.61%\n",
      "Training Epoch 41: Loss: 0.6184, Accuracy: 78.86%\n",
      "Validation Loss: 0.4547, Accuracy: 84.50%\n",
      "Training Epoch 42: Loss: 0.6048, Accuracy: 79.45%\n",
      "Validation Loss: 0.4396, Accuracy: 84.99%\n",
      "Training Epoch 43: Loss: 0.5997, Accuracy: 79.42%\n",
      "Validation Loss: 0.4481, Accuracy: 84.87%\n",
      "Training Epoch 44: Loss: 0.5923, Accuracy: 79.53%\n",
      "Validation Loss: 0.4319, Accuracy: 84.86%\n",
      "Training Epoch 45: Loss: 0.5810, Accuracy: 80.28%\n",
      "Validation Loss: 0.4218, Accuracy: 85.45%\n",
      "Training Epoch 46: Loss: 0.5731, Accuracy: 80.47%\n",
      "Validation Loss: 0.4327, Accuracy: 85.65%\n",
      "Training Epoch 47: Loss: 0.5766, Accuracy: 80.23%\n",
      "Validation Loss: 0.4478, Accuracy: 84.35%\n",
      "Training Epoch 48: Loss: 0.5699, Accuracy: 80.51%\n",
      "Validation Loss: 0.4111, Accuracy: 86.24%\n",
      "Training Epoch 49: Loss: 0.5650, Accuracy: 80.52%\n",
      "Validation Loss: 0.4271, Accuracy: 85.64%\n",
      "Training Epoch 50: Loss: 0.5569, Accuracy: 80.90%\n",
      "Validation Loss: 0.4151, Accuracy: 85.93%\n",
      "Training Epoch 51: Loss: 0.5557, Accuracy: 80.92%\n",
      "Validation Loss: 0.4179, Accuracy: 85.87%\n",
      "Training Epoch 52: Loss: 0.5418, Accuracy: 81.35%\n",
      "Validation Loss: 0.3993, Accuracy: 86.55%\n",
      "Training Epoch 53: Loss: 0.5382, Accuracy: 81.37%\n",
      "Validation Loss: 0.3951, Accuracy: 86.74%\n",
      "Training Epoch 54: Loss: 0.5352, Accuracy: 81.62%\n",
      "Validation Loss: 0.3999, Accuracy: 86.67%\n",
      "Training Epoch 55: Loss: 0.5236, Accuracy: 82.08%\n",
      "Validation Loss: 0.3976, Accuracy: 86.85%\n",
      "Training Epoch 56: Loss: 0.5349, Accuracy: 81.59%\n",
      "Validation Loss: 0.4082, Accuracy: 86.47%\n",
      "Training Epoch 57: Loss: 0.5114, Accuracy: 82.37%\n",
      "Validation Loss: 0.3947, Accuracy: 86.64%\n",
      "Training Epoch 58: Loss: 0.5150, Accuracy: 82.38%\n",
      "Validation Loss: 0.4104, Accuracy: 86.21%\n",
      "Training Epoch 59: Loss: 0.5110, Accuracy: 82.56%\n",
      "Validation Loss: 0.4031, Accuracy: 86.29%\n",
      "Training Epoch 60: Loss: 0.5020, Accuracy: 82.60%\n",
      "Validation Loss: 0.3961, Accuracy: 86.71%\n",
      "Training Epoch 61: Loss: 0.5147, Accuracy: 82.28%\n",
      "Validation Loss: 0.4228, Accuracy: 85.49%\n",
      "Training Epoch 62: Loss: 0.5022, Accuracy: 82.66%\n",
      "Validation Loss: 0.4046, Accuracy: 86.54%\n",
      "Training Epoch 63: Loss: 0.4980, Accuracy: 82.68%\n",
      "Validation Loss: 0.3990, Accuracy: 86.39%\n",
      "Training Epoch 64: Loss: 0.4925, Accuracy: 83.23%\n",
      "Validation Loss: 0.3834, Accuracy: 86.92%\n",
      "Training Epoch 65: Loss: 0.4788, Accuracy: 83.63%\n",
      "Validation Loss: 0.4730, Accuracy: 85.74%\n",
      "Training Epoch 66: Loss: 0.4769, Accuracy: 83.44%\n",
      "Validation Loss: 0.3852, Accuracy: 86.87%\n",
      "Training Epoch 67: Loss: 0.4857, Accuracy: 83.36%\n",
      "Validation Loss: 0.4184, Accuracy: 86.14%\n",
      "Training Epoch 68: Loss: 0.4699, Accuracy: 83.62%\n",
      "Validation Loss: 0.4264, Accuracy: 86.23%\n",
      "Training Epoch 69: Loss: 0.4678, Accuracy: 83.92%\n",
      "Validation Loss: 0.3958, Accuracy: 87.03%\n",
      "Training Epoch 70: Loss: 0.4655, Accuracy: 84.13%\n",
      "Validation Loss: 0.3748, Accuracy: 87.48%\n",
      "Training Epoch 71: Loss: 0.4546, Accuracy: 84.44%\n",
      "Validation Loss: 0.3901, Accuracy: 87.17%\n",
      "Training Epoch 72: Loss: 0.4536, Accuracy: 84.30%\n",
      "Validation Loss: 0.4019, Accuracy: 86.69%\n",
      "Training Epoch 73: Loss: 0.4634, Accuracy: 84.17%\n",
      "Validation Loss: 0.4098, Accuracy: 86.67%\n",
      "Training Epoch 74: Loss: 0.4640, Accuracy: 83.86%\n",
      "Validation Loss: 0.3939, Accuracy: 87.08%\n",
      "Training Epoch 75: Loss: 0.4449, Accuracy: 84.58%\n",
      "Validation Loss: 0.3698, Accuracy: 87.82%\n",
      "Training Epoch 76: Loss: 0.4342, Accuracy: 85.33%\n",
      "Validation Loss: 0.3824, Accuracy: 87.22%\n",
      "Training Epoch 77: Loss: 0.4283, Accuracy: 85.31%\n",
      "Validation Loss: 0.3735, Accuracy: 87.47%\n",
      "Training Epoch 78: Loss: 0.4825, Accuracy: 83.51%\n",
      "Validation Loss: 0.4703, Accuracy: 84.55%\n",
      "Training Epoch 79: Loss: 0.4455, Accuracy: 84.69%\n",
      "Validation Loss: 0.3746, Accuracy: 87.35%\n",
      "Training Epoch 80: Loss: 0.4292, Accuracy: 85.29%\n",
      "Validation Loss: 0.3694, Accuracy: 87.78%\n",
      "Training Epoch 81: Loss: 0.4203, Accuracy: 85.69%\n",
      "Validation Loss: 0.4062, Accuracy: 87.30%\n",
      "Training Epoch 82: Loss: 0.4382, Accuracy: 84.87%\n",
      "Validation Loss: 0.3902, Accuracy: 87.15%\n",
      "Training Epoch 83: Loss: 0.4176, Accuracy: 85.51%\n",
      "Validation Loss: 0.3799, Accuracy: 87.10%\n",
      "Training Epoch 84: Loss: 0.4225, Accuracy: 85.52%\n",
      "Validation Loss: 0.3758, Accuracy: 87.85%\n",
      "Training Epoch 85: Loss: 0.4113, Accuracy: 85.95%\n",
      "Validation Loss: 0.3688, Accuracy: 87.99%\n",
      "Training Epoch 86: Loss: 0.4099, Accuracy: 86.04%\n",
      "Validation Loss: 0.3630, Accuracy: 87.96%\n",
      "Training Epoch 87: Loss: 0.4023, Accuracy: 86.23%\n",
      "Validation Loss: 0.3657, Accuracy: 87.76%\n",
      "Training Epoch 88: Loss: 0.4041, Accuracy: 86.20%\n",
      "Validation Loss: 0.4546, Accuracy: 87.08%\n",
      "Training Epoch 89: Loss: 0.4046, Accuracy: 85.96%\n",
      "Validation Loss: 0.3666, Accuracy: 88.49%\n",
      "Training Epoch 90: Loss: 0.4038, Accuracy: 86.21%\n",
      "Validation Loss: 0.4610, Accuracy: 86.73%\n",
      "Training Epoch 91: Loss: 0.3955, Accuracy: 86.34%\n",
      "Validation Loss: 0.3541, Accuracy: 88.44%\n",
      "Training Epoch 92: Loss: 0.3895, Accuracy: 86.62%\n",
      "Validation Loss: 0.3632, Accuracy: 88.28%\n",
      "Training Epoch 93: Loss: 0.3898, Accuracy: 86.51%\n",
      "Validation Loss: 0.3582, Accuracy: 87.96%\n",
      "Training Epoch 94: Loss: 0.3790, Accuracy: 87.09%\n",
      "Validation Loss: 0.3677, Accuracy: 88.00%\n",
      "Training Epoch 95: Loss: 0.3884, Accuracy: 86.72%\n",
      "Validation Loss: 0.3660, Accuracy: 88.07%\n",
      "Training Epoch 96: Loss: 0.3774, Accuracy: 86.90%\n",
      "Validation Loss: 0.3607, Accuracy: 88.41%\n",
      "Training Epoch 97: Loss: 0.3792, Accuracy: 86.96%\n",
      "Validation Loss: 0.3726, Accuracy: 87.55%\n",
      "Training Epoch 98: Loss: 0.3771, Accuracy: 86.90%\n",
      "Validation Loss: 0.3630, Accuracy: 88.04%\n",
      "Training Epoch 99: Loss: 0.3767, Accuracy: 87.18%\n",
      "Validation Loss: 0.3591, Accuracy: 88.67%\n",
      "Training Epoch 100: Loss: 0.3706, Accuracy: 87.24%\n",
      "Validation Loss: 0.3643, Accuracy: 88.33%\n",
      "Training Epoch 101: Loss: 0.3687, Accuracy: 87.32%\n",
      "Validation Loss: 0.3449, Accuracy: 88.46%\n",
      "Training Epoch 102: Loss: 0.3617, Accuracy: 87.40%\n",
      "Validation Loss: 0.3596, Accuracy: 88.16%\n",
      "Training Epoch 103: Loss: 0.3608, Accuracy: 87.65%\n",
      "Validation Loss: 0.3468, Accuracy: 88.92%\n",
      "Training Epoch 104: Loss: 0.3615, Accuracy: 87.61%\n",
      "Validation Loss: 0.3555, Accuracy: 88.71%\n",
      "Training Epoch 105: Loss: 0.3583, Accuracy: 87.66%\n",
      "Validation Loss: 0.3475, Accuracy: 88.58%\n",
      "Training Epoch 106: Loss: 0.3537, Accuracy: 87.86%\n",
      "Validation Loss: 0.3628, Accuracy: 88.67%\n",
      "Training Epoch 107: Loss: 0.3706, Accuracy: 87.29%\n",
      "Validation Loss: 0.3680, Accuracy: 88.44%\n",
      "Training Epoch 108: Loss: 0.3491, Accuracy: 87.79%\n",
      "Validation Loss: 0.3587, Accuracy: 88.85%\n",
      "Training Epoch 109: Loss: 0.3434, Accuracy: 88.14%\n",
      "Validation Loss: 0.3556, Accuracy: 88.65%\n",
      "Training Epoch 110: Loss: 0.3369, Accuracy: 88.43%\n",
      "Validation Loss: 0.3579, Accuracy: 88.42%\n",
      "Training Epoch 111: Loss: 0.3481, Accuracy: 88.11%\n",
      "Validation Loss: 0.3540, Accuracy: 88.89%\n",
      "Training Epoch 112: Loss: 0.3425, Accuracy: 88.34%\n",
      "Validation Loss: 0.3693, Accuracy: 88.73%\n",
      "Training Epoch 113: Loss: 0.3375, Accuracy: 88.49%\n",
      "Validation Loss: 0.3560, Accuracy: 88.60%\n",
      "Training Epoch 114: Loss: 0.3409, Accuracy: 88.38%\n",
      "Validation Loss: 0.3566, Accuracy: 88.82%\n",
      "Training Epoch 115: Loss: 0.3280, Accuracy: 88.74%\n",
      "Validation Loss: 0.3552, Accuracy: 88.88%\n",
      "Training Epoch 116: Loss: 0.3331, Accuracy: 88.54%\n",
      "Validation Loss: 0.3528, Accuracy: 89.12%\n",
      "Training Epoch 117: Loss: 0.3302, Accuracy: 88.65%\n",
      "Validation Loss: 0.3400, Accuracy: 89.02%\n",
      "Training Epoch 118: Loss: 0.3229, Accuracy: 89.01%\n",
      "Validation Loss: 0.3560, Accuracy: 88.80%\n",
      "Training Epoch 119: Loss: 0.3208, Accuracy: 88.94%\n",
      "Validation Loss: 0.3500, Accuracy: 88.80%\n",
      "Training Epoch 120: Loss: 0.3203, Accuracy: 88.97%\n",
      "Validation Loss: 0.3478, Accuracy: 89.07%\n",
      "Training Epoch 121: Loss: 0.3210, Accuracy: 89.00%\n",
      "Validation Loss: 0.3523, Accuracy: 88.69%\n",
      "Training Epoch 122: Loss: 0.3172, Accuracy: 89.06%\n",
      "Validation Loss: 0.3600, Accuracy: 88.57%\n",
      "Training Epoch 123: Loss: 0.3277, Accuracy: 88.84%\n",
      "Validation Loss: 0.3463, Accuracy: 89.21%\n",
      "Training Epoch 124: Loss: 0.3154, Accuracy: 89.21%\n",
      "Validation Loss: 0.3462, Accuracy: 89.15%\n",
      "Training Epoch 125: Loss: 0.3183, Accuracy: 89.23%\n",
      "Validation Loss: 0.3492, Accuracy: 88.90%\n",
      "Training Epoch 126: Loss: 0.3070, Accuracy: 89.61%\n",
      "Validation Loss: 0.3540, Accuracy: 88.69%\n",
      "Training Epoch 127: Loss: 0.3044, Accuracy: 89.55%\n",
      "Validation Loss: 0.3459, Accuracy: 89.38%\n",
      "Training Epoch 128: Loss: 0.3107, Accuracy: 89.25%\n",
      "Validation Loss: 0.3504, Accuracy: 89.22%\n",
      "Training Epoch 129: Loss: 0.3057, Accuracy: 89.56%\n",
      "Validation Loss: 0.3406, Accuracy: 89.25%\n",
      "Training Epoch 130: Loss: 0.2988, Accuracy: 89.81%\n",
      "Validation Loss: 0.3418, Accuracy: 89.31%\n",
      "Training Epoch 131: Loss: 0.3389, Accuracy: 88.38%\n",
      "Validation Loss: 0.5553, Accuracy: 85.76%\n",
      "Training Epoch 132: Loss: 0.3351, Accuracy: 88.62%\n",
      "Validation Loss: 0.3566, Accuracy: 88.79%\n",
      "Training Epoch 133: Loss: 0.2958, Accuracy: 89.95%\n",
      "Validation Loss: 0.3471, Accuracy: 89.18%\n",
      "Training Epoch 134: Loss: 0.3000, Accuracy: 89.75%\n",
      "Validation Loss: 0.3506, Accuracy: 89.12%\n",
      "Training Epoch 135: Loss: 0.2857, Accuracy: 90.27%\n",
      "Validation Loss: 0.3451, Accuracy: 89.22%\n",
      "Training Epoch 136: Loss: 0.2901, Accuracy: 90.18%\n",
      "Validation Loss: 0.3534, Accuracy: 89.08%\n",
      "Training Epoch 137: Loss: 0.2858, Accuracy: 90.24%\n",
      "Validation Loss: 0.3628, Accuracy: 89.08%\n",
      "Training Epoch 138: Loss: 0.2856, Accuracy: 90.23%\n",
      "Validation Loss: 0.3497, Accuracy: 89.41%\n",
      "Training Epoch 139: Loss: 0.2989, Accuracy: 89.70%\n",
      "Validation Loss: 0.3681, Accuracy: 88.71%\n",
      "Training Epoch 140: Loss: 0.2912, Accuracy: 90.07%\n",
      "Validation Loss: 0.3520, Accuracy: 89.22%\n",
      "Training Epoch 141: Loss: 0.2856, Accuracy: 90.14%\n",
      "Validation Loss: 0.3447, Accuracy: 89.29%\n",
      "Training Epoch 142: Loss: 0.2838, Accuracy: 90.26%\n",
      "Validation Loss: 0.3631, Accuracy: 89.16%\n",
      "Training Epoch 143: Loss: 0.2834, Accuracy: 90.43%\n",
      "Validation Loss: 0.3594, Accuracy: 89.30%\n",
      "Training Epoch 144: Loss: 0.2830, Accuracy: 90.45%\n",
      "Validation Loss: 0.3497, Accuracy: 89.37%\n",
      "Training Epoch 145: Loss: 0.2737, Accuracy: 90.67%\n",
      "Validation Loss: 0.3614, Accuracy: 89.12%\n",
      "Training Epoch 146: Loss: 0.3063, Accuracy: 89.76%\n",
      "Validation Loss: 0.3885, Accuracy: 88.18%\n",
      "Training Epoch 147: Loss: 0.2900, Accuracy: 90.16%\n",
      "Validation Loss: 0.3505, Accuracy: 89.36%\n",
      "Training Epoch 148: Loss: 0.2771, Accuracy: 90.55%\n",
      "Validation Loss: 0.3502, Accuracy: 89.00%\n",
      "Training Epoch 149: Loss: 0.2773, Accuracy: 90.42%\n",
      "Validation Loss: 0.3698, Accuracy: 88.83%\n",
      "Training Epoch 150: Loss: 0.2825, Accuracy: 90.19%\n",
      "Validation Loss: 0.3625, Accuracy: 88.78%\n",
      "Training Epoch 151: Loss: 0.2766, Accuracy: 90.50%\n",
      "Validation Loss: 0.3572, Accuracy: 89.33%\n",
      "Training Epoch 152: Loss: 0.2672, Accuracy: 90.77%\n",
      "Validation Loss: 0.3618, Accuracy: 89.16%\n",
      "Training Epoch 153: Loss: 0.2860, Accuracy: 90.20%\n",
      "Validation Loss: 0.3528, Accuracy: 88.92%\n",
      "Training Epoch 154: Loss: 0.2747, Accuracy: 90.57%\n",
      "Validation Loss: 0.3568, Accuracy: 89.32%\n",
      "Training Epoch 155: Loss: 0.2679, Accuracy: 90.80%\n",
      "Validation Loss: 0.3539, Accuracy: 89.18%\n",
      "Training Epoch 156: Loss: 0.2664, Accuracy: 90.72%\n",
      "Validation Loss: 0.3541, Accuracy: 89.10%\n",
      "Training Epoch 157: Loss: 0.2638, Accuracy: 90.98%\n",
      "Validation Loss: 0.3609, Accuracy: 89.21%\n",
      "Training Epoch 158: Loss: 0.2592, Accuracy: 91.28%\n",
      "Validation Loss: 0.3544, Accuracy: 89.49%\n",
      "Training Epoch 159: Loss: 0.2648, Accuracy: 90.91%\n",
      "Validation Loss: 0.3555, Accuracy: 89.35%\n",
      "Training Epoch 160: Loss: 0.2557, Accuracy: 91.42%\n",
      "Validation Loss: 0.3649, Accuracy: 89.09%\n",
      "Training Epoch 161: Loss: 0.2548, Accuracy: 91.44%\n",
      "Validation Loss: 0.3717, Accuracy: 89.13%\n",
      "Training Epoch 162: Loss: 0.2573, Accuracy: 91.25%\n",
      "Validation Loss: 0.3624, Accuracy: 89.36%\n",
      "Training Epoch 163: Loss: 0.2561, Accuracy: 91.39%\n",
      "Validation Loss: 0.3539, Accuracy: 89.35%\n",
      "Training Epoch 164: Loss: 0.2551, Accuracy: 91.30%\n",
      "Validation Loss: 0.3569, Accuracy: 89.64%\n",
      "Training Epoch 165: Loss: 0.2577, Accuracy: 91.18%\n",
      "Validation Loss: 0.3526, Accuracy: 89.62%\n",
      "Training Epoch 166: Loss: 0.2542, Accuracy: 91.38%\n",
      "Validation Loss: 0.3562, Accuracy: 89.30%\n",
      "Training Epoch 167: Loss: 0.2530, Accuracy: 91.44%\n",
      "Validation Loss: 0.3536, Accuracy: 89.59%\n",
      "Training Epoch 168: Loss: 0.2666, Accuracy: 91.05%\n",
      "Validation Loss: 0.3576, Accuracy: 89.49%\n",
      "Training Epoch 169: Loss: 0.2557, Accuracy: 91.14%\n",
      "Validation Loss: 0.3595, Accuracy: 89.39%\n",
      "Training Epoch 170: Loss: 0.2445, Accuracy: 91.56%\n",
      "Validation Loss: 0.3528, Accuracy: 89.65%\n",
      "Training Epoch 171: Loss: 0.2422, Accuracy: 91.77%\n",
      "Validation Loss: 0.3666, Accuracy: 89.56%\n",
      "Training Epoch 172: Loss: 0.2496, Accuracy: 91.55%\n",
      "Validation Loss: 0.3499, Accuracy: 89.39%\n",
      "Training Epoch 173: Loss: 0.2439, Accuracy: 91.70%\n",
      "Validation Loss: 0.3542, Accuracy: 89.65%\n",
      "Training Epoch 174: Loss: 0.2464, Accuracy: 91.64%\n",
      "Validation Loss: 0.3632, Accuracy: 89.39%\n",
      "Training Epoch 175: Loss: 0.2357, Accuracy: 92.10%\n",
      "Validation Loss: 0.3554, Accuracy: 89.56%\n",
      "Training Epoch 176: Loss: 0.2436, Accuracy: 91.63%\n",
      "Validation Loss: 0.3521, Accuracy: 89.86%\n",
      "Training Epoch 177: Loss: 0.2421, Accuracy: 91.64%\n",
      "Validation Loss: 0.3531, Accuracy: 89.54%\n",
      "Training Epoch 178: Loss: 0.2353, Accuracy: 92.07%\n",
      "Validation Loss: 0.3580, Accuracy: 89.66%\n",
      "Training Epoch 179: Loss: 0.2353, Accuracy: 91.98%\n",
      "Validation Loss: 0.3573, Accuracy: 89.94%\n",
      "Training Epoch 180: Loss: 0.2342, Accuracy: 92.09%\n",
      "Validation Loss: 0.3552, Accuracy: 89.71%\n",
      "Training Epoch 181: Loss: 0.2376, Accuracy: 91.92%\n",
      "Validation Loss: 0.3577, Accuracy: 89.56%\n",
      "Training Epoch 182: Loss: 0.2332, Accuracy: 92.03%\n",
      "Validation Loss: 0.3586, Accuracy: 90.06%\n",
      "Training Epoch 183: Loss: 0.2347, Accuracy: 91.94%\n",
      "Validation Loss: 0.3546, Accuracy: 89.62%\n",
      "Training Epoch 184: Loss: 0.2321, Accuracy: 92.11%\n",
      "Validation Loss: 0.3560, Accuracy: 89.56%\n",
      "Training Epoch 185: Loss: 0.2333, Accuracy: 92.05%\n",
      "Validation Loss: 0.3516, Accuracy: 89.58%\n",
      "Training Epoch 186: Loss: 0.2309, Accuracy: 92.06%\n",
      "Validation Loss: 0.3572, Accuracy: 89.61%\n",
      "Training Epoch 187: Loss: 0.2291, Accuracy: 92.30%\n",
      "Validation Loss: 0.3609, Accuracy: 89.68%\n",
      "Training Epoch 188: Loss: 0.2280, Accuracy: 92.06%\n",
      "Validation Loss: 0.3551, Accuracy: 89.87%\n",
      "Training Epoch 189: Loss: 0.2305, Accuracy: 92.17%\n",
      "Validation Loss: 0.3575, Accuracy: 89.86%\n",
      "Training Epoch 190: Loss: 0.2254, Accuracy: 92.25%\n",
      "Validation Loss: 0.3467, Accuracy: 89.70%\n",
      "Training Epoch 191: Loss: 0.2239, Accuracy: 92.33%\n",
      "Validation Loss: 0.3538, Accuracy: 89.86%\n",
      "Training Epoch 192: Loss: 0.2319, Accuracy: 92.21%\n",
      "Validation Loss: 0.3593, Accuracy: 89.73%\n",
      "Training Epoch 193: Loss: 0.2208, Accuracy: 92.50%\n",
      "Validation Loss: 0.3601, Accuracy: 89.76%\n",
      "Training Epoch 194: Loss: 0.2277, Accuracy: 92.31%\n",
      "Validation Loss: 0.3607, Accuracy: 89.72%\n",
      "Training Epoch 195: Loss: 0.2266, Accuracy: 92.33%\n",
      "Validation Loss: 0.3552, Accuracy: 89.88%\n",
      "Training Epoch 196: Loss: 0.2246, Accuracy: 92.30%\n",
      "Validation Loss: 0.3583, Accuracy: 90.02%\n",
      "Training Epoch 197: Loss: 0.2239, Accuracy: 92.52%\n",
      "Validation Loss: 0.3690, Accuracy: 89.61%\n",
      "Training Epoch 198: Loss: 0.2182, Accuracy: 92.60%\n",
      "Validation Loss: 0.3594, Accuracy: 89.69%\n",
      "Training Epoch 199: Loss: 0.2199, Accuracy: 92.49%\n",
      "Validation Loss: 0.3635, Accuracy: 89.75%\n",
      "Training Epoch 200: Loss: 0.2155, Accuracy: 92.59%\n",
      "Validation Loss: 0.3632, Accuracy: 89.71%\n",
      "Training Epoch 201: Loss: 0.2161, Accuracy: 92.71%\n",
      "Validation Loss: 0.3529, Accuracy: 90.04%\n",
      "Training Epoch 202: Loss: 0.2174, Accuracy: 92.55%\n",
      "Validation Loss: 0.3587, Accuracy: 89.91%\n",
      "Training Epoch 203: Loss: 0.2193, Accuracy: 92.59%\n",
      "Validation Loss: 0.3592, Accuracy: 90.16%\n",
      "Training Epoch 204: Loss: 0.2158, Accuracy: 92.70%\n",
      "Validation Loss: 0.3495, Accuracy: 90.04%\n",
      "Training Epoch 205: Loss: 0.2184, Accuracy: 92.54%\n",
      "Validation Loss: 0.3566, Accuracy: 89.82%\n",
      "Training Epoch 206: Loss: 0.2142, Accuracy: 92.62%\n",
      "Validation Loss: 0.3553, Accuracy: 89.94%\n",
      "Training Epoch 207: Loss: 0.2137, Accuracy: 92.75%\n",
      "Validation Loss: 0.3592, Accuracy: 90.10%\n",
      "Training Epoch 208: Loss: 0.2125, Accuracy: 92.61%\n",
      "Validation Loss: 0.3605, Accuracy: 89.92%\n",
      "Training Epoch 209: Loss: 0.2144, Accuracy: 92.61%\n",
      "Validation Loss: 0.3594, Accuracy: 89.93%\n",
      "Training Epoch 210: Loss: 0.2131, Accuracy: 92.81%\n",
      "Validation Loss: 0.3516, Accuracy: 90.23%\n",
      "Training Epoch 211: Loss: 0.2154, Accuracy: 92.67%\n",
      "Validation Loss: 0.3552, Accuracy: 90.07%\n",
      "Training Epoch 212: Loss: 0.2118, Accuracy: 92.71%\n",
      "Validation Loss: 0.3512, Accuracy: 89.99%\n",
      "Training Epoch 213: Loss: 0.2080, Accuracy: 92.89%\n",
      "Validation Loss: 0.3594, Accuracy: 90.03%\n",
      "Training Epoch 214: Loss: 0.2106, Accuracy: 92.87%\n",
      "Validation Loss: 0.3585, Accuracy: 90.12%\n",
      "Training Epoch 215: Loss: 0.2133, Accuracy: 92.83%\n",
      "Validation Loss: 0.3561, Accuracy: 89.95%\n",
      "Training Epoch 216: Loss: 0.2125, Accuracy: 92.92%\n",
      "Validation Loss: 0.3528, Accuracy: 90.17%\n",
      "Training Epoch 217: Loss: 0.2140, Accuracy: 92.65%\n",
      "Validation Loss: 0.3595, Accuracy: 90.09%\n",
      "Training Epoch 218: Loss: 0.2043, Accuracy: 93.19%\n",
      "Validation Loss: 0.3593, Accuracy: 89.98%\n",
      "Training Epoch 219: Loss: 0.2118, Accuracy: 92.86%\n",
      "Validation Loss: 0.3512, Accuracy: 90.03%\n",
      "Training Epoch 220: Loss: 0.2086, Accuracy: 93.00%\n",
      "Validation Loss: 0.3574, Accuracy: 89.95%\n",
      "Training Epoch 221: Loss: 0.2090, Accuracy: 92.86%\n",
      "Validation Loss: 0.3567, Accuracy: 89.88%\n",
      "Training Epoch 222: Loss: 0.2072, Accuracy: 92.88%\n",
      "Validation Loss: 0.3607, Accuracy: 90.01%\n",
      "Training Epoch 223: Loss: 0.2071, Accuracy: 93.01%\n",
      "Validation Loss: 0.3586, Accuracy: 90.00%\n",
      "Training Epoch 224: Loss: 0.2095, Accuracy: 92.81%\n",
      "Validation Loss: 0.3598, Accuracy: 90.17%\n",
      "Training Epoch 225: Loss: 0.2046, Accuracy: 93.07%\n",
      "Validation Loss: 0.3565, Accuracy: 90.06%\n",
      "Training Epoch 226: Loss: 0.2087, Accuracy: 92.94%\n",
      "Validation Loss: 0.3623, Accuracy: 89.88%\n",
      "Training Epoch 227: Loss: 0.2055, Accuracy: 92.99%\n",
      "Validation Loss: 0.3548, Accuracy: 90.01%\n",
      "Training Epoch 228: Loss: 0.2125, Accuracy: 92.68%\n",
      "Validation Loss: 0.3583, Accuracy: 90.09%\n",
      "Training Epoch 229: Loss: 0.2065, Accuracy: 92.98%\n",
      "Validation Loss: 0.3642, Accuracy: 89.98%\n",
      "Training Epoch 230: Loss: 0.2057, Accuracy: 93.06%\n",
      "Validation Loss: 0.3522, Accuracy: 90.08%\n",
      "Training Epoch 231: Loss: 0.2078, Accuracy: 92.85%\n",
      "Validation Loss: 0.3577, Accuracy: 90.04%\n",
      "Training Epoch 232: Loss: 0.2078, Accuracy: 93.00%\n",
      "Validation Loss: 0.3646, Accuracy: 90.08%\n",
      "Training Epoch 233: Loss: 0.2106, Accuracy: 92.84%\n",
      "Validation Loss: 0.3624, Accuracy: 90.08%\n",
      "Training Epoch 234: Loss: 0.2078, Accuracy: 92.88%\n",
      "Validation Loss: 0.3589, Accuracy: 90.13%\n",
      "Training Epoch 235: Loss: 0.2065, Accuracy: 93.03%\n",
      "Validation Loss: 0.3576, Accuracy: 89.92%\n",
      "Training Epoch 236: Loss: 0.2126, Accuracy: 92.76%\n",
      "Validation Loss: 0.3554, Accuracy: 90.05%\n",
      "Training Epoch 237: Loss: 0.2067, Accuracy: 92.97%\n",
      "Validation Loss: 0.3628, Accuracy: 90.01%\n",
      "Training Epoch 238: Loss: 0.2048, Accuracy: 93.13%\n",
      "Validation Loss: 0.3550, Accuracy: 89.98%\n",
      "Training Epoch 239: Loss: 0.2042, Accuracy: 93.07%\n",
      "Validation Loss: 0.3641, Accuracy: 90.00%\n",
      "Training Epoch 240: Loss: 0.2058, Accuracy: 93.10%\n",
      "Validation Loss: 0.3604, Accuracy: 89.98%\n",
      "Training Epoch 241: Loss: 0.2013, Accuracy: 93.13%\n",
      "Validation Loss: 0.3633, Accuracy: 90.03%\n",
      "Training Epoch 242: Loss: 0.2041, Accuracy: 93.05%\n",
      "Validation Loss: 0.3613, Accuracy: 89.95%\n",
      "Training Epoch 243: Loss: 0.2041, Accuracy: 93.05%\n",
      "Validation Loss: 0.3547, Accuracy: 90.01%\n",
      "Training Epoch 244: Loss: 0.2088, Accuracy: 92.94%\n",
      "Validation Loss: 0.3533, Accuracy: 90.04%\n",
      "Training Epoch 245: Loss: 0.2051, Accuracy: 93.08%\n",
      "Validation Loss: 0.3580, Accuracy: 89.96%\n",
      "Training Epoch 246: Loss: 0.2066, Accuracy: 92.93%\n",
      "Validation Loss: 0.3537, Accuracy: 90.12%\n",
      "Training Epoch 247: Loss: 0.2108, Accuracy: 92.90%\n",
      "Validation Loss: 0.3501, Accuracy: 90.12%\n",
      "Training Epoch 248: Loss: 0.2037, Accuracy: 92.98%\n",
      "Validation Loss: 0.3541, Accuracy: 90.04%\n",
      "Training Epoch 249: Loss: 0.2015, Accuracy: 93.23%\n",
      "Validation Loss: 0.3670, Accuracy: 89.99%\n",
      "Training Epoch 250: Loss: 0.2080, Accuracy: 92.92%\n",
      "Validation Loss: 0.3588, Accuracy: 89.94%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▄▆▆▇▇▇▇▇▇██▇█▇█████████████████████████</td></tr><tr><td>validation_loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>250</td></tr><tr><td>train_accuracy</td><td>92.925</td></tr><tr><td>train_loss</td><td>0.20804</td></tr><tr><td>validation_accuracy</td><td>89.94</td></tr><tr><td>validation_loss</td><td>0.35879</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_6th_train</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/e33sknvp' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/e33sknvp</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241115_110550-e33sknvp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration with AutoAugment, increased weight_decay, and other techniques\n",
    "config_modified = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-3,\n",
    "    'num_epochs': 250,\n",
    "    'model_name': 'resnet50',\n",
    "    'wandb_project_name': 'CIFAR10_training_with_various_models',\n",
    "    \"checkpoint_save_interval\": 10,\n",
    "    \"checkpoint_path\": \"checkpoints/checkpoint_modified.pth\",\n",
    "    \"best_model_path\": \"checkpoints/best_model_modified.pth\",\n",
    "    \"load_from_checkpoint\": None,\n",
    "    'test_mode': False  # 기본적으로 학습 모드\n",
    "}\n",
    "\n",
    "# Model with extended hidden dimensions, additional layer, and stochastic depth\n",
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50()\n",
    "        model.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(model.fc.in_features, 1024),  # 확장된 hidden dimension\n",
    "            torch.nn.BatchNorm1d(1024),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(1024, 512),                   # 추가된 hidden layer\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(512, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "    return model\n",
    "\n",
    "def load_cifar10_dataloaders(data_root_dir, device, batch_size, num_worker):\n",
    "    validation_size = 0.2\n",
    "    random_seed = 42\n",
    "    normalize = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "\n",
    "    # Data Augmentation for training set with AutoAugment\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    test_transforms = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=train_transforms)\n",
    "    val_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=test_transforms)\n",
    "    test_dataset = datasets.CIFAR10(root=data_root_dir, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    train_indices, val_indices = train_test_split(np.arange(len(train_dataset)), test_size=validation_size, random_state=random_seed)\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    kwargs = {'pin_memory': True} if device.startswith(\"cuda\") else {}\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_worker, **kwargs)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_worker, **kwargs)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_worker, **kwargs)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader, num_classes\n",
    "\n",
    "# Training loop without Gradient Clipping\n",
    "def train_loop(model, device, train_dataloader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step(closure) \n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Training Epoch {epoch + 1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluation_loop(model, device, dataloader, criterion, phase=\"validation\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    if phase == \"test\":\n",
    "        print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")  # Update output to show only once for test mode\n",
    "    else:\n",
    "        print(f\"{phase.capitalize()} Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "def train_main_modified(config):\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "    checkpoint_save_interval = config.get('checkpoint_save_interval', 10)\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "    test_mode = config.get('test_mode', False)\n",
    "    best_acc = 0\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.finish()\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config,\n",
    "        name=\"ResNet50_6th_train\"\n",
    "    )\n",
    "\n",
    "    # Set Device\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Model Setup\n",
    "    model = get_model(config[\"model_name\"], 10, config).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Optimizer and Scheduler Setup\n",
    "    optimizer = SAM(model.parameters(), base_optimizer=torch.optim.AdamW, lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "    # Load Data\n",
    "    train_dataloader, val_dataloader, test_dataloader, _ = load_cifar10_dataloaders(data_root_dir, device, batch_size, num_worker)\n",
    "\n",
    "    # Load checkpoint if needed\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = best_model_path if load_from_checkpoint == \"best\" else checkpoint_path\n",
    "        start_epoch, best_acc = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    # Test mode: evaluate only on the test set and skip training\n",
    "    if test_mode:\n",
    "        print(\"Running in test mode...\")\n",
    "        test_acc, test_loss = evaluation_loop(model, device, test_dataloader, criterion, phase=\"test\")\n",
    "        wandb.log({\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_acc\n",
    "        })\n",
    "        wandb.finish()\n",
    "        return\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_loss, train_acc = train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "        val_acc, val_loss = evaluation_loop(model, device, val_dataloader, criterion, phase=\"validation\")\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_acc,\n",
    "            'validation_loss': val_loss,\n",
    "            'validation_accuracy': val_acc\n",
    "        })\n",
    "\n",
    "        # Save checkpoint periodically or if it's the final epoch\n",
    "        if (epoch + 1) % checkpoint_save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "            is_best = val_acc > best_acc\n",
    "            best_acc = max(val_acc, best_acc)\n",
    "            save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_acc, is_best, best_model_path)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the modified training function to perform the experiment with AutoAugment\n",
    "train_main_modified(config_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985f834-6ccf-401c-a573-5ca07cc9e2be",
   "metadata": {},
   "source": [
    "## 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "720b982c-2952-4267-b0cb-ab89756dda8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Deep_Learning_and_its_Application_2/lab_05/wandb/run-20241115_115832-gweohlqy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/gweohlqy' target=\"_blank\">ResNet50_6th_train</a></strong> to <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/gweohlqy' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/gweohlqy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model resnet50 with 26139210 parameters (26139210 trainable)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "=> loaded checkpoint 'checkpoints/best_model_modified.pth' (epoch 210)\n",
      "Running in test mode...\n",
      "Test Loss: 0.3728, Test Accuracy: 89.64%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>89.64</td></tr><tr><td>test_loss</td><td>0.37281</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet50_6th_train</strong> at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/gweohlqy' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models/runs/gweohlqy</a><br/> View project at: <a href='https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models' target=\"_blank\">https://wandb.ai/unknownlimitless0301-university-of-suwon6591/CIFAR10_training_with_various_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241115_115832-gweohlqy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration for Test Mode\n",
    "config_testmode = {\n",
    "    **config_modified,\n",
    "    'test_mode': True,  # 평가 모드 설정\n",
    "    'load_from_checkpoint': 'best'\n",
    "}\n",
    "\n",
    "# Run the model in test mode for evaluation\n",
    "train_main_modified(config_testmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931900c-a6f5-401e-b0f7-757f8f1b0687",
   "metadata": {},
   "source": [
    "<mark>제출물</mark>\n",
    "\n",
    "1. 본인 이름이 나오도록 wandb 결과 화면을 캡처하여 `YOUR_PRIVATE_REPOSITORY_NAME/lab_05/wandb_results.png`에 저장한다. (5 points)\n",
    "2. 결과를 table로 정리한 뒤 그 아래에 분석 및 논의를 작성 한다. (15 points)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58889521-e289-4a08-b1cd-4861eded1da6",
   "metadata": {},
   "source": [
    "# wandb 결과\n",
    "\n",
    "<center><img src=\"wandb_results_updated.png\" width=\"1000px\"></img></center>\n",
    "\n",
    "# 5개 이상의 실험 결과\n",
    "\n",
    "|모델|실험 조건|test_accuracy|설명|\n",
    "|----|---------------------------------------------------|------|---------------------|\n",
    "|ResNet50|Activation Function: RELU, Learning Rate: 5e-4, Epochs: 100, Optimizer: Adam, Learning Rate Scheduler: CosineAnnealingLR|74.93%|      |\n",
    "|ResNet50|Activation Function: RELU, Learning Rate: 1e-3, Epochs: 100, Optimizer: SGD with Momentum(momentum=0.9, weight_decay=1e-4를 사용), Learning Rate Scheduler: ReduceLROnPlateau, 데이터 증강(Data Augmentation) 적용~>Random Crop: 이미지 크기를 32로 유지하면서 padding=4를 적용한 Random Crop을 사용, Random Horizontal Flip 시용, 0.5의 Dropout 확률 적용|76.18%|마지막 레이어를 확장하여, 512 유닛의 레이어와 ReLU 활성화 함수, Dropout(0.5)을 추가|\n",
    "|ResNet50|두번째 모델과 모두 동일하게 설정하였고, epochs만 200으로 변경.|77.19%||\n",
    "|ResNet50|Activation Function: GELU, Learning Rate: 1e-3, Epochs: 150, Optimizer: SAM(Sharpness-Aware Minimization)에 SGD with Momentum(momentum=0.9, weight_decay=1e-4를 사용)을 적용, Learning Rate Scheduler: ReduceLROnPlateau, 데이터 증강(Data Augmentation) 적용~>Random Crop: 이미지 크기를 32로 유지하면서 padding=4를 적용한 Random Crop을 사용, Random Horizontal Flip 시용, 0.5의 Dropout 확률 적용|80.79%|최종 FC 층을 재정의함. 2048 → 512로 변환, 배치 정규화 및 GELU 활성화 함수 적용, 드롭아웃을 통한 과적합 방지.|\n",
    "|ResNet50|Activation Function: GELU, Learning Rate: 1e-3, Epochs: 250, Optimizer: SAM (base_optimizer: AdamW, weight_decay=1e-4), Learning Rate Scheduler: CosineAnnealingLR (T_max=250, eta_min=1e-6), 데이터 증강 (Data Augmentation): AutoAugment, Random Crop (padding=4), Random Horizontal Flip|83.83%|Hidden Layer를 1024 유닛으로 확장하고, 추가적인 512 유닛의 레이어를 더함., Batch Normalization과 GELU 활성화 함수 적용., Dropout(0.5)를 두 번 적용하여 과적합 방지., AutoAugment를 활용하여 다양한 데이터 증강을 수행함으로써 모델의 일반화 성능을 높임.|\n",
    "|ResNet50|Activation Function: GELU, Learning Rate: 1e-3, Epochs: 250, Optimizer: SAM (base_optimizer: AdamW, weight_decay=1e-4), Learning Rate Scheduler: CosineAnnealingLR (T_max=250, eta_min=1e-6), 데이터 증강 (Data Augmentation): AutoAugment, Random Crop (padding=4), Random Horizontal Flip|89.64%|Dropout 비율을 0.3으로 낮춤.|\n",
    "\n",
    "**best model test_set accuracy**: **89.64%**\n",
    "\n",
    "# 분석 및 논의\n",
    "* **모델 학습 과정:** 모든 모델이 초기 학습 시 높은 손실값을 보이다가 학습이 진행됨에 따라 손실값이 감소하고 정확도가 증가하는 경향을 보였다. 첫 번째 모델부터 여섯 번째 모델까지 각기 다른 학습 조건과 최적화 기법을 적용하였고 모델 성능의 지속적인 향상이 관찰되었다. 네 번째 모델 이후부터는 현재까지 알려진 optimizer로 알려진 SAM 옵티마이저를 기본적으로 사용하였다. 그래서인지 모델의 일반화 성능이 네번째 모델 이후부터는 80%를 넘는 등 이전과 비교할때 개선된 것을 확인할 수 있었다.\n",
    "\n",
    "* **Validation 성능:** 학습이 진행됨에 따라 검증 정확도 또한 모든 모델에서 지속적으로 향상되는 것을 확인할 수 있었다. 첫 번째와 두 번째 모델에서는 검증 성능이 상대적으로 낮고 학습 정확도와의 차이가 나타나면서 과적합의 징후가 보였다. 반면, 다섯 번째와 여섯 번째 모델에서는 학습 정확도와 검증 정확도 사이의 차이가 거의 없어, 과적합이 줄어들고 모델의 일반화 능력이 개선된 것을 알 수 있었다.\n",
    "\n",
    "* **Adam 옵티마이저와 학습률 스케줄러:** 첫 번째 모델에서는 Adam 옵티마이저와 CosineAnnealingLR 학습률 스케줄러를 사용하여 학습이 빠르게 진행되었고, 초기에는 높은 학습률로 빠르게 학습한 후 후반부에는 낮은 학습률로 안정적인 학습을 유도했지만 과적합의 징후가 보이며 안정적이지 못한 모습이 관찰되었다. 두 번째와 세 번째 모델에서는 SGD 옵티마이저를 사용하면서도 ReduceLROnPlateau를 활용하여 학습률을 동적으로 감소시키고자 하였다. 네 번째 모델부터는 SAM(Sharpness-Aware Minimization)과 AutoAugment를 적용하여 test 데이터셋에서 더욱 향상된 일반화 성능을 달성할 수 있었다.\n",
    "\n",
    "* **데이터 증강 및 모델 구조 변경:** 앞선 네 가지 모델들에서는 데이터 증강 기법으로 Random Crop과 Random Horizontal Flip 기법만 적용하여 학습하였다. 다섯 번째 모델부터는 여기에 AutoAugment를 적용하였다. 또한 다섯 번째와 여섯 번째 모델에서는 기본 ResNet의 마지막 Fully Connected 층의 구조를 변경하여 Hidden Layer를 확장하고 Dropout을 사용해 과적합을 방지하고자 하여 모델의 일반화 성능을 높이고자 하였다.\n",
    "\n",
    "* **결과 분석:** 여섯 번째 모델에서 최종 테스트 정확도는 89.64%로, 이전 모델들과 비교할때 가장 높은 일반화 성능 수치를 관찰하였다. 이 모델의 경우 SAM 옵티마이저와 AutoAugment를 사용하여 모델이 다양한 데이터에 대해 더욱 일반화된 성능을 보일 수 있었고, AdamW 옵티마이저와 CosineAnnealingLR을 통해 학습을 더 효율적으로 진행할 수 있었다. 또한, Dropout의 수치를 0.3으로 설정하여, 모델의 뉴런들이 지나치게 적어지지 않도록 하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2d2d0-e0a7-4f0a-b0d1-eecf470451c9",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Lab을 마무리 짓기 전 저장된 checkpoint를 모두 지워 저장공간을 확보한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b674ca2-1c6a-4bd4-8619-0855e9ae6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.exists('checkpoints/'):\n",
    "    shutil.rmtree('checkpoints/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
